<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.2.0"/>
    <title>micro_sam API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>
            <img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/logo/logo_and_text.jpg" class="logo" alt="project logo"/>

            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>

            <h2>Contents</h2>
            <ul>
  <li><a href="#segment-anything-for-microscopy">Segment Anything for Microscopy</a>
  <ul>
    <li><a href="#quickstart">Quickstart</a></li>
    <li><a href="#citation">Citation</a></li>
  </ul></li>
  <li><a href="#installation">Installation</a>
  <ul>
    <li><a href="#from-mamba">From mamba</a></li>
    <li><a href="#from-source">From source</a></li>
    <li><a href="#from-installer">From installer</a></li>
  </ul></li>
  <li><a href="#annotation-tools">Annotation Tools</a>
  <ul>
    <li><a href="#starting-via-gui">Starting via GUI</a></li>
    <li><a href="#annotator-2d">Annotator 2D</a></li>
    <li><a href="#annotator-3d">Annotator 3D</a></li>
    <li><a href="#annotator-tracking">Annotator Tracking</a></li>
    <li><a href="#tips-tricks">Tips &amp; Tricks</a></li>
    <li><a href="#known-limitations">Known limitations</a></li>
  </ul></li>
  <li><a href="#how-to-use-the-python-library">How to use the Python Library</a>
  <ul>
    <li><a href="#training-your-own-model">Training your own model</a></li>
  </ul></li>
  <li><a href="#finetuned-models">Finetuned models</a>
  <ul>
    <li><a href="#which-model-should-i-choose">Which model should I choose?</a></li>
    <li><a href="#model-sources">Model Sources</a></li>
  </ul></li>
  <li><a href="#how-to-contribute">How to contribute</a>
  <ul>
    <li><a href="#discuss-your-ideas">Discuss your ideas</a></li>
    <li><a href="#clone-the-repository">Clone the repository</a></li>
    <li><a href="#create-your-development-environment">Create your development environment</a></li>
    <li><a href="#make-your-changes">Make your changes</a></li>
    <li><a href="#testing">Testing</a></li>
    <li><a href="#open-a-pull-request">Open a pull request</a></li>
    <li><a href="#optional-build-the-documentation">Optional: Build the documentation</a></li>
    <li><a href="#optional-benchmark-performance">Optional: Benchmark performance</a></li>
  </ul></li>
  <li><a href="#for-developers">For Developers</a>
  <ul>
    <li><a href="#annotation-tools-2">Annotation Tools</a></li>
  </ul></li>
  <li><a href="#using-micro_sam-on-band">Using micro_sam on BAND</a>
  <ul>
    <li><a href="#start-band">Start BAND</a></li>
    <li><a href="#start-micro_sam-in-band">Start micro_sam in BAND</a></li>
    <li><a href="#transfering-data-to-band">Transfering data to BAND</a></li>
  </ul></li>
</ul>


            <h2>Submodules</h2>
            <ul>
                    <li><a href="micro_sam/evaluation.html">evaluation</a></li>
                    <li><a href="micro_sam/inference.html">inference</a></li>
                    <li><a href="micro_sam/instance_segmentation.html">instance_segmentation</a></li>
                    <li><a href="micro_sam/multi_dimensional_segmentation.html">multi_dimensional_segmentation</a></li>
                    <li><a href="micro_sam/precompute_state.html">precompute_state</a></li>
                    <li><a href="micro_sam/prompt_based_segmentation.html">prompt_based_segmentation</a></li>
                    <li><a href="micro_sam/prompt_generators.html">prompt_generators</a></li>
                    <li><a href="micro_sam/sam_annotator.html">sam_annotator</a></li>
                    <li><a href="micro_sam/sample_data.html">sample_data</a></li>
                    <li><a href="micro_sam/training.html">training</a></li>
                    <li><a href="micro_sam/util.html">util</a></li>
                    <li><a href="micro_sam/visualization.html">visualization</a></li>
            </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
micro_sam    </h1>

                        <div class="docstring"><h1 id="segment-anything-for-microscopy">Segment Anything for Microscopy</h1>

<p>Segment Anything for Microscopy implements automatic and interactive annotation for microscopy data. It is built on top of <a href="https://segment-anything.com/">Segment Anything</a> by Meta AI and specializes it for microscopy and other bio-imaging data.
Its core components are:</p>

<ul>
<li>The <code><a href="">micro_sam</a></code> tools for interactive data annotation with <a href="https://napari.org/stable/">napari</a>.</li>
<li>The <code><a href="">micro_sam</a></code> library to apply Segment Anything to 2d and 3d data or fine-tune it on your data.</li>
<li>The <code><a href="">micro_sam</a></code> models that are fine-tuned on publicly available microscopy data.</li>
</ul>

<p>Our goal is to build fast and interactive annotation tools for microscopy data, like interactive cell segmentation from bounding boxes:</p>

<p><img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d04cb158-9f5b-4460-98cd-023c4f19cccd" alt="box-prompts" /></p>

<p><code><a href="">micro_sam</a></code> is under active development, but our goal is to keep the changes to the user interface and the interface of the python library as small as possible.
On our roadmap for more functionality are:</p>

<ul>
<li>Providing an installer for running <code><a href="">micro_sam</a></code> as a standalone application.</li>
<li>Releasing more and better finetuned models as well as the code for fine-tuning.</li>
<li>Integration of the finetuned models with <a href="https://bioimage.io/#/">bioimage.io</a></li>
<li>Implementing a napari plugin for <code><a href="">micro_sam</a></code>.</li>
</ul>

<p>If you run into any problems or have questions please open an issue on Github or reach out via <a href="https://forum.image.sc/">image.sc</a> using the tag <code>micro-sam</code> and tagging @constantinpape.</p>

<h2 id="quickstart">Quickstart</h2>

<p>You can install <code><a href="">micro_sam</a></code> via conda:</p>

<pre><code>$ conda install -c conda-forge micro_sam napari pyqt
</code></pre>

<p>We also provide experimental installers for all operating systems.
For more details on the available installation options check out <a href="#installation">the installation section</a>.</p>

<p>After installing <code><a href="">micro_sam</a></code> you can run the annotation tool via <code>$ micro_sam.annotator</code>, which opens a menu for selecting the annotation tool and its inputs.
See <a href="#annotation-tools">the annotation tool section</a> for an overview and explanation of the annotation functionality.</p>

<p>The <code><a href="">micro_sam</a></code> python library can be used via</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">import</span> <span class="nn">micro_sam</span>
</code></pre>
</div>

<p>It is explained in more detail <a href="#how-to-use-the-python-library">here</a>.</p>

<p>Our support for finetuned models is still experimental. We will soon release better finetuned models and host them on zenodo.
For now, check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/sam_annotator_2d.py#L62">the example script for the 2d annotator</a> to see how the finetuned models can be used within <code><a href="">micro_sam</a></code>.</p>

<h2 id="citation">Citation</h2>

<p>If you are using <code><a href="">micro_sam</a></code> in your research please cite</p>

<ul>
<li>Our <a href="https://doi.org/10.1101/2023.08.21.554208">preprint</a></li>
<li>and the original <a href="https://arxiv.org/abs/2304.02643">Segment Anything publication</a>.</li>
<li>If you use a <code>vit-tiny</code> models please also cite <a href="https://arxiv.org/abs/2306.14289">Mobile SAM</a>.</li>
</ul>

<h1 id="installation">Installation</h1>

<p>There are three ways to install <code><a href="">micro_sam</a></code>:</p>

<ul>
<li><a href="#from-mamba">From mamba</a> is the recommended way if you want to use all functionality.</li>
<li><a href="#from-source">From source</a> for setting up a development environment to use the latest version and be able to change and contribute to our software.</li>
<li><a href="#from-installer">From installer</a> to install without having to use conda (supported platforms: Windows and Linux, only for CPU users). </li>
</ul>

<h2 id="from-mamba">From mamba</h2>

<p><a href="https://mamba.readthedocs.io/en/latest/">mamba</a> is a drop-in replacement for conda, but much faster.
While the steps below may also work with <code>conda</code>, we highly recommend using <code>mamba</code>.
You can follow the instructions <a href="https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html">here</a> to install <code>mamba</code>.</p>

<p><strong>IMPORTANT</strong>: Make sure to avoid installing anything in the base environment.</p>

<p><code><a href="">micro_sam</a></code> can be installed in an existing environment via:</p>

<pre><code>$ mamba install -c conda-forge micro_sam
</code></pre>

<p>or you should create a new environment (here called <code>micro-sam</code>) via:</p>

<pre><code>$ mamba create -c conda-forge -n micro-sam micro_sam
</code></pre>

<p>if you want to use the GPU you need to install PyTorch from the <code>pytorch</code> channel instead of <code>conda-forge</code>. For example:</p>

<pre><code>$ mamba create -c pytorch -c nvidia -c conda-forge micro_sam pytorch pytorch-cuda=12.1
</code></pre>

<p>You may need to change this command to install the correct CUDA version for your computer, see <a href="https://pytorch.org/">https://pytorch.org/</a> for details.</p>

<p>You also need to install napari to use the annotation tool:</p>

<pre><code>$ mamba install -c conda-forge napari pyqt
</code></pre>

<p>(We don't include napari in the default installation dependencies to keep the choice of rendering backend flexible.)</p>

<h2 id="from-source">From source</h2>

<p>To install <code><a href="">micro_sam</a></code> from source, we recommend to first set up an environment with the necessary requirements:</p>

<ul>
<li><a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_gpu.yaml">environment_gpu.yaml</a>: sets up an environment with GPU support.</li>
<li><a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_cpu.yaml">environment_cpu.yaml</a>: sets up an environment with CPU support.</li>
</ul>

<p>To create one of these environments and install <code><a href="">micro_sam</a></code> into it follow these steps</p>

<ol>
<li>Clone the repository:</li>
</ol>

<pre><code>$ git clone https://github.com/computational-cell-analytics/micro-sam
</code></pre>

<ol start="2">
<li>Enter it:</li>
</ol>

<pre><code>$ cd micro-sam
</code></pre>

<ol start="3">
<li>Create the GPU or CPU environment:</li>
</ol>

<pre><code>$ mamba env create -f &lt;ENV_FILE&gt;.yaml
</code></pre>

<ol start="4">
<li>Activate the environment:</li>
</ol>

<pre><code>$ mamba activate sam
</code></pre>

<ol start="5">
<li>Install <code><a href="">micro_sam</a></code>:</li>
</ol>

<pre><code>$ pip install -e .
</code></pre>

<p><strong>Troubleshooting:</strong></p>

<ul>
<li>Installation on MAC with a M1 or M2 processor:
<ul>
<li>The pytorch installation from <code>environment_cpu.yaml</code> does not work with a MAC that has an M1 or M2 processor. Instead you need to:
<ul>
<li>Create a new environment: <code>mamba create -c conda-forge python pip -n sam</code></li>
<li>Activate it va <code>mamba activate sam</code></li>
<li>Follow the instructions for how to install pytorch for MAC via conda from <a href="https://pytorch.org/">pytorch.org</a>.</li>
<li>Install additional dependencies: <code>mamba install -c conda-forge napari python-elf tqdm</code></li>
<li>Install SegmentAnything: <code>pip install git+https://github.com/facebookresearch/segment-anything.git</code></li>
<li>Install <code><a href="">micro_sam</a></code> by running <code>pip install -e .</code> in this folder.</li>
</ul></li>
<li><strong>Note:</strong> we have seen many issues with the pytorch installation on MAC. If a wrong pytorch version is installed for you (which will cause pytorch errors once you run the application) please try again with a clean <code>mambaforge</code> installation. Please install the <code>OS X, arm64</code> version from <a href="https://github.com/conda-forge/miniforge#mambaforge">here</a>.</li>
<li>Some MACs require a specific installation order of packages. If the steps layed out above don't work for you please check out the procedure described <a href="https://github.com/computational-cell-analytics/micro-sam/issues/77">in this github issue</a>.</li>
</ul></li>
</ul>

<h2 id="from-installer">From installer</h2>

<p>We also provide installers for Linux and Windows:</p>

<ul>
<li><a href="https://owncloud.gwdg.de/index.php/s/hM1bQ108YmcwyDn">Linux</a></li>
<li><a href="https://owncloud.gwdg.de/index.php/s/T1weJclOiYUUULE">Windows</a>
<!---</li>
<li><a href="https://owncloud.gwdg.de/index.php/s/7YupGgACw9SHy2P">Mac</a>
--></li>
</ul>

<p><strong>The installers are still experimental and not fully tested.</strong> Mac is not supported yet, but we are working on also providing an installer for it.</p>

<p>If you encounter problems with them then please consider installing <code><a href="">micro_sam</a></code> via <a href="#from-mamba">mamba</a> instead.</p>

<p><strong>Linux Installer:</strong></p>

<p>To use the installer:</p>

<ul>
<li>Unpack the zip file you have downloaded.</li>
<li>Make the installer executable: <code>$ chmod +x micro_sam-0.2.0post1-Linux-x86_64.sh</code></li>
<li>Run the installer: <code>$./micro_sam-0.2.0post1-Linux-x86_64.sh$</code> 
<ul>
<li>You can select where to install <code><a href="">micro_sam</a></code> during the installation. By default it will be installed in <code>$HOME/micro_sam</code>.</li>
<li>The installer will unpack all <code><a href="">micro_sam</a></code> files to the installation directory.</li>
</ul></li>
<li>After the installation you can start the annotator with the command <code>.../micro_sam/bin/micro_sam.annotator</code>.
<ul>
<li>To make it easier to run the annotation tool you can add <code>.../micro_sam/bin</code> to your <code>PATH</code> or set a softlink to <code>.../micro_sam/bin/micro_sam.annotator</code>.</li>
</ul></li>
</ul>

<p><!---
<strong>Mac Installer:</strong></p>

<p>To use the Mac installer you will need to enable installing unsigned applications. Please follow <a href="https://disable-gatekeeper.github.io/">the instructions for 'Disabling Gatekeeper for one application only' here</a>.</p>

<p>Alternative link on how to disable gatekeeper.
<a href="https://www.makeuseof.com/how-to-disable-gatekeeper-mac/">https://www.makeuseof.com/how-to-disable-gatekeeper-mac/</a></p>

<p>TODO detailed instruction
--></p>

<p><strong>Windows Installer:</strong></p>

<ul>
<li>Unpack the zip file you have downloaded.</li>
<li>Run the installer by double clicking on it.</li>
<li>Choose installation type: <code>Just Me(recommended)</code> or <code>All Users(requires admin privileges)</code>.</li>
<li>Choose installation path. By default it will be installed in <code>C:\Users\&lt;Username&gt;\micro_sam</code> for <code>Just Me</code> installation or in <code>C:\ProgramData\micro_sam</code> for <code>All Users</code>.
<ul>
<li>The installer will unpack all micro_sam files to the installation directory.</li>
</ul></li>
<li>After the installation you can start the annotator by double clicking on <code>.\micro_sam\Scripts\micro_sam.annotator.exe</code> or  with the command <code>.\micro_sam\Scripts\micro_sam.annotator.exe</code> from the Command Prompt.</li>
</ul>

<h1 id="annotation-tools">Annotation Tools</h1>

<p><code><a href="">micro_sam</a></code> provides applications for fast interactive 2d segmentation, 3d segmentation and tracking.
See an example for interactive cell segmentation in phase-contrast microscopy (left), interactive segmentation
of mitochondria in volume EM (middle) and interactive tracking of cells (right).</p>

<p><img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d5ee2080-ab08-4716-b4c4-c169b4ed29f5" width="256">
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/dfca3d9b-dba5-440b-b0f9-72a0683ac410" width="256">
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/aefbf99f-e73a-4125-bb49-2e6592367a64" width="256"></p>

<p>The annotation tools can be started from the <code><a href="">micro_sam</a></code> GUI, the command line or from python scripts. The <code><a href="">micro_sam</a></code> GUI can be started by</p>

<pre><code>$ micro_sam.annotator
</code></pre>

<p>They are built using <a href="https://napari.org/stable/">napari</a> and <a href="https://pyapp-kit.github.io/magicgui/">magicgui</a> to provide the viewer and user interface.
If you are not familiar with napari yet, <a href="https://napari.org/stable/tutorials/fundamentals/quick_start.html">start here</a>.
The <code><a href="">micro_sam</a></code> tools use <a href="https://napari.org/stable/howtos/layers/points.html">the point layer</a>, <a href="https://napari.org/stable/howtos/layers/shapes.html">shape layer</a> and <a href="https://napari.org/stable/howtos/layers/labels.html">label layer</a>.</p>

<p>The annotation tools are explained in detail below. In addition to the documentation here we also provide <a href="https://www.youtube.com/watch?v=ket7bDUP9tI&list=PLwYZXQJ3f36GQPpKCrSbHjGiH39X4XjSO">video tutorials</a>.</p>

<h2 id="starting-via-gui">Starting via GUI</h2>

<p>The annotation toools can be started from a central GUI, which can be started with the command <code>$ micro_sam.annotator</code> or using the executable <a href="#from-installer">from an installer</a>.</p>

<p>In the GUI you can select with of the four annotation tools you want to use:
<img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/micro-sam-gui.png"></p>

<p>And after selecting them a new window will open where you can select the input file path and other optional parameter. Then click the top button to start the tool. <strong>Note: If you are not starting the annotation tool with a path to pre-computed embeddings then it can take several minutes to open napari after pressing the button because the embeddings are being computed.</strong></p>

<p><strong>Changes in version 0.3:</strong></p>

<p>We have made two changes in version 0.3 that are not reflected in the documentation below yet:</p>

<ul>
<li>We now support prompts from box, ellipse and polygon annotations. To reflect this we have renamed the <code>box_prompts</code> layer to <code>prompts</code> and the <code>prompts</code> layer to <code>point_prompts</code>.</li>
<li>We support automatic segmentation in 3d! To use it, you can first run automated segmentation in the current slice via <code>Automatic Segmentation</code>, and then extend the segmentation of these objects to 3d by running <code>Segment All Slices</code> with <code>layer: auto segmentation</code>.</li>
</ul>

<h2 id="annotator-2d">Annotator 2D</h2>

<p>The 2d annotator can be started by</p>

<ul>
<li>clicking <code>2d annotator</code> in the <code><a href="">micro_sam</a></code> GUI.</li>
<li>running <code>$ micro_sam.annotator_2d</code> in the command line. Run <code>micro_sam.annotator_2d -h</code> for details.</li>
<li>calling <code><a href="micro_sam/sam_annotator/annotator_2d.html">micro_sam.sam_annotator.annotator_2d</a></code> in a python script. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py">examples/annotator_2d.py</a> for details. </li>
</ul>

<p>The user interface of the 2d annotator looks like this:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/2d-annotator-menu.png" width="768"></p>

<p>It contains the following elements:</p>

<ol>
<li>The napari layers for the image, segmentations and prompts:
<ul>
<li><code>box_prompts</code>: shape layer that is used to provide box prompts to SegmentAnything.</li>
<li><code>prompts</code>: point layer that is used to provide prompts to SegmentAnything. Positive prompts (green points) for marking the object you want to segment, negative prompts (red points) for marking the outside of the object.</li>
<li><code>current_object</code>: label layer that contains the object you're currently segmenting.</li>
<li><code>committed_objects</code>: label layer with the objects that have already been segmented.</li>
<li><code>auto_segmentation</code>: label layer results from using SegmentAnything for automatic instance segmentation.</li>
<li><code>raw</code>: image layer that shows the image data.</li>
</ul></li>
<li>The prompt menu for changing the currently selected point from positive to negative and vice versa. This can also be done by pressing <code>t</code>.</li>
<li>The menu for automatic segmentation. Pressing <code>Segment All Objects</code> will run automatic segmentation. The results will be displayed in the <code>auto_segmentation</code> layer. Change the parameters <code>pred iou thresh</code> and <code>stability score thresh</code> to control how many objects are segmented.</li>
<li>The menu for interactive segmentation. Pressing <code>Segment Object</code> (or <code>s</code>) will run segmentation for the current prompts. The result is displayed in <code>current_object</code></li>
<li>The menu for commiting the segmentation. When pressing <code>Commit</code> (or <code>c</code>) the result from the selected layer (either <code>current_object</code> or <code>auto_segmentation</code>) will be transferred from the respective layer to <code>committed_objects</code>.</li>
<li>The menu for clearing the current annotations. Pressing <code>Clear Annotations</code> (or <code>shift c</code>) will clear the current annotations and the current segmentation.</li>
</ol>

<p>Note that point prompts and box prompts can be combined. When you're using point prompts you can only segment one object at a time. With box prompts you can segment several objects at once.</p>

<p>Check out <a href="https://youtu.be/ket7bDUP9tI">this video</a> for a tutorial for the 2d annotation tool.</p>

<p>We also provide the <code>image series annotator</code>, which can be used for running the 2d annotator for several images in a folder. You can start by clicking <code>Image series annotator</code> in the GUI, running <code>micro_sam.image_series_annotator</code> in the command line or from a <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/image_series_annotator.py">python script</a>.</p>

<h2 id="annotator-3d">Annotator 3D</h2>

<p>The 3d annotator can be started by</p>

<ul>
<li>clicking <code>3d annotator</code> in the <code><a href="">micro_sam</a></code> GUI.</li>
<li>running <code>$ micro_sam.annotator_3d</code> in the command line. Run <code>micro_sam.annotator_3d -h</code> for details.</li>
<li>calling <code><a href="micro_sam/sam_annotator/annotator_3d.html">micro_sam.sam_annotator.annotator_3d</a></code> in a python script. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_3d.py">examples/annotator_3d.py</a> for details.</li>
</ul>

<p>The user interface of the 3d annotator looks like this:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/3d-annotator-menu.png" width="768"></p>

<p>Most elements are the same as in <a href="#annotator-2d">the 2d annotator</a>:</p>

<ol>
<li>The napari layers that contain the image, segmentation and prompts. Same as for <a href="#annotator-2d">the 2d annotator</a> but without the <code>auto_segmentation</code> layer.</li>
<li>The prompt menu.</li>
<li>The menu for interactive segmentation.</li>
<li>The 3d segmentation menu. Pressing <code>Segment All Slices</code> (or <code>Shift-S</code>) will extend the segmentation for the current object across the volume.</li>
<li>The menu for committing the segmentation.</li>
<li>The menu for clearing the current annotations.</li>
</ol>

<p>Note that you can only segment one object at a time with the 3d annotator.</p>

<p>Check out <a href="https://youtu.be/PEy9-rTCdS4">this video</a> for a tutorial for the 3d annotation tool.</p>

<h2 id="annotator-tracking">Annotator Tracking</h2>

<p>The tracking annotator can be started by</p>

<ul>
<li>clicking <code>Tracking annotator</code> in the <code><a href="">micro_sam</a></code> GUI.</li>
<li>running <code>$ micro_sam.annotator_tracking</code> in the command line. Run <code>micro_sam.annotator_tracking -h</code> for details.</li>
<li>calling <code><a href="micro_sam/sam_annotator/annotator_tracking.html">micro_sam.sam_annotator.annotator_tracking</a></code> in a python script. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_tracking.py">examples/annotator_tracking.py</a> for details. </li>
</ul>

<p>The user interface of the tracking annotator looks like this:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/tracking-annotator-menu.png" width="768"></p>

<p>Most elements are the same as in <a href="#annotator-2d">the 2d annotator</a>:</p>

<ol>
<li>The napari layers that contain the image, segmentation and prompts. Same as for <a href="#annotator-2d">the 2d segmentation app</a> but without the <code>auto_segmentation</code> layer, <code>current_tracks</code> and <code>committed_tracks</code> are the equivalent of <code>current_object</code> and <code>committed_objects</code>.</li>
<li>The prompt menu.</li>
<li>The menu with tracking settings: <code>track_state</code> is used to indicate that the object you are tracking is dividing in the current frame. <code>track_id</code> is used to select which of the tracks after division you are following.</li>
<li>The menu for interactive segmentation.</li>
<li>The tracking menu. Press <code>Track Object</code> (or <code>Shift-S</code>) to segment the current object across time.</li>
<li>The menu for committing the current tracking result.</li>
<li>The menu for clearing the current annotations.</li>
</ol>

<p>Note that the tracking annotator only supports 2d image data, volumetric data is not supported.</p>

<p>Check out <a href="https://youtu.be/Xi5pRWMO6_w">this video</a> for a tutorial for how to use the tracking annotation tool.</p>

<h2 id="tips-tricks">Tips &amp; Tricks</h2>

<ul>
<li>Segment Anything was trained with a fixed image size of 1024 x 1024 pixels. Inputs that do not match this size will be internally resized to match it. Hence, applying Segment Anything to a much larger image will often lead to inferior results, because it will be downsampled by a large factor and the objects in the image become too small.
To address this image we implement tiling: cutting up the input image into tiles of a fixed size (with a fixed overlap) and running Segment Anything for the individual tiles.
You can activate tiling by passing the parameters <code>tile_shape</code>, which determines the size of the inner tile and <code>halo</code>, which determines the size of the additional overlap.
<ul>
<li>If you're using the <code><a href="">micro_sam</a></code> GUI you can specify the values for the <code>halo</code> and <code>tile_shape</code> via the <code>Tile X</code>, <code>Tile Y</code>, <code>Halo X</code> and <code>Halo Y</code>.</li>
<li>If you're using a python script you can pass them as tuples, e.g. <code>tile_shape=(1024, 1024), halo=(128, 128)</code>. See also <a href="https://github.com/computational-cell-analytics/micro-sam/blob/0921581e2964139194d235a87cb002d3f3667f45/examples/annotator_2d.py#L40">the wholeslide_annotator example</a>.</li>
<li>If you're using the command line functions you can pass them via the options <code>--tile_shape 1024 1024 --halo 128 128</code></li>
<li>Note that prediction with tiling only works when the embeddings are cached to file, so you must specify an <code>embedding_path</code> (<code>-e</code> in the CLI).</li>
<li>You should choose the <code>halo</code> such that it is larger than half of the maximal radius of the objects your segmenting.</li>
</ul></li>
<li>The applications pre-compute the image embeddings produced by SegmentAnything and (optionally) store them on disc. If you are using a CPU this step can take a while for 3d data or timeseries (you will see a progress bar with a time estimate). If you have access to a GPU without graphical interface (e.g. via a local computer cluster or a cloud provider), you can also pre-compute the embeddings there and then copy them to your laptop / local machine to speed this up. You can use the command <code>micro_sam.precompute_embeddings</code> for this (it is installed with the rest of the applications). You can specify the location of the precomputed embeddings via the <code>embedding_path</code> argument.</li>
<li>Most other processing steps are very fast even on a CPU, so interactive annotation is possible. An exception is the automatic segmentation step (2d segmentation), which takes several minutes without a GPU (depending on the image size). For large volumes and timeseries segmenting an object in 3d / tracking across time can take a couple settings with a CPU (it is very fast with a GPU).</li>
<li>You can also try using a smaller version of the SegmentAnything model to speed up the computations. For this you can pass the <code>model_type</code> argument and either set it to <code>vit_b</code> or to <code>vit_l</code> (default is <code>vit_h</code>). However, this may lead to worse results.</li>
<li>You can save and load the results from the <code>committed_objects</code> / <code>committed_tracks</code> layer to correct segmentations you obtained from another tool (e.g. CellPose) or to save intermediate annotation results. The results can be saved via <code>File -&gt; Save Selected Layer(s) ...</code> in the napari menu (see the tutorial videos for details). They can be loaded again by specifying the corresponding location via the <code>segmentation_result</code> (2d and 3d segmentation) or <code>tracking_result</code> (tracking) argument.</li>
</ul>

<h2 id="known-limitations">Known limitations</h2>

<ul>
<li>Segment Anything does not work well for very small or fine-grained objects (e.g. filaments).</li>
<li>For the automatic segmentation functionality we currently rely on the automatic mask generation provided by SegmentAnything. It is slow and often misses objects in microscopy images. For now, we only offer this functionality in the 2d segmentation app; we are working on improving it and extending it to 3d segmentation and tracking.</li>
<li>Prompt bounding boxes do not provide the full functionality for tracking yet (they cannot be used for divisions or for starting new tracks). See also <a href="https://github.com/computational-cell-analytics/micro-sam/issues/23">this github issue</a>.</li>
</ul>

<h1 id="how-to-use-the-python-library">How to use the Python Library</h1>

<p>The python library can be imported via</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">import</span> <span class="nn">micro_sam</span>
</code></pre>
</div>

<p>The library</p>

<ul>
<li>implements function to apply Segment Anything to 2d and 3d data more conveniently in <code><a href="micro_sam/prompt_based_segmentation.html">micro_sam.prompt_based_segmentation</a></code>.</li>
<li>provides more and improved automatic instance segmentation functionality in <code><a href="micro_sam/instance_segmentation.html">micro_sam.instance_segmentation</a></code>.</li>
<li>implements training functionality that can be used for finetuning on your own data in <code><a href="micro_sam/training.html">micro_sam.training</a></code>.</li>
<li>provides functionality for quantitative and qualitative evaluation of Segment Anything models in <code><a href="micro_sam/evaluation.html">micro_sam.evaluation</a></code>.</li>
</ul>

<p>You can import these sub-modules via</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">import</span> <span class="nn"><a href="micro_sam/prompt_based_segmentation.html">micro_sam.prompt_based_segmentation</a></span>
<span class="kn">import</span> <span class="nn"><a href="micro_sam/instance_segmentation.html">micro_sam.instance_segmentation</a></span>
<span class="c1"># etc.</span>
</code></pre>
</div>

<p>This functionality is used to implement the interactive annotation tools and can also be used as a standalone python library.
Some preliminary examples for how to use the python library can be found <a href="https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/use_as_library">here</a>. Check out the <code>Submodules</code> documentation for more details.</p>

<h2 id="training-your-own-model">Training your own model</h2>

<p>We reimplement the training logic described in the <a href="https://arxiv.org/abs/2304.02643">Segment Anything publication</a> to enable finetuning on custom data.
We use this functionality to provide the <a href="#finetuned-models">finetuned microscopy models</a> and it can also be used to finetune models on your own data.
In fact the best results can be expected when finetuning on your own data, and we found that it does not require much annotated training data to get siginficant improvements in model performance.
So a good strategy is to annotate a few images with one of the provided models using one of the interactive annotation tools and, if the annotation is not working as good as expected yet, finetune on the annotated data.
<!--
TODO: provide link to the paper with results on how much data is needed
--></p>

<p>The training logic is implemented in <code><a href="micro_sam/training.html">micro_sam.training</a></code> and is based on <a href="https://github.com/constantinpape/torch-em">torch-em</a>. Please check out <a href="https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning">examples/finetuning</a> to see how you can finetune on your own data with it. The script <code>finetune_hela.py</code> contains an example for finetuning on a small custom microscopy dataset and <code>use_finetuned_model.py</code> shows how this model can then be used in the interactive annotation tools.</p>

<p>Since release v0.4.0 we also support training an additional decoder for automatic instance segmentation. This yields better results than the automatic mask generation of segment anything and is significantly faster.
You can enable training of the decoder by setting <code>train_instance_segmentation = True</code> <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/finetuning/finetune_hela.py#L165">here</a>.
The script <code>instance_segmentation_with_finetuned_model.py</code> shows how to use it for automatic instance segmentation.
We will fully integrate this functionality with the annotation tool in the next release.</p>

<p>More advanced examples, including quantitative and qualitative evaluation, of finetuned models can be found in <a href="https://github.com/computational-cell-analytics/micro-sam/tree/master/finetuning">finetuning</a>, which contains the code for training and evaluating our microscopy models.</p>

<h1 id="finetuned-models">Finetuned models</h1>

<p>In addition to the original Segment anything models, we provide models that finetuned on microscopy data using the functionality from <code><a href="micro_sam/training.html">micro_sam.training</a></code>.
The models are hosted on zenodo. We currently offer the following models:</p>

<ul>
<li><code>vit_h</code>: Default Segment Anything model with vit-h backbone.</li>
<li><code>vit_l</code>: Default Segment Anything model with vit-l backbone.</li>
<li><code>vit_b</code>: Default Segment Anything model with vit-b backbone.</li>
<li><code>vit_t</code>: Segment Anything model with vit-tiny backbone. From the <a href="https://arxiv.org/abs/2306.14289">Mobile SAM publication</a>. </li>
<li><code>vit_b_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with vit-b backbone.</li>
<li><code>vit_b_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with vit-b backbone.</li>
<li><code>vit_b_em_boundaries</code>: Finetuned Segment Anything model for neurites and cells in electron microscopy data with vit-b backbone.</li>
</ul>

<p>See the two figures below of the improvements through the finetuned model for LM and EM data. </p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/lm_comparison.png" width="768"></p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/em_comparison.png" width="768"></p>

<p>You can select which of the models is used in the annotation tools by selecting the corresponding name from the <code>Model Type</code> menu:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/model-type-selector.png" width="256"></p>

<p>To use a specific model in the python library you need to pass the corresponding name as value to the <code>model_type</code> parameter exposed by all relevant functions.
See for example the <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L62">2d annotator example</a> where <code>use_finetuned_model</code> can be set to <code>True</code> to use the <code>vit_b_lm</code> model.</p>

<p>Note that we are still working on improving these models and may update them from time to time. All older models will stay available for download on zenodo, see <a href="#model-sources">model sources</a> below</p>

<h2 id="which-model-should-i-choose">Which model should I choose?</h2>

<p>As a rule of thumb:</p>

<ul>
<li>Use the <code>vit_b_lm</code> model for segmenting cells or nuclei in light microscopy.</li>
<li>Use the <code>vit_b_em_organelles</code> models for segmenting mitochondria, nuclei or other organelles in electron microscopy.</li>
<li>Use the <code>vit_b_em_boundaries</code> models for segmenting cells or neurites in electron microscopy.</li>
<li>For other use-cases use one of the default models.</li>
</ul>

<p>See also the figures above for examples where the finetuned models work better than the vanilla models.
Currently the model <code>vit_h</code> is used by default.</p>

<p>We are working on further improving these models and adding new models for other biomedical imaging domains.</p>

<h2 id="model-sources">Model Sources</h2>

<p>Here is an overview of all finetuned models we have released to zenodo so far:</p>

<ul>
<li><a href="https://zenodo.org/records/10524894">vit_b_em_boundaries</a>: for segmenting compartments delineated by boundaries such as cells or neurites in EM.</li>
<li><a href="https://zenodo.org/records/10524828">vit_b_em_organelles</a>: for segmenting mitochondria, nuclei or other organelles in EM.</li>
<li><a href="https://zenodo.org/records/10524791">vit_b_lm</a>: for segmenting cells and nuclei in LM.</li>
<li><a href="https://zenodo.org/records/8250291">vit_h_em</a>: this model is outdated.</li>
<li><a href="https://zenodo.org/records/8250299">vit_h_lm</a>: this model is outdated.</li>
</ul>

<p>Some of these models contain multiple versions.</p>

<h1 id="how-to-contribute">How to contribute</h1>

<ul>
<li><a href="#discuss-your-ideas">Discuss your ideas</a></li>
<li><a href="#clone-the-repository">Clone the repository</a></li>
<li><a href="#create-your-development-environment">Create your development environment</a></li>
<li><a href="#make-your-changes">Make your changes</a></li>
<li><a href="#testing">Testing</a>
<ul>
<li><a href="#run-the-tests">Run the tests</a></li>
<li><a href="#writing-your-own-tests">Writing your own tests</a></li>
</ul></li>
<li><a href="#open-a-pull-request">Open a pull request</a></li>
<li><a href="#optional-build-the-documentation">Optional: Build the documentation</a></li>
<li><a href="#optional-benchmark-performance">Optional: Benchmark performance</a>
<ul>
<li><a href="#run-the-benchmark-script">Run the benchmark script</a></li>
<li><a href="#line-profiling">Line profiling</a></li>
<li><a href="#snakeviz-visualization">Snakeviz visualization</a></li>
<li><a href="#memory-profiling-with-memray">Memory profiling with memray</a></li>
</ul></li>
</ul>

<h2 id="discuss-your-ideas">Discuss your ideas</h2>

<p>We welcome new contributions!</p>

<p>First, discuss your idea by opening a <a href="https://github.com/computational-cell-analytics/micro-sam/issues/new">new issue</a> in micro-sam.</p>

<p>This allows you to ask questions, and have the current developers make suggestions about the best way to implement your ideas.</p>

<p>You may also find it helpful to look at this <a href="#for-developers">developer guide</a>, which explains the organization of the micro-sam code.</p>

<h2 id="clone-the-repository">Clone the repository</h2>

<p>We use <a href="https://git-scm.com/">git</a> for version control.</p>

<p>Clone the repository, and checkout the development branch:</p>

<pre><code>git clone https://github.com/computational-cell-analytics/micro-sam.git
cd micro-sam
git checkout dev
</code></pre>

<h2 id="create-your-development-environment">Create your development environment</h2>

<p>We use <a href="https://docs.conda.io/en/latest/">conda</a> to <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">manage our environments</a>. If you don't have this already, install <a href="https://docs.conda.io/projects/miniconda/en/latest/">miniconda</a> or <a href="https://mamba.readthedocs.io/en/latest/">mamba</a> to get started.</p>

<p>Now you can create the environment, install user and develoepr dependencies, and micro-sam as an editable installation:</p>

<pre><code>conda env create environment-gpu.yml
conda activate sam
python -m pip install requirements-dev.txt
python -m pip install -e .
</code></pre>

<h2 id="make-your-changes">Make your changes</h2>

<p>Now it's time to make your code changes.</p>

<p>Typically, changes are made branching off from the development branch. Checkout <code>dev</code> and then create a new branch to work on your changes.</p>

<pre><code>git checkout dev
git checkout -b my-new-feature
</code></pre>

<p>We use <a href="https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html">google style python docstrings</a> to create documentation for all new code.</p>

<p>You may also find it helpful to look at this <a href="#for-developers">developer guide</a>, which explains the organization of the micro-sam code.</p>

<h2 id="testing">Testing</h2>

<h3 id="run-the-tests">Run the tests</h3>

<p>The tests for micro-sam are run with <a href="https://docs.pytest.org/en/7.4.x/">pytest</a></p>

<p>To run the tests:</p>

<pre><code>pytest
</code></pre>

<h3 id="writing-your-own-tests">Writing your own tests</h3>

<p>If you have written new code, you will need to write tests to go with it.</p>

<h4 id="unit-tests">Unit tests</h4>

<p>Unit tests are the preferred style of tests for user contributions. Unit tests check small, isolated parts of the code for correctness. If your code is too complicated to write unit tests easily, you may need to consider breaking it up into smaller functions that are easier to test.</p>

<h4 id="tests-involving-napari">Tests involving napari</h4>

<p>In cases where tests <em>must</em> use the napari viewer, <a href="https://napari.org/stable/plugins/test_deploy.html#tips-for-testing-napari-plugins">these tips might be helpful</a> (in particular, the <code>make_napari_viewer_proxy</code> fixture).</p>

<p>These kinds of tests should be used only in limited circumstances. Developers are <a href="https://napari.org/stable/plugins/test_deploy.html#prefer-smaller-unit-tests-when-possible">advised to prefer smaller unit tests, and avoid integration tests</a> wherever possible.</p>

<h4 id="code-coverage">Code coverage</h4>

<p>Pytest uses the <a href="https://pytest-cov.readthedocs.io/en/latest/">pytest-cov</a> plugin to automatically determine which lines of code are covered by tests.</p>

<p>A short summary report is printed to the terminal output whenever you run pytest. The full results are also automatically written to a file named <code>coverage.xml</code>.</p>

<p>The <a href="https://marketplace.visualstudio.com/items?itemName=ryanluker.vscode-coverage-gutters">Coverage Gutters VSCode extension</a> is useful for visualizing which parts of the code need better test coverage. PyCharm professional <a href="https://www.jetbrains.com/pycharm/guide/tips/spot-coverage-in-gutter/">has a similar feature</a>, and you may be able to find similar tools for your preferred editor.</p>

<p>We also use <a href="https://app.codecov.io/gh/computational-cell-analytics/micro-sam">codecov.io</a> to display the code coverage results from our Github Actions continuous integration.</p>

<h2 id="open-a-pull-request">Open a pull request</h2>

<p>Once you've made changes to the code and written some tests to go with it, you are ready to <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests">open a pull request</a>. You can <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-requests">mark your pull request as a draft</a> if you are still working on it, and still get the benefit of discussing the best approach with maintainers.</p>

<p>Remember that typically changes to micro-sam are made branching off from the development branch. So, you will need to open your pull request to merge back into the <code>dev</code> branch <a href="https://github.com/computational-cell-analytics/micro-sam/compare/dev...dev">like this</a>.</p>

<h2 id="optional-build-the-documentation">Optional: Build the documentation</h2>

<p>We use <a href="https://pdoc.dev/docs/pdoc.html">pdoc</a> to build the documentation.</p>

<p>To build the documentation locally, run this command:</p>

<pre><code>python build_doc.py
</code></pre>

<p>This will start a local server and display the HTML documentation. Any changes you make to the documentation will be updated in real time (you may need to refresh your browser to see the changes).</p>

<p>If you want to save the HTML files, append <code>--out</code> to the command, like this:</p>

<pre><code>python build_doc.py --out
</code></pre>

<p>This will save the HTML files into a new directory named <code>tmp</code>.</p>

<p>You can add content to the documentation in two ways:</p>

<ol>
<li>By adding or updating <a href="https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html">google style python docstrings</a> in the micro-sam code.
<ul>
<li><a href="https://pdoc.dev/docs/pdoc.html">pdoc</a> will automatically find and include docstrings in the documentation.</li>
</ul></li>
<li>By adding or editing markdown files in the micro-sam <code>doc</code> directory.
<ul>
<li>If you add a new markdown file to the documentation, you must tell <a href="https://pdoc.dev/docs/pdoc.html">pdoc</a> that it exists by adding a line to the <code>micro_sam/__init__.py</code> module docstring (eg: <code>.. include:: ../doc/my_amazing_new_docs_page.md</code>). Otherwise it will not be included in the final documentation build!</li>
</ul></li>
</ol>

<h2 id="optional-benchmark-performance">Optional: Benchmark performance</h2>

<p>There are a number of options you can use to benchmark performance, and identify problems like slow run times or high memory use in micro-sam.</p>

<ul>
<li><a href="#run-the-benchmark-script">Run the benchmark script</a></li>
<li><a href="#line-profiling">Line profiling</a></li>
<li><a href="#snakeviz-visualization">Snakeviz visualization</a></li>
<li><a href="#memory-profiling-with-memray">Memory profiling with memray</a></li>
</ul>

<h3 id="run-the-benchmark-script">Run the benchmark script</h3>

<p>There is a performance benchmark script available in the micro-sam repository at <code>development/benchmark.py</code>.</p>

<p>To run the benchmark script:</p>

<pre><code>python development/benchmark.py --model_type vit_t --device cpu`
</code></pre>

<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>

<pre><code>python development/benchmark.py --help
</code></pre>

<h3 id="line-profiling">Line profiling</h3>

<p>For more detailed line by line performance results, we can use <a href="https://github.com/pyutils/line_profiler">line-profiler</a>.</p>

<blockquote>
  <p><a href="https://github.com/pyutils/line_profiler">line_profiler</a> is a module for doing line-by-line profiling of functions. kernprof is a convenient script for running either line_profiler or the Python standard library's cProfile or profile modules, depending on what is available.</p>
</blockquote>

<p>To do line-by-line profiling:</p>

<ol>
<li>Ensure you have line profiler installed: <code>python -m pip install line_profiler</code></li>
<li>Add <code>@profile</code> decorator to any function in the call stack</li>
<li>Run <code>kernprof -lv benchmark.py --model_type vit_t --device cpu</code></li>
</ol>

<p>For more details about how to use line-profiler and kernprof, see <a href="https://kernprof.readthedocs.io/en/latest/">the documentation</a>.</p>

<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>

<pre><code>python development/benchmark.py --help
</code></pre>

<h3 id="snakeviz-visualization">Snakeviz visualization</h3>

<p>For more detailed visualizations of profiling results, we use <a href="https://jiffyclub.github.io/snakeviz/">snakeviz</a>.</p>

<blockquote>
  <p>SnakeViz is a browser based graphical viewer for the output of Pythons cProfile module</p>
</blockquote>

<ol>
<li>Ensure you have snakeviz installed: <code>python -m pip install snakeviz</code></li>
<li>Generate profile file: <code>python -m cProfile -o program.prof benchmark.py --model_type vit_h --device cpu</code></li>
<li>Visualize profile file: <code>snakeviz program.prof</code></li>
</ol>

<p>For more details about how to use snakeviz, see <a href="https://jiffyclub.github.io/snakeviz/">the documentation</a>.</p>

<h3 id="memory-profiling-with-memray">Memory profiling with memray</h3>

<p>If you need to investigate memory use specifically, we use <a href="https://github.com/bloomberg/memray">memray</a>.</p>

<blockquote>
  <p>Memray is a memory profiler for Python. It can track memory allocations in Python code, in native extension modules, and in the Python interpreter itself. It can generate several different types of reports to help you analyze the captured memory usage data. While commonly used as a CLI tool, it can also be used as a library to perform more fine-grained profiling tasks.</p>
</blockquote>

<p>For more details about how to use memray, see <a href="https://bloomberg.github.io/memray/getting_started.html">the documentation</a>.</p>

<h1 id="for-developers">For Developers</h1>

<p>This software consists of four different python (sub-)modules:</p>

<ul>
<li>The top-level <code><a href="">micro_sam</a></code> module implements general purpose functionality for using Segment Anything for multi-dimension data.</li>
<li><code><a href="micro_sam/evaluation.html">micro_sam.evaluation</a></code> provides functionality to evaluate Segment Anything models on (microscopy) segmentation tasks.</li>
<li><code>micro_sam.traning</code> implements the training functionality to finetune Segment Anything on custom segmentation datasets.</li>
<li><code><a href="micro_sam/sam_annotator.html">micro_sam.sam_annotator</a></code> implements the interactive annotation tools.</li>
</ul>

<h2 id="annotation-tools-2">Annotation Tools</h2>

<p>The annotation tools are currently implemented as stand-alone napari applications. We are in the process of implementing them as napari plugins instead (see <a href="https://github.com/computational-cell-analytics/micro-sam/issues/167">https://github.com/computational-cell-analytics/micro-sam/issues/167</a> for details), and the descriptions here refer to the planned architecture for the plugins.</p>

<p>There are four annotation tools:</p>

<ul>
<li><code><a href="micro_sam/sam_annotator/annotator_2d.html">micro_sam.sam_annotator.annotator_2d</a></code>: for interactive segmentation of 2d images.</li>
<li><code><a href="micro_sam/sam_annotator/annotator_3d.html">micro_sam.sam_annotator.annotator_3d</a></code>: for interactive segmentation of volumetric images.</li>
<li><code><a href="micro_sam/sam_annotator/annotator_tracking.html">micro_sam.sam_annotator.annotator_tracking</a></code>: for interactive tracking in timeseries of 2d images.</li>
<li><code><a href="micro_sam/sam_annotator/image_series_annotator.html">micro_sam.sam_annotator.image_series_annotator</a></code>: for applying the 2d annotation tool to a series of images.</li>
</ul>

<p>An overview of the functionality of the different tools:</p>

<table>
<thead>
<tr>
  <th>Functionality</th>
  <th>annotator_2d</th>
  <th>annotator_3d</th>
  <th>annotator_tracking</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Interactive segmentation</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td>
</tr>
<tr>
  <td>For multiple objects at a time</td>
  <td>Yes</td>
  <td>No</td>
  <td>No</td>
</tr>
<tr>
  <td>Interactive 3d segmentation via projection</td>
  <td>No</td>
  <td>Yes</td>
  <td>Yes</td>
</tr>
<tr>
  <td>Support for dividing objects</td>
  <td>No</td>
  <td>No</td>
  <td>Yes</td>
</tr>
<tr>
  <td>Automatic segmentation</td>
  <td>Yes</td>
  <td>Yes (on <code>dev</code>)</td>
  <td>No</td>
</tr>
</tbody>
</table>

<p>The functionality for the <code>image_series_annotator</code> is not listed because it is identical with the functionality of the <code>annotator_2d</code>.</p>

<p>Each tool implements the follwing core logic:</p>

<ol>
<li>The image embeddings (prediction from SAM image encoder) are pre-computed for the input data (2d image, image volume or timeseries). These embeddings can be cached to a zarr file.</li>
<li>Interactive (and automatic) segmentation functionality is implemented by a UI based on <code>napari</code> and <code>magicgui</code> functionality.</li>
</ol>

<p>Each tool has two different entry points:</p>

<ul>
<li>From napari plugin menu, e.g. <code>plugin-&gt;micro_sam-&gt;annotator_2d</code> (This entry point is called <em>plugin</em> in the following).</li>
<li>From the command line, e.g. <code>micro_sam.annotator_2d -i /path/to/image</code> (This entry point is called <em>CLI</em> in the following).</li>
</ul>

<p>The tools are implemented their own submodules, e.g. <code><a href="micro_sam/sam_annotator/annotator_2d.html">micro_sam.sam_annotator.annotator_2d</a></code> with shared functionality implemented in <code><a href="micro_sam/sam_annotator/util.html">micro_sam.sam_annotator.util</a></code>. The function <code>micro_sam.sam_annotator.annotator_2d.annotator_2d_plugin</code> implements the <em>plugin</em> entry point, using the <code>magicgui.magic_factory</code> decorator. <code><a href="micro_sam/sam_annotator/annotator_2d.html#annotator_2d">micro_sam.sam_annotator.annotator_2d.annotator_2d</a></code>  implements the <em>CLI</em> entry point; it calls the <code>annotator_2d_plugin</code> function internally.
The image embeddings are computed by the <code>embedding widget</code> (@GenevieveBuckley: will need to be implemented in your PR), which takes the image data from an image layer.
In case of the <em>plugin</em> entry point this image layer is created by the user (by loading an image into napari), and the user can then select in the <code>embedding widget</code> which layer to use for embedding computation. 
In case of <em>CLI</em> the image data is specified via the <code>-i</code> parameter, the layer is created for that image and the embeddings are computed for it automatically.
The same overall design holds true for the other plugins. The flow chart below shows a flow chart with a simplified overview of the design of the 2d annotation tool. Rounded squares represent functions or the corresponding widget and squares napari layers or other data, orange represents the <em>plugin</em> enty point, cyan <em>CLI</em>. Arrows that do not have a label correspond to a simple input/output relation.</p>

<p><img src="./images/2d-annotator-flow.png" alt="annotator 2d flow diagram" /></p>

<p><!---
Source for the diagram is here:
<a href="https://docs.google.com/presentation/d/1fMDNBYMYxeqe4dk6OmmFxoI8sYvCu4EPZS_LyTsTg_s/edit#slide=id.p">https://docs.google.com/presentation/d/1fMDNBYMYxeqe4dk6OmmFxoI8sYvCu4EPZS_LyTsTg_s/edit#slide=id.p</a>
--></p>

<h1 id="using-micro_sam-on-band">Using micro_sam on BAND</h1>

<p>BAND is a service offered by EMBL Heidelberg that gives access to a virtual desktop for image analysis tasks. It is free to use and <code><a href="">micro_sam</a></code> is installed there.
In order to use BAND and start <code><a href="">micro_sam</a></code> on it follow these steps:</p>

<h2 id="start-band">Start BAND</h2>

<ul>
<li>Go to <a href="https://band.embl.de/">https://band.embl.de/</a> and click <strong>Login</strong>. If you have not used BAND before you will need to register for BAND. Currently you can only sign up via a google account.</li>
<li>Launch a BAND desktop with sufficient resources. It's particularly important to select a GPU. The settings from the image below are a good choice.</li>
<li>Go to the desktop by clicking <strong>GO TO DESKTOP</strong> in the <strong>Running Desktops</strong> menu. See also the screenshot below.</li>
</ul>

<p><img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/f965fce2-b924-4fc8-871b-f3201e502138" alt="image" /></p>

<h2 id="start-micro_sam-in-band">Start micro_sam in BAND</h2>

<ul>
<li>Select <strong>Applications->Image Analysis->uSAM</strong> (see screenshot)
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5daeafb3-119b-4104-8708-aab2960cb21c" alt="image" /></li>
<li>This will open the micro_sam menu, where you can select the tool you want to use (see screenshot). Note: this may take a few minutes.
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/900ce0b9-4cf8-418c-94f1-e99ac7bc0086" alt="image" /></li>
<li>For testing if the tool works, it's best to use the <strong>2d annotator</strong> first.
<ul>
<li>You can find an example image to use here: <code>/scratch/cajal-connectomics/hela-2d-image.png</code>. Select it via <strong>Select image</strong>. (see screenshot)
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5fbd1c53-2ba1-47d4-ae50-dfab890ac9d3" alt="image" /></li>
</ul></li>
<li>Then press <strong>2d annotator</strong> and the tool will start.</li>
</ul>

<h2 id="transfering-data-to-band">Transfering data to BAND</h2>

<p>To copy data to and from BAND you can use any cloud storage, e.g. ownCloud, dropbox or google drive. For this, it's important to note that copy and paste, which you may need for accessing links on BAND, works a bit different in BAND:</p>

<ul>
<li>To copy text into BAND you first need to copy it on your computer (e.g. via selecting it + <code>ctrl + c</code>).</li>
<li>Then go to the browser window with BAND and press <code>ctrl + shift + alt</code>. This will open a side window where you can paste your text via <code>ctrl + v</code>.</li>
<li>Then select the text in this window and copy it via <code>ctrl + c</code>.</li>
<li>Now you can close the side window via <code>ctrl + shift + alt</code> and paste the text in band via <code>ctrl + v</code></li>
</ul>

<p>The video below shows how to copy over a link from owncloud and then download the data on BAND using copy and paste:</p>

<p><a href="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287">https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287</a></p>
</div>

                        <input id="mod-micro_sam-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-micro_sam-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos"> 1</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos"> 2</span></a><span class="sd">.. include:: ../doc/start_page.md</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos"> 3</span></a><span class="sd">.. include:: ../doc/installation.md</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos"> 4</span></a><span class="sd">.. include:: ../doc/annotation_tools.md</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos"> 5</span></a><span class="sd">.. include:: ../doc/python_library.md</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos"> 6</span></a><span class="sd">.. include:: ../doc/finetuned_models.md</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos"> 7</span></a><span class="sd">.. include:: ../doc/contributing.md</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos"> 8</span></a><span class="sd">.. include:: ../doc/development.md</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos"> 9</span></a><span class="sd">.. include:: ../doc/band.md</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">10</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">11</span></a><span class="kn">import</span> <span class="nn">os</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">12</span></a>
</span><span id="L-13"><a href="#L-13"><span class="linenos">13</span></a><span class="kn">from</span> <span class="nn">.__version__</span> <span class="kn">import</span> <span class="n">__version__</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">14</span></a>
</span><span id="L-15"><a href="#L-15"><span class="linenos">15</span></a><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTORCH_ENABLE_MPS_FALLBACK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
</span></pre></div>


            </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>