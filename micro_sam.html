<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.2.0"/>
    <title>micro_sam API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .pdoc-alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:1rem center;margin-bottom:1rem;}.pdoc .pdoc-alert > *:last-child{margin-bottom:0;}.pdoc .pdoc-alert-note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .pdoc-alert-danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>
            <img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/logo/logo_and_text.png" class="logo" alt="project logo"/>

            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>

            <h2>Contents</h2>
            <ul>
  <li><a href="#segment-anything-for-microscopy">Segment Anything for Microscopy</a>
  <ul>
    <li><a href="#quickstart">Quickstart</a></li>
    <li><a href="#citation">Citation</a></li>
  </ul></li>
  <li><a href="#installation">Installation</a>
  <ul>
    <li><a href="#from-mamba">From mamba</a></li>
    <li><a href="#from-source">From source</a></li>
    <li><a href="#from-installer">From installer</a></li>
  </ul></li>
  <li><a href="#annotation-tools">Annotation Tools</a>
  <ul>
    <li><a href="#annotator-2d">Annotator 2D</a></li>
    <li><a href="#annotator-3d">Annotator 3D</a></li>
    <li><a href="#annotator-tracking">Annotator Tracking</a></li>
    <li><a href="#image-series-annotator">Image Series Annotator</a></li>
    <li><a href="#finetuning-ui">Finetuning UI</a></li>
  </ul></li>
  <li><a href="#using-the-python-library">Using the Python Library</a>
  <ul>
    <li><a href="#training-your-own-model">Training your Own Model</a></li>
  </ul></li>
  <li><a href="#finetuned-models">Finetuned Models</a>
  <ul>
    <li><a href="#choosing-a-model">Choosing a Model</a></li>
    <li><a href="#other-models">Other Models</a></li>
  </ul></li>
  <li><a href="#faq">FAQ</a>
  <ul>
    <li><a href="#installation-questions">Installation questions</a></li>
    <li><a href="#usage-questions">Usage questions</a></li>
    <li><a href="#fine-tuning-questions">Fine-tuning questions</a></li>
  </ul></li>
  <li><a href="#contribution-guide">Contribution Guide</a>
  <ul>
    <li><a href="#testing">Testing</a></li>
    <li><a href="#creating-a-new-release">Creating a new release</a></li>
  </ul></li>
  <li><a href="#using-micro_sam-on-band">Using <code>micro_sam</code> on BAND</a>
  <ul>
    <li><a href="#start-band">Start BAND</a></li>
    <li><a href="#start-micro_sam-in-band">Start <code>micro_sam</code> in BAND</a></li>
    <li><a href="#transfering-data-to-band">Transfering data to BAND</a></li>
  </ul></li>
</ul>


            <h2>Submodules</h2>
            <ul>
                    <li><a href="micro_sam/bioimageio.html">bioimageio</a></li>
                    <li><a href="micro_sam/evaluation.html">evaluation</a></li>
                    <li><a href="micro_sam/inference.html">inference</a></li>
                    <li><a href="micro_sam/instance_segmentation.html">instance_segmentation</a></li>
                    <li><a href="micro_sam/multi_dimensional_segmentation.html">multi_dimensional_segmentation</a></li>
                    <li><a href="micro_sam/precompute_state.html">precompute_state</a></li>
                    <li><a href="micro_sam/prompt_based_segmentation.html">prompt_based_segmentation</a></li>
                    <li><a href="micro_sam/prompt_generators.html">prompt_generators</a></li>
                    <li><a href="micro_sam/sam_annotator.html">sam_annotator</a></li>
                    <li><a href="micro_sam/sample_data.html">sample_data</a></li>
                    <li><a href="micro_sam/training.html">training</a></li>
                    <li><a href="micro_sam/util.html">util</a></li>
                    <li><a href="micro_sam/visualization.html">visualization</a></li>
            </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
micro_sam    </h1>

                        <div class="docstring"><h1 id="segment-anything-for-microscopy">Segment Anything for Microscopy</h1>

<p>Segment Anything for Microscopy implements automatic and interactive annotation for microscopy data. It is built on top of <a href="https://segment-anything.com/">Segment Anything</a> by Meta AI and specializes it for microscopy and other biomedical imaging data.
Its core components are:</p>

<ul>
<li>The <code><a href="">micro_sam</a></code> tools for interactive data annotation, built as <a href="https://napari.org/stable/">napari</a> plugin.</li>
<li>The <code><a href="">micro_sam</a></code> library to apply Segment Anything to 2d and 3d data or fine-tune it on your data.</li>
<li>The <code><a href="">micro_sam</a></code> models that are fine-tuned on publicly available microscopy data and that are available on <a href="https://bioimage.io/#/">BioImage.IO</a>.</li>
</ul>

<p>Based on these components <code><a href="">micro_sam</a></code> enables fast interactive and automatic annotation for microscopy data, like interactive cell segmentation from bounding boxes:</p>

<p><img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d04cb158-9f5b-4460-98cd-023c4f19cccd" alt="box-prompts" /></p>

<p><code><a href="">micro_sam</a></code> is now available as stable version 1.0 and we will not change its user interface significantly in the foreseeable future.
We are still working on improving and extending its functionality. The current roadmap includes:</p>

<ul>
<li>Releasing more and better finetuned models for the biomedical imaging domain.</li>
<li>Integrating parameter efficient training and compressed models for efficient fine-tuning and faster inference.</li>
<li>Improving the 3D segmentation and tracking functionality.</li>
</ul>

<p>If you run into any problems or have questions please <a href="https://github.com/computational-cell-analytics/micro-sam/issues/new">open an issue</a> or reach out via <a href="https://forum.image.sc/">image.sc</a> using the tag <code>micro-sam</code>.</p>

<h2 id="quickstart">Quickstart</h2>

<p>You can install <code><a href="">micro_sam</a></code> via mamba:</p>

<pre><code>$ mamba install -c conda-forge micro_sam
</code></pre>

<p>We also provide installers for Windows and Linux. For more details on the available installation options, check out <a href="#installation">the installation section</a>.</p>

<p>After installing <code><a href="">micro_sam</a></code> you can start napari and select the annotation tool you want to use from <code>Plugins -&gt; SegmentAnything for Microscopy</code>. Check out the <a href="https://youtu.be/gcv0fa84mCc">quickstart tutorial video</a> for a short introduction and <a href="#annotation-tools">the annotation tool section</a> for details.</p>

<p>The <code><a href="">micro_sam</a></code> python library can be imported via</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">import</span> <span class="nn">micro_sam</span>
</code></pre>
</div>

<p>It is explained in more detail <a href="#using-the-python-library">here</a>.</p>

<p>We provide different finetuned models for microscopy that can be used within our tools or any other tool that supports Segment Anything. See <a href="#finetuned-models">finetuned models</a> for details on the available models.
You can also train models on your own data, see <a href="#training-your-own-model">here for details</a>.</p>

<h2 id="citation">Citation</h2>

<p>If you are using <code><a href="">micro_sam</a></code> in your research please cite</p>

<ul>
<li>our <a href="https://doi.org/10.1101/2023.08.21.554208">preprint</a></li>
<li>and the original <a href="https://arxiv.org/abs/2304.02643">Segment Anything publication</a>.</li>
<li>If you use a <code>vit-tiny</code> models, please also cite <a href="https://arxiv.org/abs/2306.14289">Mobile SAM</a>.</li>
</ul>

<h1 id="installation">Installation</h1>

<p>There are three ways to install <code><a href="">micro_sam</a></code>:</p>

<ul>
<li><a href="#from-mamba">From mamba</a> is the recommended way if you want to use all functionality.</li>
<li><a href="#from-source">From source</a> for setting up a development environment to use the latest version and to change and contribute to our software.</li>
<li><a href="#from-installer">From installer</a> to install it without having to use mamba (supported platforms: Windows and Linux, supports only CPU). </li>
</ul>

<p>You can find more information on the installation and how to troubleshoot it in <a href="#installation-questions">the FAQ section</a>.</p>

<h2 id="from-mamba">From mamba</h2>

<p><a href="https://mamba.readthedocs.io/en/latest/">mamba</a> is a drop-in replacement for conda, but much faster.
The steps below may also work with <code>conda</code>, but we recommend using <code>mamba</code>, especially if the installation does not work with <code>conda</code>.
You can follow the instructions <a href="https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html">here</a> to install <code>mamba</code>.</p>

<p><strong>IMPORTANT</strong>: Make sure to avoid installing anything in the base environment.</p>

<p><code><a href="">micro_sam</a></code> can be installed in an existing environment via:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>mamba<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>micro_sam
</code></pre>
</div>

<p>or you can create a new environment (here called <code>micro-sam</code>) via:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>mamba<span class="w"> </span>create<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-n<span class="w"> </span>micro-sam<span class="w"> </span>micro_sam
</code></pre>
</div>

<p>if you want to use the GPU you need to install PyTorch from the <code>pytorch</code> channel instead of <code>conda-forge</code>. For example:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>mamba<span class="w"> </span>create<span class="w"> </span>-c<span class="w"> </span>pytorch<span class="w"> </span>-c<span class="w"> </span>nvidia<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-n<span class="w"> </span>micro-sam<span class="w"> </span>micro_sam<span class="w"> </span>pytorch<span class="w"> </span>pytorch-cuda<span class="o">=</span><span class="m">12</span>.1
</code></pre>
</div>

<p>You may need to change this command to install the correct CUDA version for your system, see <a href="https://pytorch.org/">https://pytorch.org/</a> for details.</p>

<h2 id="from-source">From source</h2>

<p>To install <code><a href="">micro_sam</a></code> from source, we recommend to first set up an environment with the necessary requirements:</p>

<ul>
<li><a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_gpu.yaml">environment_gpu.yaml</a>: sets up an environment with GPU support.</li>
<li><a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_cpu.yaml">environment_cpu.yaml</a>: sets up an environment with CPU support.</li>
</ul>

<p>To create one of these environments and install <code><a href="">micro_sam</a></code> into it follow these steps</p>

<ol>
<li>Clone the repository:</li>
</ol>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/computational-cell-analytics/micro-sam
</code></pre>
</div>

<ol start="2">
<li>Enter it:</li>
</ol>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>micro-sam
</code></pre>
</div>

<ol start="3">
<li>Create the GPU or CPU environment:</li>
</ol>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>mamba<span class="w"> </span>env<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>&lt;ENV_FILE&gt;.yaml
</code></pre>
</div>

<ol start="4">
<li>Activate the environment:</li>
</ol>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>mamba<span class="w"> </span>activate<span class="w"> </span>sam
</code></pre>
</div>

<ol start="5">
<li>Install <code><a href="">micro_sam</a></code>:</li>
</ol>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</code></pre>
</div>

<h2 id="from-installer">From installer</h2>

<p>We also provide installers for Linux and Windows:</p>

<ul>
<li><a href="https://owncloud.gwdg.de/index.php/s/nrNBuHr9ncJqid6">Linux</a></li>
<li><a href="https://owncloud.gwdg.de/index.php/s/kZmpAIBDmUSu4e9">Windows</a>
<!---</li>
<li><a href="https://owncloud.gwdg.de/index.php/s/7YupGgACw9SHy2P">Mac</a>
--></li>
</ul>

<p>The installers will not enable you to use a GPU, so if you have one then please consider installing <code><a href="">micro_sam</a></code> via <a href="#from-mamba">mamba</a> instead. They will also not enable using the python library.</p>

<h3 id="linux-installer">Linux Installer:</h3>

<p>To use the installer:</p>

<ul>
<li>Unpack the zip file you have downloaded.</li>
<li>Make the installer executable: <code>$ chmod +x micro_sam-1.0.0post0-Linux-x86_64.sh</code></li>
<li>Run the installer: <code>./micro_sam-1.0.0post0-Linux-x86_64.sh</code> 
<ul>
<li>You can select where to install <code><a href="">micro_sam</a></code> during the installation. By default it will be installed in <code>$HOME/micro_sam</code>.</li>
<li>The installer will unpack all <code><a href="">micro_sam</a></code> files to the installation directory.</li>
</ul></li>
<li>After the installation you can start the annotator with the command <code>.../micro_sam/bin/napari</code>.
<ul>
<li>Proceed with the steps described in <a href="#annotation-tools">Annotation Tools</a></li>
<li>To make it easier to run the annotation tool you can add <code>.../micro_sam/bin</code> to your <code>PATH</code> or set a softlink to <code>.../micro_sam/bin/napari</code>.</li>
</ul></li>
</ul>

<h3 id="windows-installer">Windows Installer:</h3>

<ul>
<li>Unpack the zip file you have downloaded.</li>
<li>Run the installer by double clicking on it.</li>
<li>Choose installation type: <code>Just Me(recommended)</code> or <code>All Users(requires admin privileges)</code>.</li>
<li>Choose installation path. By default it will be installed in <code>C:\Users\&lt;Username&gt;\micro_sam</code> for <code>Just Me</code> installation or in <code>C:\ProgramData\micro_sam</code> for <code>All Users</code>.
<ul>
<li>The installer will unpack all micro_sam files to the installation directory.</li>
</ul></li>
<li>After the installation you can start the annotator by double clicking on <code>.\micro_sam\Scripts\micro_sam.annotator.exe</code> or  with the command <code>.\micro_sam\Scripts\napari.exe</code> from the Command Prompt.</li>
<li>Proceed with the steps described in <a href="#annotation-tools">Annotation Tools</a> </li>
</ul>

<p><!---
<strong>Mac Installer:</strong></p>

<p>To use the Mac installer you will need to enable installing unsigned applications. Please follow <a href="https://disable-gatekeeper.github.io/">the instructions for 'Disabling Gatekeeper for one application only' here</a>.</p>

<p>Alternative link on how to disable gatekeeper.
<a href="https://www.makeuseof.com/how-to-disable-gatekeeper-mac/">https://www.makeuseof.com/how-to-disable-gatekeeper-mac/</a></p>

<p>TODO detailed instruction
--></p>

<h1 id="annotation-tools">Annotation Tools</h1>

<p><code><a href="">micro_sam</a></code> provides applications for fast interactive 2d segmentation, 3d segmentation and tracking.
See an example for interactive cell segmentation in phase-contrast microscopy (left), interactive segmentation
of mitochondria in volume EM (middle) and interactive tracking of cells (right).</p>

<p><img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d5ee2080-ab08-4716-b4c4-c169b4ed29f5" width="256">
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/dfca3d9b-dba5-440b-b0f9-72a0683ac410" width="256">
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/aefbf99f-e73a-4125-bb49-2e6592367a64" width="256"></p>

<p>The annotation tools can be started from the napari plugin menu, the command line or from python scripts.
They are built as napari plugin and make use of existing napari functionality wherever possible. If you are not familiar with napari, we recommend to <a href="https://napari.org/stable/tutorials/fundamentals/quick_start.html">start here</a>.
The <code><a href="">micro_sam</a></code> tools mainly use the <a href="https://napari.org/stable/howtos/layers/points.html">point layer</a>, <a href="https://napari.org/stable/howtos/layers/shapes.html">shape layer</a> and <a href="https://napari.org/stable/howtos/layers/labels.html">label layer</a>.</p>

<p>The annotation tools are explained in detail below. We also provide <a href="https://youtube.com/playlist?list=PLwYZXQJ3f36GQPpKCrSbHjGiH39X4XjSO&si=qNbB8IFXqAX33r_Z">video tutorials</a>.</p>

<p>The annotation tools can be started from the napari plugin menu:
<img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/napari-plugin.png" width="768"></p>

<p>You can find additional information on the annotation tools <a href="#usage-question">in the FAQ section</a>.</p>

<h2 id="annotator-2d">Annotator 2D</h2>

<p>The 2d annotator can be started by</p>

<ul>
<li>clicking <code>Annotator 2d</code> in the plugin menu.</li>
<li>running <code>$ micro_sam.annotator_2d</code> in the command line.</li>
<li>calling <code><a href="micro_sam/sam_annotator/annotator_2d.html">micro_sam.sam_annotator.annotator_2d</a></code> in a python script. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py">examples/annotator_2d.py</a> for details.</li>
</ul>

<p>The user interface of the 2d annotator looks like this:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/2d-annotator-menu.png" width="1024"></p>

<p>It contains the following elements:</p>

<ol>
<li>The napari layers for the segmentations and prompts:
<ul>
<li><code>prompts</code>: shape layer that is used to provide box prompts to Segment Anything. Prompts can be given as rectangle (marked as box prompt in the image), ellipse or polygon.</li>
<li><code>point_prompts</code>: point layer that is used to provide point prompts to Segment Anything. Positive prompts (green points) for marking the object you want to segment, negative prompts (red points) for marking the outside of the object.</li>
<li><code>committed_objects</code>: label layer with the objects that have already been segmented.</li>
<li><code>auto_segmentation</code>: label layer with the results from automatic instance segmentation.</li>
<li><code>current_object</code>: label layer for the object(s) you're currently segmenting.</li>
</ul></li>
<li>The embedding menu. For selecting the image to process, the Segment Anything model that is used and computing its image embeddings. The <code>Embedding Settings</code> contain advanced settings for loading cached embeddings from file or for using tiled embeddings.</li>
<li>The prompt menu for changing whether the currently selected point is a positive or a negative prompt. This can also be done by pressing <code>T</code>.</li>
<li>The menu for interactive segmentation. Clicking <code>Segment Object</code> (or pressing <code>S</code>) will run segmentation for the current prompts. The result is displayed in <code>current_object</code>. Activating <code>batched</code> enables segmentation of multiple objects with point prompts. In this case one object will be segmented per positive prompt.</li>
<li>The menu for automatic segmentation. Clicking <code>Automatic Segmentation</code> will segment all objects n the image. The results will be displayed in the <code>auto_segmentation</code> layer. We support two different methods for automatic segmentation: automatic mask generation (supported for all models) and instance segmentation with an additional decoder (only supported for our models).
Changing the parameters under <code>Automatic Segmentation Settings</code> controls the segmentation results, check the tooltips for details.</li>
<li>The menu for commiting the segmentation. When clicking <code>Commit</code> (or pressing <code>C</code>) the result from the selected layer (either <code>current_object</code> or <code>auto_segmentation</code>) will be transferred from the respective layer to <code>committed_objects</code>.
When <code>commit_path</code> is given the results will automatically be saved there.</li>
<li>The menu for clearing the current annotations. Clicking <code>Clear Annotations</code> (or pressing <code>Shift + C</code>) will clear the current annotations and the current segmentation.</li>
</ol>

<p>Point prompts and box prompts can be combined. When you're using point prompts you can only segment one object at a time, unless the <code>batched</code> mode is activated. With box prompts you can segment several objects at once, both in the normal and <code>batched</code> mode.</p>

<p>Check out <a href="https://youtu.be/9xjJBg_Bfuc">the video tutorial</a> for an in-depth explanation on how to use this tool.</p>

<h2 id="annotator-3d">Annotator 3D</h2>

<p>The 3d annotator can be started by</p>

<ul>
<li>clicking <code>Annotator 3d</code> in the plugin menu.</li>
<li>running <code>$ micro_sam.annotator_3d</code> in the command line.</li>
<li>calling <code><a href="micro_sam/sam_annotator/annotator_3d.html">micro_sam.sam_annotator.annotator_3d</a></code> in a python script. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_3d.py">examples/annotator_3d.py</a> for details.</li>
</ul>

<p>The user interface of the 3d annotator looks like this:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/3d-annotator-menu.png" width="1024"></p>

<p>Most elements are the same as in <a href="#annotator-2d">the 2d annotator</a>:</p>

<ol>
<li>The napari layers that contain the segmentations and prompts.</li>
<li>The embedding menu.</li>
<li>The prompt menu.</li>
<li>The menu for interactive segmentation in the current slice.</li>
<li>The menu for interactive 3d segmentation. Clicking <code>Segment All Slices</code> (or pressing <code>Shift + S</code>) will extend the segmentation of the current object across the volume by projecting prompts across slices. The parameters for prompt projection can be set in <code>Segmentation Settings</code>, please refer to the tooltips for details.</li>
<li>The menu for automatic segmentation. The overall functionality is the same as <a href="#annotator-2d">for the 2d annotator</a>. To segment the full volume <code>Apply to Volume</code> needs to be checked, otherwise only the current slice will be segmented. Note that 3D segmentation can take quite long without a GPU.</li>
<li>The menu for committing the current object.</li>
<li>The menu for clearing the current annotations. If <code>all slices</code> is set all annotations will be cleared, otherwise they are only cleared for the current slice.</li>
</ol>

<p>You can only segment one object at a time using the interactive segmentation functionality with this tool.</p>

<p>Check out <a href="https://youtu.be/nqpyNQSyu74">the video tutorial</a> for an in-depth explanation on how to use this tool.</p>

<h2 id="annotator-tracking">Annotator Tracking</h2>

<p>The tracking annotator can be started by</p>

<ul>
<li>clicking <code>Annotator Tracking</code> in the plugin menu.</li>
<li>running <code>$ micro_sam.annotator_tracking</code> in the command line.</li>
<li>calling <code><a href="micro_sam/sam_annotator/annotator_tracking.html">micro_sam.sam_annotator.annotator_tracking</a></code> in a python script. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_tracking.py">examples/annotator_tracking.py</a> for details. </li>
</ul>

<p>The user interface of the tracking annotator looks like this:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/tracking-annotator-menu.png" width="1024"></p>

<p>Most elements are the same as in <a href="#annotator-2d">the 2d annotator</a>:</p>

<ol>
<li>The napari layers that contain the segmentations and prompts. Same as for <a href="#annotator-2d">the 2d segmentation application</a> but without the <code>auto_segmentation</code> layer.</li>
<li>The embedding menu.</li>
<li>The prompt menu.</li>
<li>The menu with tracking settings: <code>track_state</code> is used to indicate that the object you are tracking is dividing in the current frame. <code>track_id</code> is used to select which of the tracks after division you are following.</li>
<li>The menu for interactive segmentation in the current frame.</li>
<li>The menu for interactive tracking. Click <code>Track Object</code> (or press <code>Shift + S</code>) to segment the current object across time.</li>
<li>The menu for committing the current tracking result.</li>
<li>The menu for clearing the current annotations.</li>
</ol>

<p>The tracking annotator only supports 2d image data with a time dimension, volumetric data + time is not supported. We also do not support automatic tracking yet.</p>

<p>Check out <a href="https://youtu.be/1gg8OPHqOyc">the video tutorial</a> for an in-depth explanation on how to use this tool.</p>

<h2 id="image-series-annotator">Image Series Annotator</h2>

<p>The image series annotation tool enables running the <a href="#annotator-2d">2d annotator</a> or <a href="#annotator-3d">3d annotator</a> for multiple images that are saved in a folder. This makes it convenient to annotate many images without having to restart the tool for every image. It can be started by</p>

<ul>
<li>clicking <code>Image Series Annotator</code> in the plugin menu.</li>
<li>running <code>$ micro_sam.image_series_annotator</code> in the command line.</li>
<li>calling <code><a href="micro_sam/sam_annotator/image_series_annotator.html">micro_sam.sam_annotator.image_series_annotator</a></code> in a python script. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/image_series_annotator.py">examples/image_series_annotator.py</a> for details. </li>
</ul>

<p>When starting this tool via the plugin menu the following interface opens:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/series-menu.png" width="512"></p>

<p>You can select the folder where your images are saved with <code>Input Folder</code>. The annotation results will be saved in <code>Output Folder</code>.
You can specify a rule for loading only a subset of images via <code>pattern</code>, for example <code>*.tif</code> to only load tif images. Set <code>is_volumetric</code> if the data you want to annotate is 3d. The rest of the options are settings for the image embedding computation and are the same as for the embedding menu (see above).
Once you click <code>Annotate Images</code> the images from the folder you have specified will be loaded and the annotation tool is started for them.</p>

<p>This menu will not open if you start the image series annotator from the command line or via python. In this case the input folder and other settings are passed as parameters instead.</p>

<p>Check out <a href="https://youtu.be/HqRoImdTX3c">the video tutorial</a> for an in-depth explanation on how to use the image series annotator.</p>

<h2 id="finetuning-ui">Finetuning UI</h2>

<p>We also provide a graphical inferface for fine-tuning models on your own data. It can be started by clicking <code>Finetuning</code> in the plugin menu.</p>

<p><strong>Note:</strong> if you know a bit of python programming we recommend to use a script for model finetuning instead. This will give you more options to configure the training. See <a href="#training-your-own-model">these instructions</a> for details.</p>

<p>When starting this tool via the plugin menu the following interface opens:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/finetuning-menu.png" width="512"></p>

<p>You can select the image data via <code>Path to images</code>. You can either load images from a folder or select a single image file. By providing <code>Image data key</code> you can either provide a pattern for selecting files from the folder or provide an internal filepath for HDF5, Zarr or similar fileformats.</p>

<p>You can select the label data via <code>Path to labels</code> and <code>Label data key</code>, following the same logic as for the image data. The label masks are expected to have the same size as the image data. You can for example use annotations created with one of the <code><a href="">micro_sam</a></code> annotation tools for this, they are stored in the correct format. See <a href="#fine-tuning-questions">the FAQ</a> for more details on the expected label data.</p>

<p>The <code>Configuration</code> option allows you to choose the hardware configuration for training. We try to automatically select the correct setting for your system, but it can also be changed. Details on the configurations can be found <a href="#training-your-own-model">here</a>.</p>

<h1 id="using-the-python-library">Using the Python Library</h1>

<p>The python library can be imported via</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">import</span> <span class="nn">micro_sam</span>
</code></pre>
</div>

<p>This library extends the <a href="https://github.com/facebookresearch/segment-anything">Segment Anything library</a> and</p>

<ul>
<li>implements functions to apply Segment Anything to 2d and 3d data in <code><a href="micro_sam/prompt_based_segmentation.html">micro_sam.prompt_based_segmentation</a></code>.</li>
<li>provides improved automatic instance segmentation functionality in <code><a href="micro_sam/instance_segmentation.html">micro_sam.instance_segmentation</a></code>.</li>
<li>implements training functionality that can be used for finetuning Segment Anything on your own data in <code><a href="micro_sam/training.html">micro_sam.training</a></code>.</li>
<li>provides functionality for quantitative and qualitative evaluation of Segment Anything models in <code><a href="micro_sam/evaluation.html">micro_sam.evaluation</a></code>.</li>
</ul>

<p>You can import these sub-modules via</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="kn">import</span> <span class="nn"><a href="micro_sam/prompt_based_segmentation.html">micro_sam.prompt_based_segmentation</a></span>
<span class="kn">import</span> <span class="nn"><a href="micro_sam/instance_segmentation.html">micro_sam.instance_segmentation</a></span>
<span class="c1"># etc.</span>
</code></pre>
</div>

<p>This functionality is used to implement the interactive annotation tools in <code><a href="micro_sam/sam_annotator.html">micro_sam.sam_annotator</a></code> and can be used as a standalone python library.
We provide jupyter notebooks that demonstrate how to use it <a href="https://github.com/computational-cell-analytics/micro-sam/tree/master/notebooks">here</a>. You can find the full library documentation by scrolling to the end of this page. </p>

<h2 id="training-your-own-model">Training your Own Model</h2>

<p>We reimplement the training logic described in the <a href="https://arxiv.org/abs/2304.02643">Segment Anything publication</a> to enable finetuning on custom data.
We use this functionality to provide the <a href="#finetuned-models">finetuned microscopy models</a> and it can also be used to train models on your own data.
In fact the best results can be expected when finetuning on your own data, and we found that it does not require much annotated training data to get significant improvements in model performance.
So a good strategy is to annotate a few images with one of the provided models using our interactive annotation tools and, if the model is not working as good as required for your use-case, finetune on the annotated data.
We recommend checking out our latest <a href="https://doi.org/10.1101/2023.08.21.554208">preprint</a> for details on the results on how much data is required for finetuning Segment Anything.</p>

<p>The training logic is implemented in <code><a href="micro_sam/training.html">micro_sam.training</a></code> and is based on <a href="https://github.com/constantinpape/torch-em">torch-em</a>. Check out <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb">the finetuning notebook</a> to see how to use it.
We also support training an additional decoder for automatic instance segmentation. This yields better results than the automatic mask generation of segment anything and is significantly faster.
The notebook explains how to train it together with the rest of SAM and how to then use it.</p>

<p>More advanced examples, including quantitative and qualitative evaluation, can be found in <a href="https://github.com/computational-cell-analytics/micro-sam/tree/master/finetuning">the finetuning directory</a>, which contains the code for training and evaluating <a href="finetuned-models">our models</a>. You can find further information on model training in the <a href="fine-tuning-questions">FAQ section</a>.</p>

<p>Here is a list of resources, together with their recommended training settings, for which we have tested model finetuning:</p>

<table>
<thead>
<tr>
  <th>Resource Name</th>
  <th>Capacity</th>
  <th>Model Type</th>
  <th>Batch Size</th>
  <th>Finetuned Parts</th>
  <th>Number of Objects</th>
</tr>
</thead>
<tbody>
<tr>
  <td>CPU</td>
  <td>32GB</td>
  <td>ViT Base</td>
  <td>1</td>
  <td><em>all</em></td>
  <td>10</td>
</tr>
<tr>
  <td>CPU</td>
  <td>64GB</td>
  <td>ViT Base</td>
  <td>1</td>
  <td><em>all</em></td>
  <td>15</td>
</tr>
<tr>
  <td>GPU (NVIDIA GTX 1080Ti)</td>
  <td>8GB</td>
  <td>ViT Base</td>
  <td>1</td>
  <td>Mask Decoder, Prompt Encoder</td>
  <td>10</td>
</tr>
<tr>
  <td>GPU (NVIDIA Quadro RTX5000)</td>
  <td>16GB</td>
  <td>ViT Base</td>
  <td>1</td>
  <td><em>all</em></td>
  <td>10</td>
</tr>
<tr>
  <td>GPU (Tesla V100)</td>
  <td>32GB</td>
  <td>ViT Base</td>
  <td>1</td>
  <td><em>all</em></td>
  <td>10</td>
</tr>
<tr>
  <td>GPU (NVIDIA A100)</td>
  <td>80GB</td>
  <td>ViT Tiny</td>
  <td>2</td>
  <td><em>all</em></td>
  <td>50</td>
</tr>
<tr>
  <td>GPU (NVIDIA A100)</td>
  <td>80GB</td>
  <td>ViT Base</td>
  <td>2</td>
  <td><em>all</em></td>
  <td>40</td>
</tr>
<tr>
  <td>GPU (NVIDIA A100)</td>
  <td>80GB</td>
  <td>ViT Large</td>
  <td>2</td>
  <td><em>all</em></td>
  <td>30</td>
</tr>
<tr>
  <td>GPU (NVIDIA A100)</td>
  <td>80GB</td>
  <td>ViT Huge</td>
  <td>2</td>
  <td><em>all</em></td>
  <td>25</td>
</tr>
</tbody>
</table>

<blockquote>
  <p>NOTE: If you use the <a href="#finetuning-ui">finetuning UI</a> or <code><a href="micro_sam/training/training.html#train_sam_for_configuration">micro_sam.training.training.train_sam_for_configuration</a></code> you can specify the hardware configuration and the best settings for it will be set automatically. If your hardware is not in the settings we have tested choose the closest match. You can set the training parameters yourself when using <code><a href="micro_sam/training/training.html#train_sam">micro_sam.training.training.train_sam</a></code>. Be aware that the choice for the number of objects per image, the batch size, and the type of model have a strong impact on the VRAM needed for training and the duration of training. See the <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb">finetuning notebook</a> for an overview of these parameters.</p>
</blockquote>

<h1 id="finetuned-models">Finetuned Models</h1>

<p>In addition to the original Segment Anything models, we provide models that are finetuned on microscopy data.
They are available in the <a href="https://bioimage.io/#/">BioImage.IO Model Zoo</a> and are also hosted on Zenodo.</p>

<p>We currently offer the following models:</p>

<ul>
<li><code>vit_h</code>: Default Segment Anything model with ViT Huge backbone.</li>
<li><code>vit_l</code>: Default Segment Anything model with ViT Large backbone.</li>
<li><code>vit_b</code>: Default Segment Anything model with ViT Base backbone.</li>
<li><code>vit_t</code>: Segment Anything model with ViT Tiny backbone. From the <a href="https://arxiv.org/abs/2306.14289">Mobile SAM publication</a>.</li>
<li><code>vit_l_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with ViT Large backbone. (<a href="https://doi.org/10.5281/zenodo.11111176">Zenodo</a>) (<a href="TODO">idealistic-rat on BioImage.IO</a>)</li>
<li><code>vit_b_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with ViT Base backbone. (<a href="https://zenodo.org/doi/10.5281/zenodo.11103797">Zenodo</a>) (<a href="TODO">diplomatic-bug on BioImage.IO</a>)</li>
<li><code>vit_t_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with ViT Tiny backbone. (<a href="https://doi.org/10.5281/zenodo.11111328">Zenodo</a>) (<a href="TODO">faithful-chicken BioImage.IO</a>)</li>
<li><code>vit_l_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with ViT Large backbone. (<a href="https://doi.org/10.5281/zenodo.11111054">Zenodo</a>) (<a href="TODO">humorous-crab on BioImage.IO</a>)</li>
<li><code>vit_b_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with ViT Base backbone. (<a href="https://doi.org/10.5281/zenodo.11111293">Zenodo</a>) (<a href="TODO">noisy-ox on BioImage.IO</a>)</li>
<li><code>vit_t_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with ViT Tiny backbone. (<a href="https://doi.org/10.5281/zenodo.11110950">Zenodo</a>) (<a href="TODO">greedy-whale on BioImage.IO</a>)</li>
</ul>

<p>See the two figures below of the improvements through the finetuned model for LM and EM data. </p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/lm_comparison.png" width="768"></p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/em_comparison.png" width="768"></p>

<p>You can select which model to use in the <a href="#annotation-tools">annotation tools</a> by selecting the corresponding name in the <code>Model:</code> drop-down menu in the embedding menu:</p>

<p><img src="https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/model-type-selector.png" width="256"></p>

<p>To use a specific model in the python library you need to pass the corresponding name as value to the <code>model_type</code> parameter exposed by all relevant functions.
See for example the <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L62">2d annotator example</a>.</p>

<h2 id="choosing-a-model">Choosing a Model</h2>

<p>As a rule of thumb:</p>

<ul>
<li>Use the <code>vit_l_lm</code> or <code>vit_b_lm</code> model for segmenting cells or nuclei in light microscopy. The larger model (<code>vit_l_lm</code>) yields a bit better segmentation quality, especially for automatic segmentation, but needs more computational resources.</li>
<li>Use the <code>vit_l_em_organelles</code> or <code>vit_b_em_organelles</code> models for segmenting mitochondria, nuclei or other  roundish organelles in electron microscopy.</li>
<li>For other use-cases use one of the default models.</li>
<li>The <code>vit_t_...</code> models run much faster than other models, but yield inferior quality for many applications. It can still make sense to try them for your use-case if your working on a laptop and want to annotate many images or volumetric data. </li>
</ul>

<p>See also the figures above for examples where the finetuned models work better than the default models.
We are working on further improving these models and adding new models for other biomedical imaging domains.</p>

<h2 id="other-models">Other Models</h2>

<p>Previous versions of our models are available on Zenodo:</p>

<ul>
<li><a href="https://zenodo.org/records/10524894">vit_b_em_boundaries</a>: for segmenting compartments delineated by boundaries such as cells or neurites in EM.</li>
<li><a href="https://zenodo.org/records/10524828">vit_b_em_organelles</a>: for segmenting mitochondria, nuclei or other organelles in EM.</li>
<li><a href="https://zenodo.org/records/10524791">vit_b_lm</a>: for segmenting cells and nuclei in LM.</li>
<li><a href="https://zenodo.org/records/8250291">vit_h_em</a>: for general EM segmentation.</li>
<li><a href="https://zenodo.org/records/8250299">vit_h_lm</a>: for general LM segmentation.</li>
</ul>

<p>We do not recommend to use these models since our new models improve upon them significantly. But we provide the links here in case they are needed to reproduce older segmentation workflows.</p>

<p>We provide additional models that were used for experiments in our publication on Zenodo:</p>

<ul>
<li><a href="https://doi.org/10.5281/zenodo.11115426">LIVECell Specialist Models</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11115998">TissueNet Specialist Models</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11116407">NeurIPS CellSeg Specialist Models</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11115827">DeepBacs Specialist Models</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11116603">PlantSeg (Root) Specialist Models</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11117314">CREMI Specialist Models</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11117144">ASEM (ER) Specialist Models</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11117559">The LM Generalist Model with ViT-H backend (vit_h_lm)</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11117495">The EM Generalist Model with ViT-H backend (vit_h_em_organelles)</a></li>
<li><a href="https://doi.org/10.5281/zenodo.11117615">Finetuned Models for the user studies</a></li>
</ul>

<h1 id="faq">FAQ</h1>

<p>Here we provide frequently asked questions and common issues.
If you encounter a problem or question not addressed here feel free to <a href="https://github.com/computational-cell-analytics/micro-sam/issues/new">open an issue</a> or to ask your question on <a href="https://forum.image.sc/">image.sc</a> with the tag <code>micro-sam</code>.</p>

<h2 id="installation-questions">Installation questions</h2>

<h3 id="1-how-to-install-micro_sam">1. How to install <code><a href="">micro_sam</a></code>?</h3>

<p>The <a href="https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation">installation</a> for <code><a href="">micro_sam</a></code> is supported in three ways: <a href="https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-mamba">from mamba</a> (recommended), <a href="https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-source">from source</a> and <a href="https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-installer">from installers</a>. Check out our <a href="https://youtu.be/gcv0fa84mCc">tutorial video</a> to get started with <code><a href="">micro_sam</a></code>, briefly walking you through the installation process and how to start the tool.</p>

<h3 id="2-i-cannot-install-micro_sam-using-the-installer-i-am-getting-some-errors">2. I cannot install <code><a href="">micro_sam</a></code> using the installer, I am getting some errors.</h3>

<p>The installer should work out-of-the-box on Windows and Linux platforms. Please open an issue to report the error you encounter.</p>

<blockquote>
  <p>NOTE: The installers enable using <code><a href="">micro_sam</a></code> without mamba or conda. However, we recommend the installation from mamba / from source to use all its features seamlessly. Specifically, the installers currently only support the CPU and won't enable you to use the GPU (if you have one). </p>
</blockquote>

<h3 id="3-what-is-the-minimum-system-requirement-for-micro_sam">3. What is the minimum system requirement for <code><a href="">micro_sam</a></code>?</h3>

<p>From our experience, the <code><a href="">micro_sam</a></code> annotation tools work seamlessly on most laptop or workstation CPUs and with &gt; 8GB RAM.
You might encounter some slowness for $\leq$ 8GB RAM. The resources <code><a href="">micro_sam</a></code>'s annotation tools have been tested on are:</p>

<ul>
<li>Windows:
<ul>
<li>Windows 10 Pro, Intel i5 7th Gen, 8GB RAM</li>
<li>Windows 10 Enterprise LTSC, Intel i7 13th Gen, 32GB RAM</li>
<li>Windows 10 Pro for Workstations, Intel Xeon W-2295, 128GB RAM</li>
</ul></li>
</ul>

<ul>
<li><p>Linux:</p>

<ul>
<li>Ubuntu 20.04, Intel i7 11th Gen, 32GB RAM</li>
<li>Ubuntu 22.04, Intel i7 12th Gen, 32GB RAM</li>
</ul></li>
<li><p>Mac:</p>

<ul>
<li>macOS Sonoma 14.4.1
<ul>
<li>M1 Chip, 8GB RAM</li>
<li>M3 Max Chip, 36GB RAM</li>
</ul></li>
</ul></li>
</ul>

<p>Having a GPU will significantly speed up the annotation tools and especially the model finetuning.</p>

<h3 id="4-what-is-the-recommended-pytorch-version">4. What is the recommended PyTorch version?</h3>

<p><code><a href="">micro_sam</a></code> has been tested mostly with CUDA 12.1 and PyTorch [2.1.1, 2.2.0]. However, the tool and the library is not constrained to a specific PyTorch or CUDA version. So it should work fine with the standard PyTorch installation for your system.</p>

<h3 id="5-i-am-missing-a-few-packages-eg-modulenotfounderror-no-module-named-elfio-what-should-i-do">5. I am missing a few packages (eg. <code>ModuleNotFoundError: No module named 'elf.io</code>). What should I do?</h3>

<p>With the latest release 1.0.0, the installation from mamba and source should take care of this and install all the relevant packages for you.
So please reinstall <code><a href="">micro_sam</a></code>.</p>

<h3 id="6-can-i-install-micro_sam-using-pip">6. Can I install <code><a href="">micro_sam</a></code> using pip?</h3>

<p>The installation is not supported via pip.</p>

<h3 id="7-i-get-the-following-error-importerror-cannot-import-name-unetr-from-torch_emmodel">7. I get the following error: <code>importError: cannot import name 'UNETR' from 'torch_em.model'</code>.</h3>

<p>It's possible that you have an older version of <code>torch-em</code> installed. Similar errors could often be raised from other libraries, the reasons being: a) Outdated packages installed, or b) Some non-existent module being called. If the source of such error is from <code><a href="">micro_sam</a></code>, then <code>a)</code> is most likely the reason . We recommend installing the latest version following the <a href="https://github.com/constantinpape/torch-em?tab=readme-ov-file#installation">installation instructions</a>.</p>

<h2 id="usage-questions">Usage questions</h2>

<p><!---
TODO provide relevant links here.
--></p>

<h3 id="1-i-have-some-micropscopy-images-can-i-use-the-annotator-tool-for-segmenting-them">1. I have some micropscopy images. Can I use the annotator tool for segmenting them?</h3>

<p>Yes, you can use the annotator tool for:</p>

<ul>
<li>Segmenting objects in 2d images (using automatic and/or interactive segmentation).</li>
<li>Segmenting objects in 3d volumes (using automatic and/or interactive segmentation for the entire object(s)).</li>
<li>Tracking objects over time in time-series data.</li>
<li>Segmenting objects in a series of 2d / 3d images.</li>
<li>In addition, you can finetune the Segment Anything / <code><a href="">micro_sam</a></code> models on your own microscopy data, in case the provided models do not suffice your needs. One caveat: You need to annotate a few objects before-hand (<code><a href="">micro_sam</a></code> has the potential of improving interactive segmentation with only a few annotated objects) to proceed with the supervised finetuning procedure.</li>
</ul>

<h3 id="2-which-model-should-i-use-for-my-data">2. Which model should I use for my data?</h3>

<p>We currently provide three different kind of models: the default models <code>vit_h</code>, <code>vit_l</code>, <code>vit_b</code> and <code>vit_t</code>; the models for light microscopy <code>vit_l_lm</code>, <code>vit_b_lm</code> and <code>vit_t_lm</code>; the models for electron microscopy <code>vit_l_em_organelles</code>, <code>vit_b_em_organelles</code> and <code>vit_t_em_organelles</code>.
You should first try the model that best fits the segmentation task your interested in, the <code>lm</code> model for cell or nucleus segmentation in light microscopy or the <code>em_organelles</code> model for segmenting nuclei, mitochondria or other roundish organelles in electron microscopy.
If your segmentation problem does not meet these descriptions, or if these models don't work well, you should try one of the default models instead.
The letter after <code>vit</code> denotes the size of the image encoder in SAM, <code>h</code> (huge) being the largest and <code>t</code> (tiny) the smallest. The smaller models are faster but may yield worse results. We recommend to either use a <code>vit_l</code> or <code>vit_b</code> model, they offer the best trade-off between speed and segmentation quality.
You can find more information on model choice <a href="#choosing-a-model">here</a>.</p>

<h3 id="3-i-have-high-resolution-microscopy-images-micro_sam-does-not-seem-to-work">3. I have high-resolution microscopy images, <code><a href="">micro_sam</a></code> does not seem to work.</h3>

<p>The Segment Anything model expects inputs of shape 1024 x 1024 pixels. Inputs that do not match this size will be internally resized to match it. Hence, applying Segment Anything to a much larger image will often lead to inferior results, or sometimes not work at all. To address this, <code><a href="">micro_sam</a></code> implements tiling: cutting up the input image into tiles of a fixed size (with a fixed overlap) and running Segment Anything for the individual tiles. You can activate tiling with the <code>tile_shape</code> parameter, which determines the size of the inner tile and <code>halo</code>, which determines the size of the additional overlap.</p>

<ul>
<li>If you are using the <code><a href="">micro_sam</a></code> annotation tools, you can specify the values for the <code>tile_shape</code> and <code>halo</code> via the <code>tile_x</code>, <code>tile_y</code>, <code>halo_x</code> and <code>halo_y</code> parameters in the <code>Embedding Settings</code> drop-down menu.</li>
<li>If you are using the <code><a href="">micro_sam</a></code> library in a python script, you can pass them as tuples, e.g. <code>tile_shape=(1024, 1024), halo=(256, 256)</code>. See also the <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L47-L63">wholeslide annotator example</a>.</li>
<li>If you are using the command line functionality, you can pass them via the options <code>--tile_shape 1024 1024 --halo 256 256</code>.</li>
</ul>

<blockquote>
  <p>NOTE: It's recommended to choose the <code>halo</code> so that it is larger than half of the maximal radius of the objects you want to segment.</p>
</blockquote>

<h3 id="4-the-computation-of-image-embeddings-takes-very-long-in-napari">4. The computation of image embeddings takes very long in napari.</h3>

<p><code><a href="">micro_sam</a></code> pre-computes the image embeddings produced by the vision transformer backbone in Segment Anything, and (optionally) stores them on disc. I fyou are using a CPU, this step can take a while for 3d data or time-series (you will see a progress bar in the command-line interface / on the bottom right of napari). If you have access to a GPU without graphical interface (e.g. via a local computer cluster or a cloud provider), you can also pre-compute the embeddings there and then copy them over to your laptop / local machine to speed this up.</p>

<ul>
<li>You can use the command <code>micro_sam.precompute_embeddings</code> for this (it is installed with the rest of the software). You can specify the location of the pre-computed embeddings via the <code>embedding_path</code> argument.</li>
<li>You can cache the computed embedding in the napari tool (to avoid recomputing the embeddings again) by passing the path to store the embeddings in the <code>embeddings_save_path</code> option in the <code>Embedding Settings</code> drop-down. You can later load the pre-computed image embeddings by entering the path to the stored embeddings there as well.</li>
</ul>

<h3 id="5-can-i-use-micro_sam-on-a-cpu">5. Can I use <code><a href="">micro_sam</a></code> on a CPU?</h3>

<p>Most other processing steps are very fast even on a CPU, the automatic segmentation step for the default Segment Anything models (typically called as the "Segment Anything" feature or AMG - Automatic Mask Generation) however takes several minutes without a GPU (depending on the image size). For large volumes and time-series, segmenting an object interactively in 3d / tracking across time can take a couple of seconds with a CPU (it is very fast with a GPU).</p>

<blockquote>
  <p>HINT: All the tutorial videos have been created on CPU resources.</p>
</blockquote>

<h3 id="6-i-generated-some-segmentations-from-another-tool-can-i-use-it-as-a-starting-point-in-micro_sam">6. I generated some segmentations from another tool, can I use it as a starting point in <code><a href="">micro_sam</a></code>?</h3>

<p>You can save and load the results from the <code>committed_objects</code> layer to correct segmentations you obtained from another tool (e.g. CellPose) or save intermediate annotation results. The results can be saved via <code>File</code> -> <code>Save Selected Layers (s) ...</code> in the napari menu-bar on top (see the tutorial videos for details). They can be loaded again by specifying the corresponding location via the <code>segmentation_result</code> parameter in the CLI or python script (2d and 3d segmentation).
If you are using an annotation tool you can load the segmentation you want to edit as segmentation layer and rename it to <code>committed_objects</code>.</p>

<h3 id="7-i-am-using-micro_sam-for-segmenting-objects-i-would-like-to-report-the-steps-for-reproducability-how-can-this-be-done">7. I am using <code><a href="">micro_sam</a></code> for segmenting objects. I would like to report the steps for reproducability. How can this be done?</h3>

<p>The annotation steps and segmentation results can be saved to a Zarr file by providing the <code>commit_path</code> in the <code>commit</code> widget. This file will contain all relevant information to reproduce the segmentation.</p>

<blockquote>
  <p>NOTE: This feature is still under development and we have not implemented rerunning the segmentation from this file yet. See <a href="https://github.com/computational-cell-analytics/micro-sam/issues/408">this issue</a> for details.</p>
</blockquote>

<h3 id="8-i-want-to-segment-objects-with-complex-structures-both-the-default-segment-anything-models-and-the-micro_sam-generalist-models-do-not-work-for-my-data-what-should-i-do">8. I want to segment objects with complex structures. Both the default Segment Anything models and the <code><a href="">micro_sam</a></code> generalist models do not work for my data. What should I do?</h3>

<p><code><a href="">micro_sam</a></code> supports interactive annotation using positive and negative point prompts, box prompts and polygon drawing. You can combine multiple types of prompts to improve the segmentation quality. In case the aforementioned suggestions do not work as desired, <code><a href="">micro_sam</a></code> also supports finetuning a model on your data (see the next section on <a href="#fine-tuning-questions">finetuning</a>). We recommend the following: a) Check which of the provided models performs relatively good on your data, b) Choose the best model as the starting point to train your own specialist model for the desired segmentation task.</p>

<h3 id="9-i-am-using-the-annotation-tool-and-napari-outputs-the-following-error-while-emmitting-signal-an-error-ocurred-in-callback-this-is-not-a-bug-in-psygnal-see-above-for-details">9. I am using the annotation tool and napari outputs the following error: <code>While emmitting signal ... an error ocurred in callback ... This is not a bug in psygnal. See ... above for details.</code></h3>

<p>These messages occur when an internal error happens in <code><a href="">micro_sam</a></code>. In most cases this is due to inconsistent annotations and you can fix them by clearing the annotations.
We want to remove these errors, so we would be very grateful if you can <a href="https://github.com/computational-cell-analytics/micro-sam/issues">open an issue</a> and describe the steps you did when encountering it.</p>

<h3 id="10-the-objects-are-not-segmented-in-my-3d-data-using-the-interactive-annotation-tool">10. The objects are not segmented in my 3d data using the interactive annotation tool.</h3>

<p>The first thing to check is: a) make sure you are using the latest version of <code><a href="">micro_sam</a></code> (pull the latest commit from master if your installation is from source, or update the installation from conda / mamba using <code>mamba update micro_sam</code>), and b) try out the steps from the <a href="https://youtu.be/nqpyNQSyu74">3d annotation tutorial video</a> to verify if this shows the same behaviour (or the same errors) as you faced. For 3d images, it's important to pass the inputs in the python axis convention, ZYX.
c) try using a different model and change the projection mode for 3d segmentation. This is also explained in the video.</p>

<h3 id="11-i-have-very-small-or-fine-grained-structures-in-my-high-resolution-microscopic-images-can-i-use-micro_sam-to-annotate-them">11. I have very small or fine-grained structures in my high-resolution microscopic images. Can I use <code><a href="">micro_sam</a></code> to annotate them?</h3>

<p>Segment Anything does not work well for very small or fine-grained objects (e.g. filaments). In these cases, you could try to use tiling to improve results (see <a href="#3-i-have-high-resolution-large-tomograms-micro-sam-does-not-seem-to-work">Point 3</a> above for details).</p>

<h3 id="12-napari-seems-to-be-very-slow-for-large-images">12. napari seems to be very slow for large images.</h3>

<p>Editing (drawing / erasing) very large 2d images or 3d volumes is known to be slow at the moment, as the objects in the layers are stored in-memory. See the related <a href="https://github.com/computational-cell-analytics/micro-sam/issues/39">issue</a>.</p>

<h3 id="13-while-computing-the-embeddings-and-or-automatic-segmentation-a-window-stating-napari-is-not-responding-pops-up">13. While computing the embeddings (and / or automatic segmentation), a window stating: <code>"napari" is not responding</code> pops up.</h3>

<p>This can happen for long running computations. You just need to wait a bit longer and the computation will finish.</p>

<h2 id="fine-tuning-questions">Fine-tuning questions</h2>

<h3 id="1-i-have-a-microscopy-dataset-i-would-like-to-fine-tune-segment-anything-for-is-it-possible-using-micro_sam">1. I have a microscopy dataset I would like to fine-tune Segment Anything for. Is it possible using <code><a href="">micro_sam</a></code>?</h3>

<p>Yes, you can fine-tune Segment Anything on your own dataset. Here's how you can do it:</p>

<ul>
<li>Check out the <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/micro-sam-finetuning.ipynb">tutorial notebook</a> on how to fine-tune Segment Anything with our <code><a href="micro_sam/training.html">micro_sam.training</a></code> library.</li>
<li>Or check the <a href="https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning">examples</a> for additional scripts that demonstrate finetuning.</li>
<li>If you are not familiar with coding in python at all then you can also use the <a href="finetuning-ui">graphical interface for finetuning</a>. But we recommend using a script for more flexibility and reproducibility.</li>
</ul>

<h3 id="2-i-would-like-to-fine-tune-segment-anything-on-open-source-cloud-services-eg-kaggle-notebooks-is-it-possible">2. I would like to fine-tune Segment Anything on open-source cloud services (e.g. Kaggle Notebooks), is it possible?</h3>

<p>Yes, you can fine-tune Segment Anything on your custom datasets on Kaggle (and <a href="https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#using-micro_sam-on-band">BAND</a>). Check out our <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/micro-sam-finetuning.ipynb">tutorial notebook</a> for this.</p>

<p><!---
TODO: we should improve this explanation and add a small image that visualizes the labels.
--></p>

<h3 id="3-what-kind-of-annotations-do-i-need-to-finetune-segment-anything">3. What kind of annotations do I need to finetune Segment Anything?</h3>

<p>Annotations are referred to the instance segmentation labels, i.e. each object of interests in your microscopy images have an individual id to uniquely identify all the segmented objects. You can obtain them by <code><a href="">micro_sam</a></code>'s annotation tools. In <code><a href="">micro_sam</a></code>, it's expected to provide dense segmentations (i.e. all objects per image are annotated) for finetuning Segment Anything with the additional decoder, however it's okay to use sparse segmentations (i.e. few objects per image are annotated) for just finetuning Segment Anything (without the additional decoder).</p>

<h3 id="4-i-have-finetuned-segment-anything-on-my-microscopy-data-how-can-i-use-it-for-annotating-new-images">4. I have finetuned Segment Anything on my microscopy data. How can I use it for annotating new images?</h3>

<p>You can load your finetuned model by entering the path to its checkpoint in the <code>custom_weights_path</code> field in the <code>Embedding Settings</code> drop-down menu.
If you are using the python library or CLI you can specify this path with the <code>checkpoint_path</code> parameter.</p>

<h3 id="5-what-is-the-background-of-the-new-ais-automatic-instance-segmentation-feature-in-micro_sam">5. What is the background of the new AIS (Automatic Instance Segmentation) feature in <code><a href="">micro_sam</a></code>?</h3>

<p><code><a href="">micro_sam</a></code> introduces a new segmentation decoder to the Segment Anything backbone, for enabling faster and accurate automatic instance segmentation, by predicting the <a href="https://github.com/constantinpape/torch-em/blob/main/torch_em/transform/label.py#L284">distances to the object center and boundary</a> as well as predicting foregrund, and performing <a href="https://github.com/constantinpape/torch-em/blob/main/torch_em/util/segmentation.py#L122">seeded watershed-based postprocessing</a> to obtain the instances.</p>

<h3 id="6-i-want-to-finetune-only-the-segment-anything-model-without-the-additional-instance-decoder">6. I want to finetune only the Segment Anything model without the additional instance decoder.</h3>

<p>The instance segmentation decoder is optional. So you can only finetune SAM or SAM and the additional decoder. Finetuning with the decoder will increase training times, but will enable you to use AIS. See <a href="https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning#example-for-model-finetuning">this example</a> for finetuning with both the objectives.</p>

<blockquote>
  <p>NOTE: To try out the other way round (i.e. the automatic instance segmentation framework without the interactive capability, i.e. a UNETR: a vision transformer encoder and a convolutional decoder), you can take inspiration from this <a href="https://github.com/constantinpape/torch-em/blob/main/experiments/vision-transformer/unetr/for_vimunet_benchmarking/run_livecell.py">example on LIVECell</a>.</p>
</blockquote>

<h3 id="7-i-have-a-nvidia-rtx-4090ti-gpu-with-24gb-vram-can-i-finetune-segment-anything">7. I have a NVIDIA RTX 4090Ti GPU with 24GB VRAM. Can I finetune Segment Anything?</h3>

<p>Finetuning Segment Anything is possible in most consumer-grade GPU and CPU resources (but training being a lot slower on the CPU). For the mentioned resource, it should be possible to finetune a ViT Base (also abbreviated as <code>vit_b</code>) by reducing the number of objects per image to 15.
This parameter has the biggest impact on the VRAM consumption and quality of the finetuned model.
You can find an overview of the resources we have tested for finetuning <a href="#training-your-own-model">here</a>.
We also provide a the convenience function <code>micro_sam.training.train_sam_for_configuration</code> that selects the best training settings for these configuration. This function is also used by the finetuning UI.</p>

<h3 id="8-i-want-to-create-a-dataloader-for-my-data-to-finetune-segment-anything">8. I want to create a dataloader for my data, to finetune Segment Anything.</h3>

<p>Thanks to <code>torch-em</code>, a) Creating PyTorch datasets and dataloaders using the python library is convenient and supported for various data formats and data structures.
See the <a href="https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb">tutorial notebook</a> on how to create dataloaders using <code>torch-em</code> and the <a href="https://github.com/constantinpape/torch-em/blob/main/doc/datasets_and_dataloaders.md">documentation</a> for details on creating your own datasets and dataloaders; and b) finetuning using the <code>napari</code> tool eases the aforementioned process, by allowing you to add the input parameters (path to the directory for inputs and labels etc.) directly in the tool.</p>

<blockquote>
  <p>NOTE: If you have images with large input shapes with a sparse density of instance segmentations, we recommend using <a href="https://github.com/constantinpape/torch-em/blob/main/torch_em/data/sampler.py"><code>sampler</code></a> for choosing the patches with valid segmentation for the finetuning purpose (see the <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/finetuning/specialists/training/light_microscopy/plantseg_root_finetuning.py#L29">example</a> for PlantSeg (Root) specialist model in <code><a href="">micro_sam</a></code>).</p>
</blockquote>

<h3 id="9-how-can-i-evaluate-a-model-i-have-finetuned">9. How can I evaluate a model I have finetuned?</h3>

<p>To validate a Segment Anything model for your data, you have different options, depending on the task you want to solve and whether you have segmentation annotations for your data.</p>

<ul>
<li>If you don't have any annotations you will have to validate the model visually. We suggest doing this with the <code><a href="">micro_sam</a></code> GUI tools. You can learn how to use them in the <code><a href="">micro_sam</a></code> documentation.</li>
<li>If you have segmentation annotations you can use the <code><a href="">micro_sam</a></code> python library to evaluate the segmentation quality of different models. We provide functionality to evaluate the models for interactive and for automatic segmentation:
<ul>
<li>You can use <code><a href="micro_sam/evaluation/evaluation.html#run_evaluation_for_iterative_prompting">micro_sam.evaluation.evaluation.run_evaluation_for_iterative_prompting</a></code> to evaluate models for interactive segmentation.</li>
<li>You can use <code><a href="micro_sam/evaluation/instance_segmentation.html#run_instance_segmentation_grid_search_and_inference">micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search_and_inference</a></code> to evaluate models for automatic segmentation.</li>
</ul></li>
</ul>

<p>We provide an <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/inference_and_evaluation.ipynb">example notebook</a> that shows how to use this evaluation functionality.</p>

<h1 id="contribution-guide">Contribution Guide</h1>

<ul>
<li><a href="#discuss-your-ideas">Discuss your ideas</a></li>
<li><a href="#clone-the-repository">Clone the repository</a></li>
<li><a href="#create-your-development-environment">Create your development environment</a></li>
<li><a href="#make-your-changes">Make your changes</a></li>
<li><a href="#testing">Testing</a>
<ul>
<li><a href="#run-the-tests">Run the tests</a></li>
<li><a href="#writing-your-own-tests">Writing your own tests</a></li>
</ul></li>
<li><a href="#open-a-pull-request">Open a pull request</a></li>
<li><a href="#optional-build-the-documentation">Optional: Build the documentation</a></li>
<li><a href="#optional-benchmark-performance">Optional: Benchmark performance</a>
<ul>
<li><a href="#run-the-benchmark-script">Run the benchmark script</a></li>
<li><a href="#line-profiling">Line profiling</a></li>
<li><a href="#snakeviz-visualization">Snakeviz visualization</a></li>
<li><a href="#memory-profiling-with-memray">Memory profiling with memray</a></li>
</ul></li>
</ul>

<h3 id="discuss-your-ideas">Discuss your ideas</h3>

<p>We welcome new contributions! First, discuss your idea by opening a <a href="https://github.com/computational-cell-analytics/micro-sam/issues/new">new issue</a> in micro-sam.
This allows you to ask questions, and have the current developers make suggestions about the best way to implement your ideas.</p>

<h3 id="clone-the-repository">Clone the repository</h3>

<p>We use <a href="https://git-scm.com/">git</a> for version control.</p>

<p>Clone the repository, and checkout the development branch:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/computational-cell-analytics/micro-sam.git
$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>micro-sam
$<span class="w"> </span>git<span class="w"> </span>checkout<span class="w"> </span>dev
</code></pre>
</div>

<h3 id="create-your-development-environment">Create your development environment</h3>

<p>We use <a href="https://docs.conda.io/en/latest/">conda</a> to <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">manage our environments</a>. If you don't have this already, install <a href="https://docs.conda.io/projects/miniconda/en/latest/">miniconda</a> or <a href="https://mamba.readthedocs.io/en/latest/">mamba</a> to get started.</p>

<p>Now you can create the environment, install user and developer dependencies, and micro-sam as an editable installation:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>mamba<span class="w"> </span>env<span class="w"> </span>create<span class="w"> </span>environment_gpu.yaml
$<span class="w"> </span>mamba<span class="w"> </span>activate<span class="w"> </span>sam
$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>requirements-dev.txt
$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</code></pre>
</div>

<h3 id="make-your-changes">Make your changes</h3>

<p>Now it's time to make your code changes.</p>

<p>Typically, changes are made branching off from the development branch. Checkout <code>dev</code> and then create a new branch to work on your changes.</p>

<pre><code>$ git checkout dev
$ git checkout -b my-new-feature
</code></pre>

<p>We use <a href="https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html">google style python docstrings</a> to create documentation for all new code.</p>

<p>You may also find it helpful to look at this <a href="#for-developers">developer guide</a>, which explains the organization of the micro-sam code.</p>

<h2 id="testing">Testing</h2>

<h3 id="run-the-tests">Run the tests</h3>

<p>The tests for micro-sam are run with <a href="https://docs.pytest.org/en/7.4.x/">pytest</a></p>

<p>To run the tests:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>pytest
</code></pre>
</div>

<h3 id="writing-your-own-tests">Writing your own tests</h3>

<p>If you have written new code, you will need to write tests to go with it.</p>

<h4 id="unit-tests">Unit tests</h4>

<p>Unit tests are the preferred style of tests for user contributions. Unit tests check small, isolated parts of the code for correctness. If your code is too complicated to write unit tests easily, you may need to consider breaking it up into smaller functions that are easier to test.</p>

<h4 id="tests-involving-napari">Tests involving napari</h4>

<p>In cases where tests <em>must</em> use the napari viewer, <a href="https://napari.org/stable/plugins/test_deploy.html#tips-for-testing-napari-plugins">these tips might be helpful</a> (in particular, the <code>make_napari_viewer_proxy</code> fixture).</p>

<p>These kinds of tests should be used only in limited circumstances. Developers are <a href="https://napari.org/stable/plugins/test_deploy.html#prefer-smaller-unit-tests-when-possible">advised to prefer smaller unit tests, and avoid integration tests</a> wherever possible.</p>

<h4 id="code-coverage">Code coverage</h4>

<p>Pytest uses the <a href="https://pytest-cov.readthedocs.io/en/latest/">pytest-cov</a> plugin to automatically determine which lines of code are covered by tests.</p>

<p>A short summary report is printed to the terminal output whenever you run pytest. The full results are also automatically written to a file named <code>coverage.xml</code>.</p>

<p>The <a href="https://marketplace.visualstudio.com/items?itemName=ryanluker.vscode-coverage-gutters">Coverage Gutters VSCode extension</a> is useful for visualizing which parts of the code need better test coverage. PyCharm professional <a href="https://www.jetbrains.com/pycharm/guide/tips/spot-coverage-in-gutter/">has a similar feature</a>, and you may be able to find similar tools for your preferred editor.</p>

<p>We also use <a href="https://app.codecov.io/gh/computational-cell-analytics/micro-sam">codecov.io</a> to display the code coverage results from our Github Actions continuous integration.</p>

<h3 id="open-a-pull-request">Open a pull request</h3>

<p>Once you've made changes to the code and written some tests to go with it, you are ready to <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests">open a pull request</a>. You can <a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-requests">mark your pull request as a draft</a> if you are still working on it, and still get the benefit of discussing the best approach with maintainers.</p>

<p>Remember that typically changes to micro-sam are made branching off from the development branch. So, you will need to open your pull request to merge back into the <code>dev</code> branch <a href="https://github.com/computational-cell-analytics/micro-sam/compare/dev...dev">like this</a>.</p>

<h3 id="optional-build-the-documentation">Optional: Build the documentation</h3>

<p>We use <a href="https://pdoc.dev/docs/pdoc.html">pdoc</a> to build the documentation.</p>

<p>To build the documentation locally, run this command:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>build_doc.py
</code></pre>
</div>

<p>This will start a local server and display the HTML documentation. Any changes you make to the documentation will be updated in real time (you may need to refresh your browser to see the changes).</p>

<p>If you want to save the HTML files, append <code>--out</code> to the command, like this:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>build_doc.py<span class="w"> </span>--out
</code></pre>
</div>

<p>This will save the HTML files into a new directory named <code>tmp</code>.</p>

<p>You can add content to the documentation in two ways:</p>

<ol>
<li>By adding or updating <a href="https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html">google style python docstrings</a> in the micro-sam code.
<ul>
<li><a href="https://pdoc.dev/docs/pdoc.html">pdoc</a> will automatically find and include docstrings in the documentation.</li>
</ul></li>
<li>By adding or editing markdown files in the micro-sam <code>doc</code> directory.
<ul>
<li>If you add a new markdown file to the documentation, you must tell <a href="https://pdoc.dev/docs/pdoc.html">pdoc</a> that it exists by adding a line to the <code>micro_sam/__init__.py</code> module docstring (eg: <code>.. include:: ../doc/my_amazing_new_docs_page.md</code>). Otherwise it will not be included in the final documentation build!</li>
</ul></li>
</ol>

<h3 id="optional-benchmark-performance">Optional: Benchmark performance</h3>

<p>There are a number of options you can use to benchmark performance, and identify problems like slow run times or high memory use in micro-sam.</p>

<ul>
<li><a href="#run-the-benchmark-script">Run the benchmark script</a></li>
<li><a href="#line-profiling">Line profiling</a></li>
<li><a href="#snakeviz-visualization">Snakeviz visualization</a></li>
<li><a href="#memory-profiling-with-memray">Memory profiling with memray</a></li>
</ul>

<h4 id="run-the-benchmark-script">Run the benchmark script</h4>

<p>There is a performance benchmark script available in the micro-sam repository at <code>development/benchmark.py</code>.</p>

<p>To run the benchmark script:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>development/benchmark.py<span class="w"> </span>--model_type<span class="w"> </span>vit_t<span class="w"> </span>--device<span class="w"> </span>cpu<span class="sb">`</span>
</code></pre>
</div>

<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>development/benchmark.py<span class="w"> </span>--help
</code></pre>
</div>

<h4 id="line-profiling">Line profiling</h4>

<p>For more detailed line by line performance results, we can use <a href="https://github.com/pyutils/line_profiler">line-profiler</a>.</p>

<blockquote>
  <p><a href="https://github.com/pyutils/line_profiler">line_profiler</a> is a module for doing line-by-line profiling of functions. kernprof is a convenient script for running either <code>line_profiler</code> or the Python standard library's cProfile or profile modules, depending on what is available.</p>
</blockquote>

<p>To do line-by-line profiling:</p>

<ol>
<li>Ensure you have line profiler installed: <code>python -m pip install line_profiler</code></li>
<li>Add <code>@profile</code> decorator to any function in the call stack</li>
<li>Run <code>kernprof -lv benchmark.py --model_type vit_t --device cpu</code></li>
</ol>

<p>For more details about how to use line-profiler and kernprof, see <a href="https://kernprof.readthedocs.io/en/latest/">the documentation</a>.</p>

<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>development/benchmark.py<span class="w"> </span>--help
</code></pre>
</div>

<h4 id="snakeviz-visualization">Snakeviz visualization</h4>

<p>For more detailed visualizations of profiling results, we use <a href="https://jiffyclub.github.io/snakeviz/">snakeviz</a>.</p>

<blockquote>
  <p>SnakeViz is a browser based graphical viewer for the output of Pythons cProfile module.</p>
</blockquote>

<ol>
<li>Ensure you have snakeviz installed: <code>python -m pip install snakeviz</code></li>
<li>Generate profile file: <code>python -m cProfile -o program.prof benchmark.py --model_type vit_h --device cpu</code></li>
<li>Visualize profile file: <code>snakeviz program.prof</code></li>
</ol>

<p>For more details about how to use snakeviz, see <a href="https://jiffyclub.github.io/snakeviz/">the documentation</a>.</p>

<h4 id="memory-profiling-with-memray">Memory profiling with memray</h4>

<p>If you need to investigate memory use specifically, we use <a href="https://github.com/bloomberg/memray">memray</a>.</p>

<blockquote>
  <p>Memray is a memory profiler for Python. It can track memory allocations in Python code, in native extension modules, and in the Python interpreter itself. It can generate several different types of reports to help you analyze the captured memory usage data. While commonly used as a CLI tool, it can also be used as a library to perform more fine-grained profiling tasks.</p>
</blockquote>

<p>For more details about how to use memray, see <a href="https://bloomberg.github.io/memray/getting_started.html">the documentation</a>.</p>

<h2 id="creating-a-new-release">Creating a new release</h2>

<p>To create a new release you have to edit the version number in <a href="https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/__version__.py">micro_sam/__version__.py</a> in a PR. After merging this PR the release will automatically be done by the CI.</p>

<h1 id="using-micro_sam-on-band">Using <code><a href="">micro_sam</a></code> on BAND</h1>

<p>BAND is a service offered by EMBL Heidelberg that gives access to a virtual desktop for image analysis tasks. It is free to use and <code><a href="">micro_sam</a></code> is installed there.
In order to use BAND and start <code><a href="">micro_sam</a></code> on it follow these steps:</p>

<h2 id="start-band">Start BAND</h2>

<ul>
<li>Go to <a href="https://band.embl.de/">https://band.embl.de/</a> and click <strong>Login</strong>. If you have not used BAND before you will need to register for BAND. Currently you can only sign up via a Google account.</li>
<li>Launch a BAND desktop with sufficient resources. It's particularly important to select a GPU. The settings from the image below are a good choice.</li>
<li>Go to the desktop by clicking <strong>GO TO DESKTOP</strong> in the <strong>Running Desktops</strong> menu. See also the screenshot below.</li>
</ul>

<p><img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/f965fce2-b924-4fc8-871b-f3201e502138" alt="image" /></p>

<h2 id="start-micro_sam-in-band">Start <code><a href="">micro_sam</a></code> in BAND</h2>

<ul>
<li>Select <strong>Applications -> Image Analysis -> uSAM</strong> (see screenshot)
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5daeafb3-119b-4104-8708-aab2960cb21c" alt="image" /></li>
<li>This will open the micro_sam menu, where you can select the tool you want to use (see screenshot). Note: this may take a few minutes.
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/900ce0b9-4cf8-418c-94f1-e99ac7bc0086" alt="image" /></li>
<li>For testing if the tool works, it's best to use the <strong>2d annotator</strong> first.
<ul>
<li>You can find an example image to use here: <code>/scratch/cajal-connectomics/hela-2d-image.png</code>. Select it via <strong>Select image</strong>. (see screenshot)
<img src="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5fbd1c53-2ba1-47d4-ae50-dfab890ac9d3" alt="image" /></li>
</ul></li>
<li>Then press <strong>2d annotator</strong> and the tool will start.</li>
</ul>

<h2 id="transfering-data-to-band">Transfering data to BAND</h2>

<p>To copy data to and from BAND you can use any cloud storage, e.g. ownCloud, dropbox or google drive. For this, it's important to note that copy and paste, which you may need for accessing links on BAND, works a bit different in BAND:</p>

<ul>
<li>To copy text into BAND you first need to copy it on your computer (e.g. via selecting it + <code>Ctrl + C</code>).</li>
<li>Then go to the browser window with BAND and press <code>Ctrl + Shift + Alt</code>. This will open a side window where you can paste your text via <code>Ctrl + V</code>.</li>
<li>Then select the text in this window and copy it via <code>Ctrl + C</code>.</li>
<li>Now you can close the side window via <code>Ctrl + Shift + Alt</code> and paste the text in band via <code>Ctrl + V</code></li>
</ul>

<p>The video below shows how to copy over a link from owncloud and then download the data on BAND using copy and paste:</p>

<p><a href="https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287">https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287</a></p>
</div>

                        <input id="mod-micro_sam-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-micro_sam-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos"> 1</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos"> 2</span></a><span class="sd">.. include:: ../doc/start_page.md</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos"> 3</span></a><span class="sd">.. include:: ../doc/installation.md</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos"> 4</span></a><span class="sd">.. include:: ../doc/annotation_tools.md</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos"> 5</span></a><span class="sd">.. include:: ../doc/python_library.md</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos"> 6</span></a><span class="sd">.. include:: ../doc/finetuned_models.md</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos"> 7</span></a><span class="sd">.. include:: ../doc/faq.md</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos"> 8</span></a><span class="sd">.. include:: ../doc/contributing.md</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos"> 9</span></a><span class="sd">.. include:: ../doc/band.md</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">10</span></a><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">11</span></a><span class="kn">import</span> <span class="nn">os</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">12</span></a>
</span><span id="L-13"><a href="#L-13"><span class="linenos">13</span></a><span class="kn">from</span> <span class="nn">.__version__</span> <span class="kn">import</span> <span class="n">__version__</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">14</span></a>
</span><span id="L-15"><a href="#L-15"><span class="linenos">15</span></a><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTORCH_ENABLE_MPS_FALLBACK&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
</span></pre></div>


            </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>