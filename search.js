window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "micro_sam", "modulename": "micro_sam", "kind": "module", "doc": "<h1 id=\"segment-anything-for-microscopy\">Segment Anything for Microscopy</h1>\n\n<p>Segment Anything for Microscopy implements automatic and interactive annotation for microscopy data. It is built on top of <a href=\"https://segment-anything.com/\">Segment Anything</a> by Meta AI and specializes it for microscopy and other biomedical imaging data.\nIts core components are:</p>\n\n<ul>\n<li>The <code>micro_sam</code> tools for interactive data annotation, built as <a href=\"https://napari.org/stable/\">napari</a> plugin.</li>\n<li>The <code>micro_sam</code> library to apply Segment Anything to 2d and 3d data or fine-tune it on your data.</li>\n<li>The <code>micro_sam</code> models that are fine-tuned on publicly available microscopy data and that are available on <a href=\"https://bioimage.io/#/\">BioImage.IO</a>.</li>\n</ul>\n\n<p>Based on these components <code>micro_sam</code> enables fast interactive and automatic annotation for microscopy data, like interactive cell segmentation from bounding boxes:</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d04cb158-9f5b-4460-98cd-023c4f19cccd\" alt=\"box-prompts\" /></p>\n\n<p><code>micro_sam</code> is now available as stable version 1.0 and we will not change its user interface significantly in the foreseeable future.\nWe are still working on improving and extending its functionality. The current roadmap includes:</p>\n\n<ul>\n<li>Releasing more and better finetuned models for the biomedical imaging domain.</li>\n<li>Integrating parameter efficient training and compressed models for efficient fine-tuning and faster inference.</li>\n<li>Improving the 3D segmentation and tracking functionality.</li>\n</ul>\n\n<p>If you run into any problems or have questions please <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/new\">open an issue</a> or reach out via <a href=\"https://forum.image.sc/\">image.sc</a> using the tag <code>micro-sam</code>.</p>\n\n<h2 id=\"quickstart\">Quickstart</h2>\n\n<p>You can install <code>micro_sam</code> via conda:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>install<span class=\"w\"> </span>-c<span class=\"w\"> </span>conda-forge<span class=\"w\"> </span>micro_sam\n</code></pre>\n</div>\n\n<p>We also provide installers for Windows and Linux. For more details on the available installation options, check out <a href=\"#installation\">the installation section</a>.</p>\n\n<p>After installing <code>micro_sam</code>, you can start napari from within your environment using</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>napari\n</code></pre>\n</div>\n\n<p>After starting napari, you can select the annotation tool you want to use from <code>Plugins -&gt; SegmentAnything for Microscopy</code>. Check out the <a href=\"https://youtu.be/gcv0fa84mCc\">quickstart tutorial video</a> for a short introduction, the video of our <a href=\"https://www.youtube.com/watch?v=dxjU4W7bCis&list=PLdA9Vgd1gxTbvxmtk9CASftUOl_XItjDN&index=33\">virtual I2K tutorial</a> for an in-depth explanation and <a href=\"#annotation-tools\">the annotation tool section</a> for details.</p>\n\n<p>The <code>micro_sam</code> python library can be imported via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>It is explained in more detail <a href=\"#using-the-python-library\">here</a>.</p>\n\n<p>We provide different finetuned models for microscopy that can be used within our tools or any other tool that supports Segment Anything. See <a href=\"#finetuned-models\">finetuned models</a> for details on the available models.\nYou can also train models on your own data, see <a href=\"#training-your-own-model\">here for details</a>.</p>\n\n<h2 id=\"citation\">Citation</h2>\n\n<p>If you are using <code>micro_sam</code> in your research please cite</p>\n\n<ul>\n<li>our <a href=\"https://doi.org/10.1101/2023.08.21.554208\">preprint</a></li>\n<li>and the original <a href=\"https://arxiv.org/abs/2304.02643\">Segment Anything publication</a>.</li>\n<li>If you use a <code>vit-tiny</code> models, please also cite <a href=\"https://arxiv.org/abs/2306.14289\">Mobile SAM</a>.</li>\n</ul>\n\n<h1 id=\"installation\">Installation</h1>\n\n<p>There are three ways to install <code>micro_sam</code>:</p>\n\n<ul>\n<li><a href=\"#from-conda\">From conda</a> is the recommended way if you want to use all functionality.</li>\n<li><a href=\"#from-source\">From source</a> for setting up a development environment to use the latest version and to change and contribute to our software.</li>\n<li><a href=\"#from-installer\">From installer</a> to install it without having to use conda (supported platforms: Windows and Linux, supports only CPU). </li>\n</ul>\n\n<p>You can find more information on the installation and how to troubleshoot it in <a href=\"#installation-questions\">the FAQ section</a>.</p>\n\n<p>We do <strong>not support</strong> installing <code>micro_sam</code> with pip.</p>\n\n<h2 id=\"from-conda\">From conda</h2>\n\n<p><code>conda</code> is a python package manager. If you don't have it installed yet you can follow the instructions <a href=\"https://conda-forge.org/download/\">here</a> to set it up on your system.\nPlease make sure that you are using an up-to-date version of conda to install <code>micro_sam</code>.\nYou can also use <a href=\"https://mamba.readthedocs.io/en/latest/\">mamba</a>, which is a drop-in replacement for conda, to install it. In this case, just replace the <code>conda</code> command below with <code>mamba</code>.</p>\n\n<p><strong>IMPORTANT</strong>: Do not install <code>micro_sam</code> in the base conda environment.</p>\n\n<p><strong>Installation on Linux and Mac OS:</strong></p>\n\n<p><code>micro_sam</code> can be installed in an existing environment via:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>install<span class=\"w\"> </span>-c<span class=\"w\"> </span>conda-forge<span class=\"w\"> </span>micro_sam\n</code></pre>\n</div>\n\n<p>or you can create a new environment with it (here called <code>micro-sam</code>) via:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>create<span class=\"w\"> </span>-c<span class=\"w\"> </span>conda-forge<span class=\"w\"> </span>-n<span class=\"w\"> </span>micro-sam<span class=\"w\"> </span>micro_sam\n</code></pre>\n</div>\n\n<p>and then activate it via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>activate<span class=\"w\"> </span>micro-sam\n</code></pre>\n</div>\n\n<p>This will also install <code>pytorch</code> from the <code>conda-forge</code> channel. If you have a recent enough operating system, it will automatically install the best suitable <code>pytorch</code> version on your system.\nThis means it will install the CPU version if you don't have a nVidia GPU, and will install a GPU version if you have.\nHowever, if you have an older operating system, or a CUDA version older than 12, than it may not install the correct version. In this case you will have to specify you're CUDA version, for example for CUDA 11, like this:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>install<span class=\"w\"> </span>-c<span class=\"w\"> </span>conda-forge<span class=\"w\"> </span>micro_sam<span class=\"w\"> </span><span class=\"s2\">&quot;libtorch=*=cuda11*&quot;</span>\n</code></pre>\n</div>\n\n<p><strong>Installation on Windows:</strong></p>\n\n<p><code>pytorch</code> is currently not available on conda-forge for windows. Thus, you have to install it from the <code>pytorch</code> conda channel. In addition, you have to specify two specific dependencies to avoid incompatibilities.\nThis can be done with the following commands:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>install<span class=\"w\"> </span>-c<span class=\"w\"> </span>pytorch<span class=\"w\"> </span>-c<span class=\"w\"> </span>conda-forge<span class=\"w\"> </span>micro_sam<span class=\"w\"> </span><span class=\"s2\">&quot;nifty=1.2.1=*_4&quot;</span><span class=\"w\"> </span><span class=\"s2\">&quot;protobuf&lt;5&quot;</span>\n</code></pre>\n</div>\n\n<p>to install <code>micro_sam</code> in an existing environment and</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>create<span class=\"w\"> </span>-c<span class=\"w\"> </span>pytorch<span class=\"w\"> </span>-c<span class=\"w\"> </span>conda-forge<span class=\"w\"> </span>-n<span class=\"w\"> </span>micro-sam<span class=\"w\"> </span>micro_sam<span class=\"w\"> </span><span class=\"s2\">&quot;nifty=1.2.1=*_4&quot;</span><span class=\"w\"> </span><span class=\"s2\">&quot;protobuf&lt;5&quot;</span>\n</code></pre>\n</div>\n\n<h2 id=\"from-source\">From source</h2>\n\n<p>To install <code>micro_sam</code> from source, we recommend to first set up an environment with the necessary requirements:</p>\n\n<ul>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment.yaml\">environment.yaml</a>: to set up an environment on Linux or Mac OS.</li>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_cpu_win.yaml\">environment_cpu_win.yaml</a>: to set up an environment on windows with CPU support.</li>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_gpu_win.yaml\">environment_gpu_win.yaml</a>: to set up an environment on windows with GPU support.</li>\n</ul>\n\n<p>To create one of these environments and install <code>micro_sam</code> into it follow these steps</p>\n\n<ol>\n<li>Clone the repository:</li>\n</ol>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/computational-cell-analytics/micro-sam\n</code></pre>\n</div>\n\n<ol start=\"2\">\n<li>Enter it:</li>\n</ol>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"nb\">cd</span><span class=\"w\"> </span>micro-sam\n</code></pre>\n</div>\n\n<ol start=\"3\">\n<li>Create the respective environment:</li>\n</ol>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>env<span class=\"w\"> </span>create<span class=\"w\"> </span>-f<span class=\"w\"> </span>&lt;ENV_FILE&gt;.yaml\n</code></pre>\n</div>\n\n<ol start=\"4\">\n<li>Activate the environment:</li>\n</ol>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>activate<span class=\"w\"> </span>sam\n</code></pre>\n</div>\n\n<ol start=\"5\">\n<li>Install <code>micro_sam</code>:</li>\n</ol>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-e<span class=\"w\"> </span>.\n</code></pre>\n</div>\n\n<h2 id=\"from-installer\">From installer</h2>\n\n<p>We also provide installers for Linux and Windows:</p>\n\n<ul>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/Fyf57WZuiX1NyXs\">Linux</a></li>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/ZWrY68hl7xE3kGP\">Windows</a>\n<!---</li>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/7YupGgACw9SHy2P\">Mac</a>\n--></li>\n</ul>\n\n<p>The installers will not enable you to use a GPU, so if you have one then please consider installing <code>micro_sam</code> via <a href=\"#from-conda\">conda</a> instead. They will also not enable using the python library.</p>\n\n<h3 id=\"linux-installer\">Linux Installer:</h3>\n\n<p>To use the installer:</p>\n\n<ul>\n<li>Unpack the zip file you have downloaded.</li>\n<li>Make the installer executable: <code>$ chmod +x micro_sam-1.0.0post0-Linux-x86_64.sh</code></li>\n<li>Run the installer: <code>./micro_sam-1.0.0post0-Linux-x86_64.sh</code> \n<ul>\n<li>You can select where to install <code>micro_sam</code> during the installation. By default it will be installed in <code>$HOME/micro_sam</code>.</li>\n<li>The installer will unpack all <code>micro_sam</code> files to the installation directory.</li>\n</ul></li>\n<li>After the installation you can start the annotator with the command <code>.../micro_sam/bin/napari</code>.\n<ul>\n<li>Proceed with the steps described in <a href=\"#annotation-tools\">Annotation Tools</a></li>\n<li>To make it easier to run the annotation tool you can add <code>.../micro_sam/bin</code> to your <code>PATH</code> or set a softlink to <code>.../micro_sam/bin/napari</code>.</li>\n</ul></li>\n</ul>\n\n<h3 id=\"windows-installer\">Windows Installer:</h3>\n\n<ul>\n<li>Unpack the zip file you have downloaded.</li>\n<li>Run the installer by double clicking on it.</li>\n<li>Choose installation type: <code>Just Me(recommended)</code> or <code>All Users(requires admin privileges)</code>.</li>\n<li>Choose installation path. By default it will be installed in <code>C:\\Users\\&lt;Username&gt;\\micro_sam</code> for <code>Just Me</code> installation or in <code>C:\\ProgramData\\micro_sam</code> for <code>All Users</code>.\n<ul>\n<li>The installer will unpack all micro_sam files to the installation directory.</li>\n</ul></li>\n<li>After the installation you can start the annotator by double clicking on <code>.\\micro_sam\\Scripts\\micro_sam.annotator.exe</code> or  with the command <code>.\\micro_sam\\Scripts\\napari.exe</code> from the Command Prompt.</li>\n<li>Proceed with the steps described in <a href=\"#annotation-tools\">Annotation Tools</a> </li>\n</ul>\n\n<p><!---\n<strong>Mac Installer:</strong></p>\n\n<p>To use the Mac installer you will need to enable installing unsigned applications. Please follow <a href=\"https://disable-gatekeeper.github.io/\">the instructions for 'Disabling Gatekeeper for one application only' here</a>.</p>\n\n<p>Alternative link on how to disable gatekeeper.\n<a href=\"https://www.makeuseof.com/how-to-disable-gatekeeper-mac/\">https://www.makeuseof.com/how-to-disable-gatekeeper-mac/</a></p>\n\n<p>TODO detailed instruction\n--></p>\n\n<h3 id=\"easybuild-installation\">Easybuild installation</h3>\n\n<p>There is also an easy-build recipe for <code>micro_sam</code> under development. You can find more information <a href=\"https://github.com/easybuilders/easybuild-easyconfigs/pull/20636\">here</a>.</p>\n\n<h1 id=\"annotation-tools\">Annotation Tools</h1>\n\n<p><code>micro_sam</code> provides applications for fast interactive 2d segmentation, 3d segmentation and tracking.\nSee an example for interactive cell segmentation in phase-contrast microscopy (left), interactive segmentation\nof mitochondria in volume EM (middle) and interactive tracking of cells (right).</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d5ee2080-ab08-4716-b4c4-c169b4ed29f5\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/dfca3d9b-dba5-440b-b0f9-72a0683ac410\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/aefbf99f-e73a-4125-bb49-2e6592367a64\" width=\"256\"></p>\n\n<p>The annotation tools can be started from the napari plugin menu, the command line or from python scripts.\nThey are built as napari plugin and make use of existing napari functionality wherever possible. If you are not familiar with napari, we recommend to <a href=\"https://napari.org/stable/tutorials/fundamentals/quick_start.html\">start here</a>.\nThe <code>micro_sam</code> tools mainly use the <a href=\"https://napari.org/stable/howtos/layers/points.html\">point layer</a>, <a href=\"https://napari.org/stable/howtos/layers/shapes.html\">shape layer</a> and <a href=\"https://napari.org/stable/howtos/layers/labels.html\">label layer</a>.</p>\n\n<p>The annotation tools are explained in detail below. We also provide <a href=\"https://youtube.com/playlist?list=PLwYZXQJ3f36GQPpKCrSbHjGiH39X4XjSO&si=qNbB8IFXqAX33r_Z\">video tutorials</a>.</p>\n\n<p>The annotation tools can be started from the napari plugin menu:\n<img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/napari-plugin.png\" width=\"768\"></p>\n\n<p>You can find additional information on the annotation tools <a href=\"#usage-question\">in the FAQ section</a>.</p>\n\n<p>HINT: If you would like to start napari to use <code>micro-sam</code> from the plugin menu, you must start it by activating the environment where <code>micro-sam</code> has been installed using:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>activate<span class=\"w\"> </span>&lt;ENVIRONMENT_NAME&gt;\nnapari\n</code></pre>\n</div>\n\n<h2 id=\"annotator-2d\">Annotator 2D</h2>\n\n<p>The 2d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Annotator 2d</code> in the plugin menu after starting <code>napari</code>.</li>\n<li>running <code>$ micro_sam.annotator_2d</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_2d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py\">examples/annotator_2d.py</a> for details.</li>\n</ul>\n\n<p>The user interface of the 2d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/2d-annotator-menu.png\" width=\"1024\"></p>\n\n<p>It contains the following elements:</p>\n\n<ol>\n<li>The napari layers for the segmentations and prompts:\n<ul>\n<li><code>prompts</code>: shape layer that is used to provide box prompts to Segment Anything. Prompts can be given as rectangle (marked as box prompt in the image), ellipse or polygon.</li>\n<li><code>point_prompts</code>: point layer that is used to provide point prompts to Segment Anything. Positive prompts (green points) for marking the object you want to segment, negative prompts (red points) for marking the outside of the object.</li>\n<li><code>committed_objects</code>: label layer with the objects that have already been segmented.</li>\n<li><code>auto_segmentation</code>: label layer with the results from automatic instance segmentation.</li>\n<li><code>current_object</code>: label layer for the object(s) you're currently segmenting.</li>\n</ul></li>\n<li>The embedding menu. For selecting the image to process, the Segment Anything model that is used and computing its image embeddings. The <code>Embedding Settings</code> contain advanced settings for loading cached embeddings from file or for using tiled embeddings.</li>\n<li>The prompt menu for changing whether the currently selected point is a positive or a negative prompt. This can also be done by pressing <code>T</code>.</li>\n<li>The menu for interactive segmentation. Clicking <code>Segment Object</code> (or pressing <code>S</code>) will run segmentation for the current prompts. The result is displayed in <code>current_object</code>. Activating <code>batched</code> enables segmentation of multiple objects with point prompts. In this case one object will be segmented per positive prompt.</li>\n<li>The menu for automatic segmentation. Clicking <code>Automatic Segmentation</code> will segment all objects n the image. The results will be displayed in the <code>auto_segmentation</code> layer. We support two different methods for automatic segmentation: automatic mask generation (supported for all models) and instance segmentation with an additional decoder (only supported for our models).\nChanging the parameters under <code>Automatic Segmentation Settings</code> controls the segmentation results, check the tooltips for details.</li>\n<li>The menu for commiting the segmentation. When clicking <code>Commit</code> (or pressing <code>C</code>) the result from the selected layer (either <code>current_object</code> or <code>auto_segmentation</code>) will be transferred from the respective layer to <code>committed_objects</code>.\nWhen <code>commit_path</code> is given the results will automatically be saved there.</li>\n<li>The menu for clearing the current annotations. Clicking <code>Clear Annotations</code> (or pressing <code>Shift + C</code>) will clear the current annotations and the current segmentation.</li>\n</ol>\n\n<p>Point prompts and box prompts can be combined. When you're using point prompts you can only segment one object at a time, unless the <code>batched</code> mode is activated. With box prompts you can segment several objects at once, both in the normal and <code>batched</code> mode.</p>\n\n<p>Check out <a href=\"https://youtu.be/9xjJBg_Bfuc\">the video tutorial</a> for an in-depth explanation on how to use this tool.</p>\n\n<h2 id=\"annotator-3d\">Annotator 3D</h2>\n\n<p>The 3d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Annotator 3d</code> in the plugin menu after starting <code>napari</code>.</li>\n<li>running <code>$ micro_sam.annotator_3d</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_3d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_3d.py\">examples/annotator_3d.py</a> for details.</li>\n</ul>\n\n<p>The user interface of the 3d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/3d-annotator-menu.png\" width=\"1024\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the segmentations and prompts.</li>\n<li>The embedding menu.</li>\n<li>The prompt menu.</li>\n<li>The menu for interactive segmentation in the current slice.</li>\n<li>The menu for interactive 3d segmentation. Clicking <code>Segment All Slices</code> (or pressing <code>Shift + S</code>) will extend the segmentation of the current object across the volume by projecting prompts across slices. The parameters for prompt projection can be set in <code>Segmentation Settings</code>, please refer to the tooltips for details.</li>\n<li>The menu for automatic segmentation. The overall functionality is the same as <a href=\"#annotator-2d\">for the 2d annotator</a>. To segment the full volume <code>Apply to Volume</code> needs to be checked, otherwise only the current slice will be segmented. Note that 3D segmentation can take quite long without a GPU.</li>\n<li>The menu for committing the current object.</li>\n<li>The menu for clearing the current annotations. If <code>all slices</code> is set all annotations will be cleared, otherwise they are only cleared for the current slice.</li>\n</ol>\n\n<p>You can only segment one object at a time using the interactive segmentation functionality with this tool.</p>\n\n<p>Check out <a href=\"https://youtu.be/nqpyNQSyu74\">the video tutorial</a> for an in-depth explanation on how to use this tool.</p>\n\n<h2 id=\"annotator-tracking\">Annotator Tracking</h2>\n\n<p>The tracking annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Annotator Tracking</code> in the plugin menu after starting <code>napari</code>.</li>\n<li>running <code>$ micro_sam.annotator_tracking</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_tracking</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_tracking.py\">examples/annotator_tracking.py</a> for details. </li>\n</ul>\n\n<p>The user interface of the tracking annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/tracking-annotator-menu.png\" width=\"1024\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the segmentations and prompts. Same as for <a href=\"#annotator-2d\">the 2d segmentation application</a> but without the <code>auto_segmentation</code> layer.</li>\n<li>The embedding menu.</li>\n<li>The prompt menu.</li>\n<li>The menu with tracking settings: <code>track_state</code> is used to indicate that the object you are tracking is dividing in the current frame. <code>track_id</code> is used to select which of the tracks after division you are following.</li>\n<li>The menu for interactive segmentation in the current frame.</li>\n<li>The menu for interactive tracking. Click <code>Track Object</code> (or press <code>Shift + S</code>) to segment the current object across time.</li>\n<li>The menu for committing the current tracking result.</li>\n<li>The menu for clearing the current annotations.</li>\n</ol>\n\n<p>The tracking annotator only supports 2d image data with a time dimension, volumetric data + time is not supported. We also do not support automatic tracking yet.</p>\n\n<p>Check out <a href=\"https://youtu.be/1gg8OPHqOyc\">the video tutorial</a> for an in-depth explanation on how to use this tool.</p>\n\n<h2 id=\"image-series-annotator\">Image Series Annotator</h2>\n\n<p>The image series annotation tool enables running the <a href=\"#annotator-2d\">2d annotator</a> or <a href=\"#annotator-3d\">3d annotator</a> for multiple images that are saved in a folder. This makes it convenient to annotate many images without having to restart the tool for every image. It can be started by</p>\n\n<ul>\n<li>clicking <code>Image Series Annotator</code> in the plugin menu after starting <code>napari</code>.</li>\n<li>running <code>$ micro_sam.image_series_annotator</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.image_series_annotator</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/image_series_annotator.py\">examples/image_series_annotator.py</a> for details. </li>\n</ul>\n\n<p>When starting this tool via the plugin menu the following interface opens:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/series-menu.png\" width=\"512\"></p>\n\n<p>You can select the folder where your images are saved with <code>Input Folder</code>. The annotation results will be saved in <code>Output Folder</code>.\nYou can specify a rule for loading only a subset of images via <code>pattern</code>, for example <code>*.tif</code> to only load tif images. Set <code>is_volumetric</code> if the data you want to annotate is 3d. The rest of the options are settings for the image embedding computation and are the same as for the embedding menu (see above).\nOnce you click <code>Annotate Images</code> the images from the folder you have specified will be loaded and the annotation tool is started for them.</p>\n\n<p>This menu will not open if you start the image series annotator from the command line or via python. In this case the input folder and other settings are passed as parameters instead.</p>\n\n<p>Check out <a href=\"https://youtu.be/HqRoImdTX3c\">the video tutorial</a> for an in-depth explanation on how to use the image series annotator.</p>\n\n<h2 id=\"finetuning-ui\">Finetuning UI</h2>\n\n<p>We also provide a graphical inferface for fine-tuning models on your own data. It can be started by clicking <code>Finetuning</code> in the plugin menu after starting <code>napari</code>.</p>\n\n<p><strong>Note:</strong> if you know a bit of python programming we recommend to use a script for model finetuning instead. This will give you more options to configure the training. See <a href=\"#training-your-own-model\">these instructions</a> for details.</p>\n\n<p>When starting this tool via the plugin menu the following interface opens:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/finetuning-menu.png\" width=\"512\"></p>\n\n<p>You can select the image data via <code>Path to images</code>. You can either load images from a folder or select a single image file. By providing <code>Image data key</code> you can either provide a pattern for selecting files from the folder or provide an internal filepath for HDF5, Zarr or similar fileformats.</p>\n\n<p>You can select the label data via <code>Path to labels</code> and <code>Label data key</code>, following the same logic as for the image data. The label masks are expected to have the same size as the image data. You can for example use annotations created with one of the <code>micro_sam</code> annotation tools for this, they are stored in the correct format. See <a href=\"#fine-tuning-questions\">the FAQ</a> for more details on the expected label data.</p>\n\n<p>The <code>Configuration</code> option allows you to choose the hardware configuration for training. We try to automatically select the correct setting for your system, but it can also be changed. Details on the configurations can be found <a href=\"#training-your-own-model\">here</a>.</p>\n\n<h1 id=\"using-the-command-line-interface-cli\">Using the Command Line Interface (CLI)</h1>\n\n<p><code>micro-sam</code> extends access to a bunch of functionalities using the command line interface (CLI) scripts via terminal.</p>\n\n<p>The supported CLIs can be used by</p>\n\n<ul>\n<li>Running <code>$ micro_sam.precompute_embeddings</code> for precomputing and caching the image embeddings.</li>\n<li>Running <code>$ micro_sam.annotator_2d</code> for starting the 2d annotator.</li>\n<li>Running <code>$ micro_sam.annotator_3d</code> for starting the 3d annotator.</li>\n<li>Running <code>$ micro_sam.annotator_tracking</code> for starting the tracking annotator.</li>\n<li>Running <code>$ micro_sam.image_series_annotator</code> for starting the image series annotator.</li>\n<li>Running <code>$ micro_sam.automatic_segmentation</code> for automatic instance segmentation.\n<ul>\n<li>We support all post-processing parameters for automatic instance segmentation (for both AMG and AIS).\n<ul>\n<li>The automatic segmentation mode can be controlled by: <code>--mode &lt;MODE_NAME&gt;</code>, where the available choice for <code>MODE_NAME</code> is <code>amg</code> / <code>ais</code>.</li>\n<li>AMG is supported by both default Segment Anything models and <code>micro-sam</code> models / finetuned models.</li>\n<li>AIS is supported by <code>micro-sam</code> models (or finetuned models; subjected to they are trained with the additional instance segmentation decoder)</li>\n</ul></li>\n<li>If these parameters are not provided by the user, <code>micro-sam</code> makes use of the best post-processing parameters (depending on the choice of model). </li>\n<li>The post-processing parameters can be changed by parsing the parameters via the CLI using <code>--&lt;PARAMETER_NAME&gt; &lt;VALUE&gt;.</code> For example, one can update the parameter values (eg. <code>pred_iou_thresh</code>, <code>stability_iou_thresh</code>, etc. - supported by AMG) using\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>micro_sam.automatic_segmentation<span class=\"w\"> </span>...<span class=\"w\"> </span>--pred_iou_thresh<span class=\"w\"> </span><span class=\"m\">0</span>.6<span class=\"w\"> </span>--stability_iou_thresh<span class=\"w\"> </span><span class=\"m\">0</span>.6<span class=\"w\"> </span>...\n</code></pre>\n</div></li>\n</ul></li>\n</ul>\n\n<pre><code>    - Remember to specify the automatic segmentation mode using `--mode &lt;MODE_NAME&gt;` when using additional post-processing parameters.\n- You can check details for supported parameters and their respective default values at `micro_sam/instance_segmentation.py` under the `generate` method for `AutomaticMaskGenerator` and `InstanceSegmentationWithDecoder` class.\n</code></pre>\n\n<p>NOTE: For all CLIs above, you can find more details by adding the argument <code>-h</code> to the CLI script (eg. <code>$ micro_sam.annotator_2d -h</code>).</p>\n\n<h1 id=\"using-the-python-library\">Using the Python Library</h1>\n\n<p>The python library can be imported via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>This library extends the <a href=\"https://github.com/facebookresearch/segment-anything\">Segment Anything library</a> and</p>\n\n<ul>\n<li>implements functions to apply Segment Anything to 2d and 3d data in <code>micro_sam.prompt_based_segmentation</code>.</li>\n<li>provides improved automatic instance segmentation functionality in <code>micro_sam.instance_segmentation</code>.</li>\n<li>implements training functionality that can be used for finetuning Segment Anything on your own data in <code>micro_sam.training</code>.</li>\n<li>provides functionality for quantitative and qualitative evaluation of Segment Anything models in <code>micro_sam.evaluation</code>.</li>\n</ul>\n\n<p>You can import these sub-modules via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam.prompt_based_segmentation</span>\n<span class=\"kn\">import</span> <span class=\"nn\">micro_sam.instance_segmentation</span>\n<span class=\"c1\"># etc.</span>\n</code></pre>\n</div>\n\n<p>This functionality is used to implement the interactive annotation tools in <code>micro_sam.sam_annotator</code> and can be used as a standalone python library.\nWe provide jupyter notebooks that demonstrate how to use it <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/notebooks\">here</a>. You can find the full library documentation by scrolling to the end of this page. </p>\n\n<h2 id=\"training-your-own-model\">Training your Own Model</h2>\n\n<p>We reimplement the training logic described in the <a href=\"https://arxiv.org/abs/2304.02643\">Segment Anything publication</a> to enable finetuning on custom data.\nWe use this functionality to provide the <a href=\"#finetuned-models\">finetuned microscopy models</a> and it can also be used to train models on your own data.\nIn fact the best results can be expected when finetuning on your own data, and we found that it does not require much annotated training data to get significant improvements in model performance.\nSo a good strategy is to annotate a few images with one of the provided models using our interactive annotation tools and, if the model is not working as good as required for your use-case, finetune on the annotated data.\nWe recommend checking out our latest <a href=\"https://doi.org/10.1101/2023.08.21.554208\">preprint</a> for details on the results on how much data is required for finetuning Segment Anything.</p>\n\n<p>The training logic is implemented in <code>micro_sam.training</code> and is based on <a href=\"https://github.com/constantinpape/torch-em\">torch-em</a>. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb\">the finetuning notebook</a> to see how to use it.\nWe also support training an additional decoder for automatic instance segmentation. This yields better results than the automatic mask generation of segment anything and is significantly faster.\nThe notebook explains how to train it together with the rest of SAM and how to then use it.</p>\n\n<p>More advanced examples, including quantitative and qualitative evaluation, can be found in <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/finetuning\">the finetuning directory</a>, which contains the code for training and evaluating <a href=\"finetuned-models\">our models</a>. You can find further information on model training in the <a href=\"fine-tuning-questions\">FAQ section</a>.</p>\n\n<p>Here is a list of resources, together with their recommended training settings, for which we have tested model finetuning:</p>\n\n<table>\n<thead>\n<tr>\n  <th>Resource Name</th>\n  <th>Capacity</th>\n  <th>Model Type</th>\n  <th>Batch Size</th>\n  <th>Finetuned Parts</th>\n  <th>Number of Objects</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>CPU</td>\n  <td>32GB</td>\n  <td>ViT Base</td>\n  <td>1</td>\n  <td><em>all</em></td>\n  <td>10</td>\n</tr>\n<tr>\n  <td>CPU</td>\n  <td>64GB</td>\n  <td>ViT Base</td>\n  <td>1</td>\n  <td><em>all</em></td>\n  <td>15</td>\n</tr>\n<tr>\n  <td>GPU (NVIDIA GTX 1080Ti)</td>\n  <td>8GB</td>\n  <td>ViT Base</td>\n  <td>1</td>\n  <td>Mask Decoder, Prompt Encoder</td>\n  <td>10</td>\n</tr>\n<tr>\n  <td>GPU (NVIDIA Quadro RTX5000)</td>\n  <td>16GB</td>\n  <td>ViT Base</td>\n  <td>1</td>\n  <td><em>all</em></td>\n  <td>10</td>\n</tr>\n<tr>\n  <td>GPU (Tesla V100)</td>\n  <td>32GB</td>\n  <td>ViT Base</td>\n  <td>1</td>\n  <td><em>all</em></td>\n  <td>10</td>\n</tr>\n<tr>\n  <td>GPU (NVIDIA A100)</td>\n  <td>80GB</td>\n  <td>ViT Tiny</td>\n  <td>2</td>\n  <td><em>all</em></td>\n  <td>50</td>\n</tr>\n<tr>\n  <td>GPU (NVIDIA A100)</td>\n  <td>80GB</td>\n  <td>ViT Base</td>\n  <td>2</td>\n  <td><em>all</em></td>\n  <td>40</td>\n</tr>\n<tr>\n  <td>GPU (NVIDIA A100)</td>\n  <td>80GB</td>\n  <td>ViT Large</td>\n  <td>2</td>\n  <td><em>all</em></td>\n  <td>30</td>\n</tr>\n<tr>\n  <td>GPU (NVIDIA A100)</td>\n  <td>80GB</td>\n  <td>ViT Huge</td>\n  <td>2</td>\n  <td><em>all</em></td>\n  <td>25</td>\n</tr>\n</tbody>\n</table>\n\n<blockquote>\n  <p>NOTE: If you use the <a href=\"#finetuning-ui\">finetuning UI</a> or <code>micro_sam.training.training.train_sam_for_configuration</code> you can specify the hardware configuration and the best settings for it will be set automatically. If your hardware is not in the settings we have tested choose the closest match. You can set the training parameters yourself when using <code>micro_sam.training.training.train_sam</code>. Be aware that the choice for the number of objects per image, the batch size, and the type of model have a strong impact on the VRAM needed for training and the duration of training. See the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb\">finetuning notebook</a> for an overview of these parameters.</p>\n</blockquote>\n\n<h1 id=\"finetuned-models\">Finetuned Models</h1>\n\n<p>In addition to the original Segment Anything models, we provide models that are finetuned on microscopy data.\nThey are available in the <a href=\"https://bioimage.io/#/\">BioImage.IO Model Zoo</a> and are also hosted on Zenodo.</p>\n\n<p>We currently offer the following models:</p>\n\n<ul>\n<li><code>vit_h</code>: Default Segment Anything model with ViT Huge backbone.</li>\n<li><code>vit_l</code>: Default Segment Anything model with ViT Large backbone.</li>\n<li><code>vit_b</code>: Default Segment Anything model with ViT Base backbone.</li>\n<li><code>vit_t</code>: Segment Anything model with ViT Tiny backbone. From the <a href=\"https://arxiv.org/abs/2306.14289\">Mobile SAM publication</a>.</li>\n<li><code>vit_l_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with ViT Large backbone. (<a href=\"https://doi.org/10.5281/zenodo.11111176\">Zenodo</a>) (<a href=\"TODO\">idealistic-rat on BioImage.IO</a>)</li>\n<li><code>vit_b_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with ViT Base backbone. (<a href=\"https://zenodo.org/doi/10.5281/zenodo.11103797\">Zenodo</a>) (<a href=\"TODO\">diplomatic-bug on BioImage.IO</a>)</li>\n<li><code>vit_t_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with ViT Tiny backbone. (<a href=\"https://doi.org/10.5281/zenodo.11111328\">Zenodo</a>) (<a href=\"TODO\">faithful-chicken BioImage.IO</a>)</li>\n<li><code>vit_l_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with ViT Large backbone. (<a href=\"https://doi.org/10.5281/zenodo.11111054\">Zenodo</a>) (<a href=\"TODO\">humorous-crab on BioImage.IO</a>)</li>\n<li><code>vit_b_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with ViT Base backbone. (<a href=\"https://doi.org/10.5281/zenodo.11111293\">Zenodo</a>) (<a href=\"TODO\">noisy-ox on BioImage.IO</a>)</li>\n<li><code>vit_t_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with ViT Tiny backbone. (<a href=\"https://doi.org/10.5281/zenodo.11110950\">Zenodo</a>) (<a href=\"TODO\">greedy-whale on BioImage.IO</a>)</li>\n</ul>\n\n<p>See the two figures below of the improvements through the finetuned model for LM and EM data. </p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/lm_comparison.png\" width=\"768\"></p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/em_comparison.png\" width=\"768\"></p>\n\n<p>You can select which model to use in the <a href=\"#annotation-tools\">annotation tools</a> by selecting the corresponding name in the <code>Model:</code> drop-down menu in the embedding menu:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/model-type-selector.png\" width=\"256\"></p>\n\n<p>To use a specific model in the python library you need to pass the corresponding name as value to the <code>model_type</code> parameter exposed by all relevant functions.\nSee for example the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L62\">2d annotator example</a>.</p>\n\n<h2 id=\"choosing-a-model\">Choosing a Model</h2>\n\n<p>As a rule of thumb:</p>\n\n<ul>\n<li>Use the <code>vit_l_lm</code> or <code>vit_b_lm</code> model for segmenting cells or nuclei in light microscopy. The larger model (<code>vit_l_lm</code>) yields a bit better segmentation quality, especially for automatic segmentation, but needs more computational resources.</li>\n<li>Use the <code>vit_l_em_organelles</code> or <code>vit_b_em_organelles</code> models for segmenting mitochondria, nuclei or other  roundish organelles in electron microscopy.</li>\n<li>For other use-cases use one of the default models.</li>\n<li>The <code>vit_t_...</code> models run much faster than other models, but yield inferior quality for many applications. It can still make sense to try them for your use-case if your working on a laptop and want to annotate many images or volumetric data. </li>\n</ul>\n\n<p>See also the figures above for examples where the finetuned models work better than the default models.\nWe are working on further improving these models and adding new models for other biomedical imaging domains.</p>\n\n<h2 id=\"other-models\">Other Models</h2>\n\n<p>Previous versions of our models are available on Zenodo:</p>\n\n<ul>\n<li><a href=\"https://zenodo.org/records/10524894\">vit_b_em_boundaries</a>: for segmenting compartments delineated by boundaries such as cells or neurites in EM.</li>\n<li><a href=\"https://zenodo.org/records/10524828\">vit_b_em_organelles</a>: for segmenting mitochondria, nuclei or other organelles in EM.</li>\n<li><a href=\"https://zenodo.org/records/10524791\">vit_b_lm</a>: for segmenting cells and nuclei in LM.</li>\n<li><a href=\"https://zenodo.org/records/8250291\">vit_h_em</a>: for general EM segmentation.</li>\n<li><a href=\"https://zenodo.org/records/8250299\">vit_h_lm</a>: for general LM segmentation.</li>\n</ul>\n\n<p>We do not recommend to use these models since our new models improve upon them significantly. But we provide the links here in case they are needed to reproduce older segmentation workflows.</p>\n\n<p>We provide additional models that were used for experiments in our publication on Zenodo:</p>\n\n<ul>\n<li><a href=\"https://doi.org/10.5281/zenodo.11115426\">LIVECell Specialist Models</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11115998\">TissueNet Specialist Models</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11116407\">NeurIPS CellSeg Specialist Models</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11115827\">DeepBacs Specialist Models</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11116603\">PlantSeg (Root) Specialist Models</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11117314\">CREMI Specialist Models</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11117144\">ASEM (ER) Specialist Models</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11117559\">The LM Generalist Model with ViT-H backend (vit_h_lm)</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11117495\">The EM Generalist Model with ViT-H backend (vit_h_em_organelles)</a></li>\n<li><a href=\"https://doi.org/10.5281/zenodo.11117615\">Finetuned Models for the user studies</a></li>\n</ul>\n\n<h1 id=\"faq\">FAQ</h1>\n\n<p>Here we provide frequently asked questions and common issues.\nIf you encounter a problem or question not addressed here feel free to <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/new\">open an issue</a> or to ask your question on <a href=\"https://forum.image.sc/\">image.sc</a> with the tag <code>micro-sam</code>.</p>\n\n<h2 id=\"installation-questions\">Installation questions</h2>\n\n<h3 id=\"1-how-to-install-micro_sam\">1. How to install <code>micro_sam</code>?</h3>\n\n<p>The <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation\">installation</a> for <code>micro_sam</code> is supported in three ways: <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-conda\">from conda</a> (recommended), <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-source\">from source</a> and <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-installer\">from installers</a>. Check out our <a href=\"https://youtu.be/gcv0fa84mCc\">tutorial video</a> to get started with <code>micro_sam</code>, briefly walking you through the installation process and how to start the tool.</p>\n\n<h3 id=\"2-i-cannot-install-micro_sam-using-the-installer-i-am-getting-some-errors\">2. I cannot install <code>micro_sam</code> using the installer, I am getting some errors.</h3>\n\n<p>The installer should work out-of-the-box on Windows and Linux platforms. Please open an issue to report the error you encounter.</p>\n\n<blockquote>\n  <p>NOTE: The installers enable using <code>micro_sam</code> without conda. However, we recommend the installation from conda or from source to use all its features seamlessly. Specifically, the installers currently only support the CPU and won't enable you to use the GPU (if you have one). </p>\n</blockquote>\n\n<h3 id=\"3-what-is-the-minimum-system-requirement-for-micro_sam\">3. What is the minimum system requirement for <code>micro_sam</code>?</h3>\n\n<p>From our experience, the <code>micro_sam</code> annotation tools work seamlessly on most laptop or workstation CPUs and with &gt; 8GB RAM.\nYou might encounter some slowness for $\\leq$ 8GB RAM. The resources <code>micro_sam</code>'s annotation tools have been tested on are:</p>\n\n<ul>\n<li>Windows:\n<ul>\n<li>Windows 10 Pro, Intel i5 7th Gen, 8GB RAM</li>\n<li>Windows 10 Enterprise LTSC, Intel i7 13th Gen, 32GB RAM</li>\n<li>Windows 10 Pro for Workstations, Intel Xeon W-2295, 128GB RAM</li>\n</ul></li>\n</ul>\n\n<ul>\n<li><p>Linux:</p>\n\n<ul>\n<li>Ubuntu 20.04, Intel i7 11th Gen, 32GB RAM</li>\n<li>Ubuntu 22.04, Intel i7 12th Gen, 32GB RAM</li>\n</ul></li>\n<li><p>Mac:</p>\n\n<ul>\n<li>macOS Sonoma 14.4.1\n<ul>\n<li>M1 Chip, 8GB RAM</li>\n<li>M3 Max Chip, 36GB RAM</li>\n</ul></li>\n</ul></li>\n</ul>\n\n<p>Having a GPU will significantly speed up the annotation tools and especially the model finetuning.</p>\n\n<h3 id=\"4-what-is-the-recommended-pytorch-version\">4. What is the recommended PyTorch version?</h3>\n\n<p><code>micro_sam</code> has been tested mostly with CUDA 12.1 and PyTorch [2.1.1, 2.2.0]. However, the tool and the library is not constrained to a specific PyTorch or CUDA version. So it should work fine with the standard PyTorch installation for your system.</p>\n\n<h3 id=\"5-i-am-missing-a-few-packages-eg-modulenotfounderror-no-module-named-elfio-what-should-i-do\">5. I am missing a few packages (eg. <code>ModuleNotFoundError: No module named 'elf.io</code>). What should I do?</h3>\n\n<p>With the latest release 1.0.0, the installation from conda and source should take care of this and install all the relevant packages for you.\nSo please reinstall <code>micro_sam</code>, following <a href=\"#installation\">the installation guide</a>.</p>\n\n<h3 id=\"6-can-i-install-micro_sam-using-pip\">6. Can I install <code>micro_sam</code> using pip?</h3>\n\n<p>We do <em>not</em> recommend installing <code>micro-sam</code> with pip. It has several dependencies that are only avaoiable from conda-forge, which will not install correctly via pip.</p>\n\n<p>Please see <a href=\"#installation\">the installation guide</a> for the recommended way to install <code>micro-sam</code>.</p>\n\n<p>The PyPI page for <code>micro-sam</code> exists only so that the <a href=\"https://www.napari-hub.org/\">napari-hub</a> can find it.</p>\n\n<h3 id=\"7-i-get-the-following-error-importerror-cannot-import-name-unetr-from-torch_emmodel\">7. I get the following error: <code>importError: cannot import name 'UNETR' from 'torch_em.model'</code>.</h3>\n\n<p>It's possible that you have an older version of <code>torch-em</code> installed. Similar errors could often be raised from other libraries, the reasons being: a) Outdated packages installed, or b) Some non-existent module being called. If the source of such error is from <code>micro_sam</code>, then <code>a)</code> is most likely the reason . We recommend installing the latest version following the <a href=\"https://github.com/constantinpape/torch-em?tab=readme-ov-file#installation\">installation instructions</a>.</p>\n\n<h3 id=\"8-my-system-does-not-support-internet-connection-where-should-i-put-the-model-checkpoints-for-the-micro-sam-models\">8. My system does not support internet connection. Where should I put the model checkpoints for the <code>micro-sam</code> models?</h3>\n\n<p>We recommend transferring the model checkpoints to the system-level cache directory (you can find yours by running the following in terminal: <code>python -c \"from micro_sam import util; print(util.microsam_cachedir())</code>). Once you have identified the cache directory, you need to create an additional <code>models</code> directory inside the <code>micro-sam</code> cache directory (if not present already) and move the model checkpoints there. At last, you <strong>must</strong> rename the transferred checkpoints as per the respective <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/util.py#L87\">key values</a> in the url dictionaries located in the <code>micro_sam.util.models</code> function (below mentioned is an example for Linux users).</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># Download and transfer the model checkpoints for &#39;vit_b_lm&#39; and `vit_b_lm_decoder`.</span>\n<span class=\"c1\"># Next, verify the cache directory.</span>\n&gt;<span class=\"w\"> </span>python<span class=\"w\"> </span>-c<span class=\"w\"> </span><span class=\"s2\">&quot;from micro_sam import util; print(util.microsam_cachedir())&quot;</span>\n/home/anwai/.cache/micro_sam\n\n<span class=\"c1\"># Create &#39;models&#39; folder in the cache directory</span>\n&gt;<span class=\"w\"> </span>mkdir<span class=\"w\"> </span>/home/anwai/.cache/micro_sam/models\n\n<span class=\"c1\"># Move the checkpoints to the models directory and rename them</span>\n<span class=\"c1\"># The following steps transfer and rename the checkpoints to the desired filenames.</span>\n&gt;<span class=\"w\"> </span>mv<span class=\"w\"> </span>vit_b.pt<span class=\"w\"> </span>/home/anwai/.cache/micro_sam/models/vit_b_lm\n&gt;<span class=\"w\"> </span>mv<span class=\"w\"> </span>vit_b_decoder.pt<span class=\"w\"> </span>/home/anwai/.cache/micro_sam/models/vit_b_lm_decoder\n</code></pre>\n</div>\n\n<h2 id=\"usage-questions\">Usage questions</h2>\n\n<p><!---\nTODO provide relevant links here.\n--></p>\n\n<h3 id=\"1-i-have-some-micropscopy-images-can-i-use-the-annotator-tool-for-segmenting-them\">1. I have some micropscopy images. Can I use the annotator tool for segmenting them?</h3>\n\n<p>Yes, you can use the annotator tool for:</p>\n\n<ul>\n<li>Segmenting objects in 2d images (using automatic and/or interactive segmentation).</li>\n<li>Segmenting objects in 3d volumes (using automatic and/or interactive segmentation for the entire object(s)).</li>\n<li>Tracking objects over time in time-series data.</li>\n<li>Segmenting objects in a series of 2d / 3d images.</li>\n<li>In addition, you can finetune the Segment Anything / <code>micro_sam</code> models on your own microscopy data, in case the provided models do not suffice your needs. One caveat: You need to annotate a few objects before-hand (<code>micro_sam</code> has the potential of improving interactive segmentation with only a few annotated objects) to proceed with the supervised finetuning procedure.</li>\n</ul>\n\n<h3 id=\"2-which-model-should-i-use-for-my-data\">2. Which model should I use for my data?</h3>\n\n<p>We currently provide three different kind of models: the default models <code>vit_h</code>, <code>vit_l</code>, <code>vit_b</code> and <code>vit_t</code>; the models for light microscopy <code>vit_l_lm</code>, <code>vit_b_lm</code> and <code>vit_t_lm</code>; the models for electron microscopy <code>vit_l_em_organelles</code>, <code>vit_b_em_organelles</code> and <code>vit_t_em_organelles</code>.\nYou should first try the model that best fits the segmentation task your interested in, the <code>lm</code> model for cell or nucleus segmentation in light microscopy or the <code>em_organelles</code> model for segmenting nuclei, mitochondria or other roundish organelles in electron microscopy.\nIf your segmentation problem does not meet these descriptions, or if these models don't work well, you should try one of the default models instead.\nThe letter after <code>vit</code> denotes the size of the image encoder in SAM, <code>h</code> (huge) being the largest and <code>t</code> (tiny) the smallest. The smaller models are faster but may yield worse results. We recommend to either use a <code>vit_l</code> or <code>vit_b</code> model, they offer the best trade-off between speed and segmentation quality.\nYou can find more information on model choice <a href=\"#choosing-a-model\">here</a>.</p>\n\n<h3 id=\"3-i-have-high-resolution-microscopy-images-micro_sam-does-not-seem-to-work\">3. I have high-resolution microscopy images, <code>micro_sam</code> does not seem to work.</h3>\n\n<p>The Segment Anything model expects inputs of shape 1024 x 1024 pixels. Inputs that do not match this size will be internally resized to match it. Hence, applying Segment Anything to a much larger image will often lead to inferior results, or sometimes not work at all. To address this, <code>micro_sam</code> implements tiling: cutting up the input image into tiles of a fixed size (with a fixed overlap) and running Segment Anything for the individual tiles. You can activate tiling with the <code>tile_shape</code> parameter, which determines the size of the inner tile and <code>halo</code>, which determines the size of the additional overlap.</p>\n\n<ul>\n<li>If you are using the <code>micro_sam</code> annotation tools, you can specify the values for the <code>tile_shape</code> and <code>halo</code> via the <code>tile_x</code>, <code>tile_y</code>, <code>halo_x</code> and <code>halo_y</code> parameters in the <code>Embedding Settings</code> drop-down menu.</li>\n<li>If you are using the <code>micro_sam</code> library in a python script, you can pass them as tuples, e.g. <code>tile_shape=(1024, 1024), halo=(256, 256)</code>. See also the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L47-L63\">wholeslide annotator example</a>.</li>\n<li>If you are using the command line functionality, you can pass them via the options <code>--tile_shape 1024 1024 --halo 256 256</code>.</li>\n</ul>\n\n<blockquote>\n  <p>NOTE: It's recommended to choose the <code>halo</code> so that it is larger than half of the maximal radius of the objects you want to segment.</p>\n</blockquote>\n\n<h3 id=\"4-the-computation-of-image-embeddings-takes-very-long-in-napari\">4. The computation of image embeddings takes very long in napari.</h3>\n\n<p><code>micro_sam</code> pre-computes the image embeddings produced by the vision transformer backbone in Segment Anything, and (optionally) stores them on disc. I fyou are using a CPU, this step can take a while for 3d data or time-series (you will see a progress bar in the command-line interface / on the bottom right of napari). If you have access to a GPU without graphical interface (e.g. via a local computer cluster or a cloud provider), you can also pre-compute the embeddings there and then copy them over to your laptop / local machine to speed this up.</p>\n\n<ul>\n<li>You can use the command <code>micro_sam.precompute_embeddings</code> for this (it is installed with the rest of the software). You can specify the location of the pre-computed embeddings via the <code>embedding_path</code> argument.</li>\n<li>You can cache the computed embedding in the napari tool (to avoid recomputing the embeddings again) by passing the path to store the embeddings in the <code>embeddings_save_path</code> option in the <code>Embedding Settings</code> drop-down. You can later load the pre-computed image embeddings by entering the path to the stored embeddings there as well.</li>\n</ul>\n\n<h3 id=\"5-can-i-use-micro_sam-on-a-cpu\">5. Can I use <code>micro_sam</code> on a CPU?</h3>\n\n<p>Most other processing steps are very fast even on a CPU, the automatic segmentation step for the default Segment Anything models (typically called as the \"Segment Anything\" feature or AMG - Automatic Mask Generation) however takes several minutes without a GPU (depending on the image size). For large volumes and time-series, segmenting an object interactively in 3d / tracking across time can take a couple of seconds with a CPU (it is very fast with a GPU).</p>\n\n<blockquote>\n  <p>HINT: All the tutorial videos have been created on CPU resources.</p>\n</blockquote>\n\n<h3 id=\"6-i-generated-some-segmentations-from-another-tool-can-i-use-it-as-a-starting-point-in-micro_sam\">6. I generated some segmentations from another tool, can I use it as a starting point in <code>micro_sam</code>?</h3>\n\n<p>You can save and load the results from the <code>committed_objects</code> layer to correct segmentations you obtained from another tool (e.g. CellPose) or save intermediate annotation results. The results can be saved via <code>File</code> -> <code>Save Selected Layers (s) ...</code> in the napari menu-bar on top (see the tutorial videos for details). They can be loaded again by specifying the corresponding location via the <code>segmentation_result</code> parameter in the CLI or python script (2d and 3d segmentation).\nIf you are using an annotation tool you can load the segmentation you want to edit as segmentation layer and rename it to <code>committed_objects</code>.</p>\n\n<h3 id=\"7-i-am-using-micro_sam-for-segmenting-objects-i-would-like-to-report-the-steps-for-reproducability-how-can-this-be-done\">7. I am using <code>micro_sam</code> for segmenting objects. I would like to report the steps for reproducability. How can this be done?</h3>\n\n<p>The annotation steps and segmentation results can be saved to a Zarr file by providing the <code>commit_path</code> in the <code>commit</code> widget. This file will contain all relevant information to reproduce the segmentation.</p>\n\n<blockquote>\n  <p>NOTE: This feature is still under development and we have not implemented rerunning the segmentation from this file yet. See <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/408\">this issue</a> for details.</p>\n</blockquote>\n\n<h3 id=\"8-i-want-to-segment-objects-with-complex-structures-both-the-default-segment-anything-models-and-the-micro_sam-generalist-models-do-not-work-for-my-data-what-should-i-do\">8. I want to segment objects with complex structures. Both the default Segment Anything models and the <code>micro_sam</code> generalist models do not work for my data. What should I do?</h3>\n\n<p><code>micro_sam</code> supports interactive annotation using positive and negative point prompts, box prompts and polygon drawing. You can combine multiple types of prompts to improve the segmentation quality. In case the aforementioned suggestions do not work as desired, <code>micro_sam</code> also supports finetuning a model on your data (see the next section on <a href=\"#fine-tuning-questions\">finetuning</a>). We recommend the following: a) Check which of the provided models performs relatively good on your data, b) Choose the best model as the starting point to train your own specialist model for the desired segmentation task.</p>\n\n<h3 id=\"9-i-am-using-the-annotation-tool-and-napari-outputs-the-following-error-while-emmitting-signal-an-error-ocurred-in-callback-this-is-not-a-bug-in-psygnal-see-above-for-details\">9. I am using the annotation tool and napari outputs the following error: <code>While emmitting signal ... an error ocurred in callback ... This is not a bug in psygnal. See ... above for details.</code></h3>\n\n<p>These messages occur when an internal error happens in <code>micro_sam</code>. In most cases this is due to inconsistent annotations and you can fix them by clearing the annotations.\nWe want to remove these errors, so we would be very grateful if you can <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues\">open an issue</a> and describe the steps you did when encountering it.</p>\n\n<h3 id=\"10-the-objects-are-not-segmented-in-my-3d-data-using-the-interactive-annotation-tool\">10. The objects are not segmented in my 3d data using the interactive annotation tool.</h3>\n\n<p>The first thing to check is: a) make sure you are using the latest version of <code>micro_sam</code> (pull the latest commit from master if your installation is from source, or update the installation from conda using <code>conda update micro_sam</code>), and b) try out the steps from the <a href=\"https://youtu.be/nqpyNQSyu74\">3d annotation tutorial video</a> to verify if this shows the same behaviour (or the same errors) as you faced. For 3d images, it's important to pass the inputs in the python axis convention, ZYX.\nc) try using a different model and change the projection mode for 3d segmentation. This is also explained in the video.</p>\n\n<h3 id=\"11-i-have-very-small-or-fine-grained-structures-in-my-high-resolution-microscopic-images-can-i-use-micro_sam-to-annotate-them\">11. I have very small or fine-grained structures in my high-resolution microscopic images. Can I use <code>micro_sam</code> to annotate them?</h3>\n\n<p>Segment Anything does not work well for very small or fine-grained objects (e.g. filaments). In these cases, you could try to use tiling to improve results (see <a href=\"#3-i-have-high-resolution-large-tomograms-micro-sam-does-not-seem-to-work\">Point 3</a> above for details).</p>\n\n<h3 id=\"12-napari-seems-to-be-very-slow-for-large-images\">12. napari seems to be very slow for large images.</h3>\n\n<p>Editing (drawing / erasing) very large 2d images or 3d volumes is known to be slow at the moment, as the objects in the layers are stored in-memory. See the related <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/39\">issue</a>.</p>\n\n<h3 id=\"13-while-computing-the-embeddings-and-or-automatic-segmentation-a-window-stating-napari-is-not-responding-pops-up\">13. While computing the embeddings (and / or automatic segmentation), a window stating: <code>\"napari\" is not responding</code> pops up.</h3>\n\n<p>This can happen for long running computations. You just need to wait a bit longer and the computation will finish.</p>\n\n<h3 id=\"14-i-have-3d-rgb-microscopy-volumes-how-does-micro_sam-handle-these-images\">14. I have 3D RGB microscopy volumes. How does <code>micro_sam</code> handle these images?</h3>\n\n<p><code>micro_sam</code> performs automatic segmentation in 3D volumes by first segmenting slices individually in 2D and merging the segmentations across 3D based on overlap of objects between slices. The expected shape of your 3D RGB volume should be <code>(Z * Y * X * 3)</code> (reason: Segment Anything is devised to consider 3-channel inputs, so while the user provides micro-sam with 1-channel inputs, we handle this by triplicating this to fit the requirement, or with 3-channel inputs, we use them in the expected RGB array structures as it is).</p>\n\n<h3 id=\"15-i-want-to-use-a-model-stored-in-a-different-directory-than-the-micro_sam-cache-how-can-i-do-this\">15. I want to use a model stored in a different directory than the <code>micro_sam</code> cache. How can I do this?</h3>\n\n<p>The <code>micro-sam</code> CLIs for precomputation of image embeddings and annotators (Annotator 2d, Annotator 3d, Annotator Tracking, Image Series Annotator) accept the argument <code>-c</code> / <code>--checkpoint</code> to pass model checkpoints. If you start a <code>micro-sam</code> annotator from the <code>napari</code> plugin menu, you can provide the path to model checkpoints in the annotator widget (on right) under <code>Embedding Settings</code> drop-down in the <code>custom weights path</code> option.</p>\n\n<p>NOTE: It is important to choose the correct model type when you opt for the above recommendation, using the <code>-m / --model_type</code> argument or selecting it from the <code>Model</code> dropdown in <code>Embedding Settings</code> respectively. Otherwise you will face parameter mismatch issues.</p>\n\n<h3 id=\"16-some-parameters-in-the-annotator-finetuning-widget-are-unclear-to-me\">16. Some parameters in the annotator / finetuning widget are unclear to me.</h3>\n\n<p><code>micro-sam</code> has tooltips for menu options across all widgets (i.e. an information window will appear if you hover over name of the menu), which briefly describe the utility of the specific menu option.</p>\n\n<h2 id=\"fine-tuning-questions\">Fine-tuning questions</h2>\n\n<h3 id=\"1-i-have-a-microscopy-dataset-i-would-like-to-fine-tune-segment-anything-for-is-it-possible-using-micro_sam\">1. I have a microscopy dataset I would like to fine-tune Segment Anything for. Is it possible using <code>micro_sam</code>?</h3>\n\n<p>Yes, you can fine-tune Segment Anything on your own dataset. Here's how you can do it:</p>\n\n<ul>\n<li>Check out the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb\">tutorial notebook</a> on how to fine-tune Segment Anything with our <code>micro_sam.training</code> library.</li>\n<li>Or check the <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning\">examples</a> for additional scripts that demonstrate finetuning.</li>\n<li>If you are not familiar with coding in python at all then you can also use the <a href=\"finetuning-ui\">graphical interface for finetuning</a>. But we recommend using a script for more flexibility and reproducibility.</li>\n</ul>\n\n<h3 id=\"2-i-would-like-to-fine-tune-segment-anything-on-open-source-cloud-services-eg-kaggle-notebooks-is-it-possible\">2. I would like to fine-tune Segment Anything on open-source cloud services (e.g. Kaggle Notebooks), is it possible?</h3>\n\n<p>Yes, you can fine-tune Segment Anything on your custom datasets on Kaggle (and <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#using-micro_sam-on-band\">BAND</a>). Check out our <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb\">tutorial notebook</a> for this.</p>\n\n<p><!---\nTODO: we should improve this explanation and add a small image that visualizes the labels.\n--></p>\n\n<h3 id=\"3-what-kind-of-annotations-do-i-need-to-finetune-segment-anything\">3. What kind of annotations do I need to finetune Segment Anything?</h3>\n\n<p>Annotations are referred to the instance segmentation labels, i.e. each object of interests in your microscopy images have an individual id to uniquely identify all the segmented objects. You can obtain them by <code>micro_sam</code>'s annotation tools. In <code>micro_sam</code>, it's expected to provide dense segmentations (i.e. all objects per image are annotated) for finetuning Segment Anything with the additional decoder, however it's okay to use sparse segmentations (i.e. few objects per image are annotated) for just finetuning Segment Anything (without the additional decoder).</p>\n\n<h3 id=\"4-i-have-finetuned-segment-anything-on-my-microscopy-data-how-can-i-use-it-for-annotating-new-images\">4. I have finetuned Segment Anything on my microscopy data. How can I use it for annotating new images?</h3>\n\n<p>You can load your finetuned model by entering the path to its checkpoint in the <code>custom_weights_path</code> field in the <code>Embedding Settings</code> drop-down menu.\nIf you are using the python library or CLI you can specify this path with the <code>checkpoint_path</code> parameter.</p>\n\n<h3 id=\"5-what-is-the-background-of-the-new-ais-automatic-instance-segmentation-feature-in-micro_sam\">5. What is the background of the new AIS (Automatic Instance Segmentation) feature in <code>micro_sam</code>?</h3>\n\n<p><code>micro_sam</code> introduces a new segmentation decoder to the Segment Anything backbone, for enabling faster and accurate automatic instance segmentation, by predicting the <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/transform/label.py#L284\">distances to the object center and boundary</a> as well as predicting foregrund, and performing <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/util/segmentation.py#L122\">seeded watershed-based postprocessing</a> to obtain the instances.</p>\n\n<h3 id=\"6-i-want-to-finetune-only-the-segment-anything-model-without-the-additional-instance-decoder\">6. I want to finetune only the Segment Anything model without the additional instance decoder.</h3>\n\n<p>The instance segmentation decoder is optional. So you can only finetune SAM or SAM and the additional decoder. Finetuning with the decoder will increase training times, but will enable you to use AIS. See <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning#example-for-model-finetuning\">this example</a> for finetuning with both the objectives.</p>\n\n<blockquote>\n  <p>NOTE: To try out the other way round (i.e. the automatic instance segmentation framework without the interactive capability, i.e. a UNETR: a vision transformer encoder and a convolutional decoder), you can take inspiration from this <a href=\"https://github.com/constantinpape/torch-em/blob/main/experiments/vision-transformer/unetr/for_vimunet_benchmarking/run_livecell.py\">example on LIVECell</a>.</p>\n</blockquote>\n\n<h3 id=\"7-i-have-a-nvidia-rtx-4090ti-gpu-with-24gb-vram-can-i-finetune-segment-anything\">7. I have a NVIDIA RTX 4090Ti GPU with 24GB VRAM. Can I finetune Segment Anything?</h3>\n\n<p>Finetuning Segment Anything is possible in most consumer-grade GPU and CPU resources (but training being a lot slower on the CPU). For the mentioned resource, it should be possible to finetune a ViT Base (also abbreviated as <code>vit_b</code>) by reducing the number of objects per image to 15.\nThis parameter has the biggest impact on the VRAM consumption and quality of the finetuned model.\nYou can find an overview of the resources we have tested for finetuning <a href=\"#training-your-own-model\">here</a>.\nWe also provide a the convenience function <code>micro_sam.training.train_sam_for_configuration</code> that selects the best training settings for these configuration. This function is also used by the finetuning UI.</p>\n\n<h3 id=\"8-i-want-to-create-a-dataloader-for-my-data-to-finetune-segment-anything\">8. I want to create a dataloader for my data, to finetune Segment Anything.</h3>\n\n<p>Thanks to <code>torch-em</code>, a) Creating PyTorch datasets and dataloaders using the python library is convenient and supported for various data formats and data structures.\nSee the <a href=\"https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\">tutorial notebook</a> on how to create dataloaders using <code>torch-em</code> and the <a href=\"https://github.com/constantinpape/torch-em/blob/main/doc/datasets_and_dataloaders.md\">documentation</a> for details on creating your own datasets and dataloaders; and b) finetuning using the <code>napari</code> tool eases the aforementioned process, by allowing you to add the input parameters (path to the directory for inputs and labels etc.) directly in the tool.</p>\n\n<blockquote>\n  <p>NOTE: If you have images with large input shapes with a sparse density of instance segmentations, we recommend using <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/data/sampler.py\"><code>sampler</code></a> for choosing the patches with valid segmentation for the finetuning purpose (see the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/finetuning/specialists/training/light_microscopy/plantseg_root_finetuning.py#L29\">example</a> for PlantSeg (Root) specialist model in <code>micro_sam</code>).</p>\n</blockquote>\n\n<h3 id=\"9-how-can-i-evaluate-a-model-i-have-finetuned\">9. How can I evaluate a model I have finetuned?</h3>\n\n<p>To validate a Segment Anything model for your data, you have different options, depending on the task you want to solve and whether you have segmentation annotations for your data.</p>\n\n<ul>\n<li>If you don't have any annotations you will have to validate the model visually. We suggest doing this with the <code>micro_sam</code> GUI tools. You can learn how to use them in the <code>micro_sam</code> documentation.</li>\n<li>If you have segmentation annotations you can use the <code>micro_sam</code> python library to evaluate the segmentation quality of different models. We provide functionality to evaluate the models for interactive and for automatic segmentation:\n<ul>\n<li>You can use <code>micro_sam.evaluation.evaluation.run_evaluation_for_iterative_prompting</code> to evaluate models for interactive segmentation.</li>\n<li>You can use <code>micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search_and_inference</code> to evaluate models for automatic segmentation.</li>\n</ul></li>\n</ul>\n\n<p>We provide an <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/inference_and_evaluation.ipynb\">example notebook</a> that shows how to use this evaluation functionality.</p>\n\n<h1 id=\"contribution-guide\">Contribution Guide</h1>\n\n<ul>\n<li><a href=\"#discuss-your-ideas\">Discuss your ideas</a></li>\n<li><a href=\"#clone-the-repository\">Clone the repository</a></li>\n<li><a href=\"#create-your-development-environment\">Create your development environment</a></li>\n<li><a href=\"#make-your-changes\">Make your changes</a></li>\n<li><a href=\"#testing\">Testing</a>\n<ul>\n<li><a href=\"#run-the-tests\">Run the tests</a></li>\n<li><a href=\"#writing-your-own-tests\">Writing your own tests</a></li>\n</ul></li>\n<li><a href=\"#open-a-pull-request\">Open a pull request</a></li>\n<li><a href=\"#optional-build-the-documentation\">Optional: Build the documentation</a></li>\n<li><a href=\"#optional-benchmark-performance\">Optional: Benchmark performance</a>\n<ul>\n<li><a href=\"#run-the-benchmark-script\">Run the benchmark script</a></li>\n<li><a href=\"#line-profiling\">Line profiling</a></li>\n<li><a href=\"#snakeviz-visualization\">Snakeviz visualization</a></li>\n<li><a href=\"#memory-profiling-with-memray\">Memory profiling with memray</a></li>\n</ul></li>\n</ul>\n\n<h3 id=\"discuss-your-ideas\">Discuss your ideas</h3>\n\n<p>We welcome new contributions! First, discuss your idea by opening a <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/new\">new issue</a> in micro-sam.\nThis allows you to ask questions, and have the current developers make suggestions about the best way to implement your ideas.</p>\n\n<h3 id=\"clone-the-repository\">Clone the repository</h3>\n\n<p>We use <a href=\"https://git-scm.com/\">git</a> for version control.</p>\n\n<p>Clone the repository, and checkout the development branch:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>git<span class=\"w\"> </span>clone<span class=\"w\"> </span>https://github.com/computational-cell-analytics/micro-sam.git\n$<span class=\"w\"> </span><span class=\"nb\">cd</span><span class=\"w\"> </span>micro-sam\n$<span class=\"w\"> </span>git<span class=\"w\"> </span>checkout<span class=\"w\"> </span>dev\n</code></pre>\n</div>\n\n<h3 id=\"create-your-development-environment\">Create your development environment</h3>\n\n<p>We use <a href=\"https://docs.conda.io/en/latest/\">conda</a> to <a href=\"https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\">manage our environments</a>. If you don't have this already, install <a href=\"https://docs.conda.io/projects/miniconda/en/latest/\">miniconda</a> or <a href=\"https://mamba.readthedocs.io/en/latest/\">mamba</a> to get started.</p>\n\n<p>Now you can create the environment, install user and developer dependencies, and micro-sam as an editable installation:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>conda<span class=\"w\"> </span>env<span class=\"w\"> </span>create<span class=\"w\"> </span>environment.yaml\nconda<span class=\"w\"> </span>activate<span class=\"w\"> </span>sam\npython<span class=\"w\"> </span>-m<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>requirements-dev.txt\npython<span class=\"w\"> </span>-m<span class=\"w\"> </span>pip<span class=\"w\"> </span>install<span class=\"w\"> </span>-e<span class=\"w\"> </span>.\n</code></pre>\n</div>\n\n<h3 id=\"make-your-changes\">Make your changes</h3>\n\n<p>Now it's time to make your code changes.</p>\n\n<p>Typically, changes are made branching off from the development branch. Checkout <code>dev</code> and then create a new branch to work on your changes.</p>\n\n<pre><code>$ git checkout dev\n$ git checkout -b my-new-feature\n</code></pre>\n\n<p>We use <a href=\"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\">google style python docstrings</a> to create documentation for all new code.</p>\n\n<p>You may also find it helpful to look at this <a href=\"#for-developers\">developer guide</a>, which explains the organization of the micro-sam code.</p>\n\n<h2 id=\"testing\">Testing</h2>\n\n<h3 id=\"run-the-tests\">Run the tests</h3>\n\n<p>The tests for micro-sam are run with <a href=\"https://docs.pytest.org/en/7.4.x/\">pytest</a></p>\n\n<p>To run the tests:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>pytest\n</code></pre>\n</div>\n\n<h3 id=\"writing-your-own-tests\">Writing your own tests</h3>\n\n<p>If you have written new code, you will need to write tests to go with it.</p>\n\n<h4 id=\"unit-tests\">Unit tests</h4>\n\n<p>Unit tests are the preferred style of tests for user contributions. Unit tests check small, isolated parts of the code for correctness. If your code is too complicated to write unit tests easily, you may need to consider breaking it up into smaller functions that are easier to test.</p>\n\n<h4 id=\"tests-involving-napari\">Tests involving napari</h4>\n\n<p>In cases where tests <em>must</em> use the napari viewer, <a href=\"https://napari.org/stable/plugins/test_deploy.html#tips-for-testing-napari-plugins\">these tips might be helpful</a> (in particular, the <code>make_napari_viewer_proxy</code> fixture).</p>\n\n<p>These kinds of tests should be used only in limited circumstances. Developers are <a href=\"https://napari.org/stable/plugins/test_deploy.html#prefer-smaller-unit-tests-when-possible\">advised to prefer smaller unit tests, and avoid integration tests</a> wherever possible.</p>\n\n<h4 id=\"code-coverage\">Code coverage</h4>\n\n<p>Pytest uses the <a href=\"https://pytest-cov.readthedocs.io/en/latest/\">pytest-cov</a> plugin to automatically determine which lines of code are covered by tests.</p>\n\n<p>A short summary report is printed to the terminal output whenever you run pytest. The full results are also automatically written to a file named <code>coverage.xml</code>.</p>\n\n<p>The <a href=\"https://marketplace.visualstudio.com/items?itemName=ryanluker.vscode-coverage-gutters\">Coverage Gutters VSCode extension</a> is useful for visualizing which parts of the code need better test coverage. PyCharm professional <a href=\"https://www.jetbrains.com/pycharm/guide/tips/spot-coverage-in-gutter/\">has a similar feature</a>, and you may be able to find similar tools for your preferred editor.</p>\n\n<p>We also use <a href=\"https://app.codecov.io/gh/computational-cell-analytics/micro-sam\">codecov.io</a> to display the code coverage results from our Github Actions continuous integration.</p>\n\n<h3 id=\"open-a-pull-request\">Open a pull request</h3>\n\n<p>Once you've made changes to the code and written some tests to go with it, you are ready to <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests\">open a pull request</a>. You can <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-requests\">mark your pull request as a draft</a> if you are still working on it, and still get the benefit of discussing the best approach with maintainers.</p>\n\n<p>Remember that typically changes to micro-sam are made branching off from the development branch. So, you will need to open your pull request to merge back into the <code>dev</code> branch <a href=\"https://github.com/computational-cell-analytics/micro-sam/compare/dev...dev\">like this</a>.</p>\n\n<h3 id=\"optional-build-the-documentation\">Optional: Build the documentation</h3>\n\n<p>We use <a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> to build the documentation.</p>\n\n<p>To build the documentation locally, run this command:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>python<span class=\"w\"> </span>build_doc.py\n</code></pre>\n</div>\n\n<p>This will start a local server and display the HTML documentation. Any changes you make to the documentation will be updated in real time (you may need to refresh your browser to see the changes).</p>\n\n<p>If you want to save the HTML files, append <code>--out</code> to the command, like this:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>python<span class=\"w\"> </span>build_doc.py<span class=\"w\"> </span>--out\n</code></pre>\n</div>\n\n<p>This will save the HTML files into a new directory named <code>tmp</code>.</p>\n\n<p>You can add content to the documentation in two ways:</p>\n\n<ol>\n<li>By adding or updating <a href=\"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\">google style python docstrings</a> in the micro-sam code.\n<ul>\n<li><a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> will automatically find and include docstrings in the documentation.</li>\n</ul></li>\n<li>By adding or editing markdown files in the micro-sam <code>doc</code> directory.\n<ul>\n<li>If you add a new markdown file to the documentation, you must tell <a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> that it exists by adding a line to the <code>micro_sam/__init__.py</code> module docstring (eg: <code>.. include:: ../doc/my_amazing_new_docs_page.md</code>). Otherwise it will not be included in the final documentation build!</li>\n</ul></li>\n</ol>\n\n<h3 id=\"optional-benchmark-performance\">Optional: Benchmark performance</h3>\n\n<p>There are a number of options you can use to benchmark performance, and identify problems like slow run times or high memory use in micro-sam.</p>\n\n<ul>\n<li><a href=\"#run-the-benchmark-script\">Run the benchmark script</a></li>\n<li><a href=\"#line-profiling\">Line profiling</a></li>\n<li><a href=\"#snakeviz-visualization\">Snakeviz visualization</a></li>\n<li><a href=\"#memory-profiling-with-memray\">Memory profiling with memray</a></li>\n</ul>\n\n<h4 id=\"run-the-benchmark-script\">Run the benchmark script</h4>\n\n<p>There is a performance benchmark script available in the micro-sam repository at <code>development/benchmark.py</code>.</p>\n\n<p>To run the benchmark script:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>python<span class=\"w\"> </span>development/benchmark.py<span class=\"w\"> </span>--model_type<span class=\"w\"> </span>vit_t<span class=\"w\"> </span>--device<span class=\"w\"> </span>cpu<span class=\"sb\">`</span>\n</code></pre>\n</div>\n\n<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>python<span class=\"w\"> </span>development/benchmark.py<span class=\"w\"> </span>--help\n</code></pre>\n</div>\n\n<h4 id=\"line-profiling\">Line profiling</h4>\n\n<p>For more detailed line by line performance results, we can use <a href=\"https://github.com/pyutils/line_profiler\">line-profiler</a>.</p>\n\n<blockquote>\n  <p><a href=\"https://github.com/pyutils/line_profiler\">line_profiler</a> is a module for doing line-by-line profiling of functions. kernprof is a convenient script for running either <code>line_profiler</code> or the Python standard library's cProfile or profile modules, depending on what is available.</p>\n</blockquote>\n\n<p>To do line-by-line profiling:</p>\n\n<ol>\n<li>Ensure you have line profiler installed: <code>python -m pip install line_profiler</code></li>\n<li>Add <code>@profile</code> decorator to any function in the call stack</li>\n<li>Run <code>kernprof -lv benchmark.py --model_type vit_t --device cpu</code></li>\n</ol>\n\n<p>For more details about how to use line-profiler and kernprof, see <a href=\"https://kernprof.readthedocs.io/en/latest/\">the documentation</a>.</p>\n\n<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code>$<span class=\"w\"> </span>python<span class=\"w\"> </span>development/benchmark.py<span class=\"w\"> </span>--help\n</code></pre>\n</div>\n\n<h4 id=\"snakeviz-visualization\">Snakeviz visualization</h4>\n\n<p>For more detailed visualizations of profiling results, we use <a href=\"https://jiffyclub.github.io/snakeviz/\">snakeviz</a>.</p>\n\n<blockquote>\n  <p>SnakeViz is a browser based graphical viewer for the output of Python\u2019s cProfile module.</p>\n</blockquote>\n\n<ol>\n<li>Ensure you have snakeviz installed: <code>python -m pip install snakeviz</code></li>\n<li>Generate profile file: <code>python -m cProfile -o program.prof benchmark.py --model_type vit_h --device cpu</code></li>\n<li>Visualize profile file: <code>snakeviz program.prof</code></li>\n</ol>\n\n<p>For more details about how to use snakeviz, see <a href=\"https://jiffyclub.github.io/snakeviz/\">the documentation</a>.</p>\n\n<h4 id=\"memory-profiling-with-memray\">Memory profiling with memray</h4>\n\n<p>If you need to investigate memory use specifically, we use <a href=\"https://github.com/bloomberg/memray\">memray</a>.</p>\n\n<blockquote>\n  <p>Memray is a memory profiler for Python. It can track memory allocations in Python code, in native extension modules, and in the Python interpreter itself. It can generate several different types of reports to help you analyze the captured memory usage data. While commonly used as a CLI tool, it can also be used as a library to perform more fine-grained profiling tasks.</p>\n</blockquote>\n\n<p>For more details about how to use memray, see <a href=\"https://bloomberg.github.io/memray/getting_started.html\">the documentation</a>.</p>\n\n<h2 id=\"creating-a-new-release\">Creating a new release</h2>\n\n<p>To create a new release you have to edit the version number in <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/__version__.py\">micro_sam/__version__.py</a> in a PR. After merging this PR the release will automatically be done by the CI.</p>\n\n<h1 id=\"using-micro_sam-on-band\">Using <code>micro_sam</code> on BAND</h1>\n\n<p>BAND is a service offered by EMBL Heidelberg that gives access to a virtual desktop for image analysis tasks. It is free to use and <code>micro_sam</code> is installed there.\nIn order to use BAND and start <code>micro_sam</code> on it follow these steps:</p>\n\n<h2 id=\"start-band\">Start BAND</h2>\n\n<ul>\n<li>Go to <a href=\"https://band.embl.de/\">https://band.embl.de/</a> and click <strong>Login</strong>. If you have not used BAND before you will need to register for BAND. Currently you can only sign up via a Google account.</li>\n<li>Launch a BAND desktop with sufficient resources. It's particularly important to select a GPU. The settings from the image below are a good choice.</li>\n<li>Go to the desktop by clicking <strong>GO TO DESKTOP</strong> in the <strong>Running Desktops</strong> menu. See also the screenshot below.</li>\n</ul>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/f965fce2-b924-4fc8-871b-f3201e502138\" alt=\"image\" /></p>\n\n<h2 id=\"start-micro_sam-in-band\">Start <code>micro_sam</code> in BAND</h2>\n\n<ul>\n<li>Select <strong>Applications -> Image Analysis -> uSAM</strong> (see screenshot)\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5daeafb3-119b-4104-8708-aab2960cb21c\" alt=\"image\" /></li>\n<li>This will open the micro_sam menu, where you can select the tool you want to use (see screenshot). Note: this may take a few minutes.\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/900ce0b9-4cf8-418c-94f1-e99ac7bc0086\" alt=\"image\" /></li>\n<li>For testing if the tool works, it's best to use the <strong>2d annotator</strong> first.\n<ul>\n<li>You can find an example image to use here: <code>/scratch/cajal-connectomics/hela-2d-image.png</code>. Select it via <strong>Select image</strong>. (see screenshot)\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5fbd1c53-2ba1-47d4-ae50-dfab890ac9d3\" alt=\"image\" /></li>\n</ul></li>\n<li>Then press <strong>2d annotator</strong> and the tool will start.</li>\n</ul>\n\n<h2 id=\"transfering-data-to-band\">Transfering data to BAND</h2>\n\n<p>To copy data to and from BAND you can use any cloud storage, e.g. ownCloud, dropbox or google drive. For this, it's important to note that copy and paste, which you may need for accessing links on BAND, works a bit different in BAND:</p>\n\n<ul>\n<li>To copy text into BAND you first need to copy it on your computer (e.g. via selecting it + <code>Ctrl + C</code>).</li>\n<li>Then go to the browser window with BAND and press <code>Ctrl + Shift + Alt</code>. This will open a side window where you can paste your text via <code>Ctrl + V</code>.</li>\n<li>Then select the text in this window and copy it via <code>Ctrl + C</code>.</li>\n<li>Now you can close the side window via <code>Ctrl + Shift + Alt</code> and paste the text in band via <code>Ctrl + V</code></li>\n</ul>\n\n<p>The video below shows how to copy over a link from owncloud and then download the data on BAND using copy and paste:</p>\n\n<p><a href=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287\">https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287</a></p>\n"}, {"fullname": "micro_sam.automatic_segmentation", "modulename": "micro_sam.automatic_segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.automatic_segmentation.get_predictor_and_segmenter", "modulename": "micro_sam.automatic_segmentation", "qualname": "get_predictor_and_segmenter", "kind": "function", "doc": "<p>Get the Segment Anything model and class for automatic instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The Segment Anything model choice.</li>\n<li><strong>checkpoint:</strong>  The filepath to the stored model checkpoints.</li>\n<li><strong>device:</strong>  The torch device.</li>\n<li><strong>amg:</strong>  Whether to perform automatic segmentation in AMG mode.\nOtherwise AIS will be used, which requires a special segmentation decoder.\nIf not specified AIS will be used if it is available and otherwise AMG will be used.</li>\n<li><strong>is_tiled:</strong>  Whether to return segmenter for performing segmentation in tiling window style.</li>\n<li><strong>kwargs:</strong>  Keyword arguments for the automatic mask generation class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The Segment Anything model.\n  The automatic instance segmentation class.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">amg</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_tiled</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.automatic_segmentation.automatic_instance_segmentation", "modulename": "micro_sam.automatic_segmentation", "qualname": "automatic_instance_segmentation", "kind": "function", "doc": "<p>Run automatic segmentation for the input image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The Segment Anything model.</li>\n<li><strong>segmenter:</strong>  The automatic instance segmentation class.</li>\n<li><strong>input_path:</strong>  input_path: The input image file(s). Can either be a single image file (e.g. tif or png),\nor a container file (e.g. hdf5 or zarr).</li>\n<li><strong>output_path:</strong>  The output path where the instance segmentations will be saved.</li>\n<li><strong>embedding_path:</strong>  The path where the embeddings are cached already / will be saved.</li>\n<li><strong>key:</strong>  The key to the input file. This is needed for container files (eg. hdf5 or zarr)\nor to load several images as 3d volume. Provide a glob patterm, eg. \"*.tif\", for this case.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data. By default the dimensionality of the data will be used.\nIf you have RGB data you have to specify this explicitly, e.g. pass ndim=2 for 2d segmentation of RGB.</li>\n<li><strong>tile_shape:</strong>  Shape of the tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>verbose:</strong>  Verbosity flag.</li>\n<li><strong>generate_kwargs:</strong>  optional keyword arguments for the generate function of the AMG or AIS class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation result.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">generate_kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio", "modulename": "micro_sam.bioimageio", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.bioengine_export", "modulename": "micro_sam.bioimageio.bioengine_export", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.bioengine_export.ENCODER_CONFIG", "modulename": "micro_sam.bioimageio.bioengine_export", "qualname": "ENCODER_CONFIG", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;name: &quot;%s&quot;\\nbackend: &quot;pytorch&quot;\\nplatform: &quot;pytorch_libtorch&quot;\\n\\nmax_batch_size : 1\\ninput [\\n  {\\n    name: &quot;input0__0&quot;\\n    data_type: TYPE_FP32\\n    dims: [3, -1, -1]\\n  }\\n]\\noutput [\\n  {\\n    name: &quot;output0__0&quot;\\n    data_type: TYPE_FP32\\n    dims: [256, 64, 64]\\n  }\\n]\\n\\nparameters: {\\n  key: &quot;INFERENCE_MODE&quot;\\n  value: {\\n    string_value: &quot;true&quot;\\n  }\\n}&#x27;"}, {"fullname": "micro_sam.bioimageio.bioengine_export.DECODER_CONFIG", "modulename": "micro_sam.bioimageio.bioengine_export", "qualname": "DECODER_CONFIG", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;name: &quot;%s&quot;\\nbackend: &quot;onnxruntime&quot;\\nplatform: &quot;onnxruntime_onnx&quot;\\n\\nparameters: {\\n  key: &quot;INFERENCE_MODE&quot;\\n  value: {\\n    string_value: &quot;true&quot;\\n  }\\n}\\n\\ninstance_group {\\n  count: 1\\n  kind: KIND_CPU\\n}&#x27;"}, {"fullname": "micro_sam.bioimageio.bioengine_export.export_image_encoder", "modulename": "micro_sam.bioimageio.bioengine_export", "qualname": "export_image_encoder", "kind": "function", "doc": "<p>Export SAM image encoder to torchscript.</p>\n\n<p>The torchscript image encoder can be used for predicting image embeddings\nwith a backed, e.g. with <a href=\"https://github.com/bioimage-io/bioengine-model-runner\">the bioengine</a>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The SAM model type.</li>\n<li><strong>output_root:</strong>  The output root directory where the exported model is saved.</li>\n<li><strong>export_name:</strong>  The name of the exported model.</li>\n<li><strong>checkpoint_path:</strong>  Optional checkpoint for loading the exported model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">export_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.bioengine_export.export_onnx_model", "modulename": "micro_sam.bioimageio.bioengine_export", "qualname": "export_onnx_model", "kind": "function", "doc": "<p>Export SAM prompt encoder and mask decoder to onnx.</p>\n\n<p>The onnx encoder and decoder can be used for interactive segmentation in the browser.\nThis code is adapted from\n<a href=\"https://github.com/facebookresearch/segment-anything/blob/main/scripts/export_onnx_model.py\">https://github.com/facebookresearch/segment-anything/blob/main/scripts/export_onnx_model.py</a></p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The SAM model type.</li>\n<li><strong>output_root:</strong>  The output root directory where the exported model is saved.</li>\n<li><strong>opset:</strong>  The ONNX opset version.</li>\n<li><strong>export_name:</strong>  The name of the exported model.</li>\n<li><strong>checkpoint_path:</strong>  Optional checkpoint for loading the SAM model.</li>\n<li><strong>return_single_mask:</strong>  Whether the mask decoder returns a single or multiple masks.</li>\n<li><strong>gelu_approximate:</strong>  Whether to use a GeLU approximation, in case the ONNX backend\ndoes not have an efficient GeLU implementation.</li>\n<li><strong>use_stability_score:</strong>  Whether to use the stability score instead of the predicted score.</li>\n<li><strong>return_extra_metrics:</strong>  Whether to return a larger set of metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span>,</span><span class=\"param\">\t<span class=\"n\">output_root</span>,</span><span class=\"param\">\t<span class=\"n\">opset</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">export_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_single_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gelu_approximate</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_stability_score</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_extra_metrics</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.bioengine_export.export_bioengine_model", "modulename": "micro_sam.bioimageio.bioengine_export", "qualname": "export_bioengine_model", "kind": "function", "doc": "<p>Export SAM model to a format compatible with the BioEngine.</p>\n\n<p><a href=\"https://github.com/bioimage-io/bioengine-model-runner\">The bioengine</a> enables running the\nimage encoder on an online backend, so that SAM can be used in an online tool, or to predict\nthe image embeddings via the online backend rather than on CPU.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The SAM model type.</li>\n<li><strong>output_root:</strong>  The output root directory where the exported model is saved.</li>\n<li><strong>opset:</strong>  The ONNX opset version.</li>\n<li><strong>export_name:</strong>  The name of the exported model.</li>\n<li><strong>checkpoint_path:</strong>  Optional checkpoint for loading the SAM model.</li>\n<li><strong>return_single_mask:</strong>  Whether the mask decoder returns a single or multiple masks.</li>\n<li><strong>gelu_approximate:</strong>  Whether to use a GeLU approximation, in case the ONNX backend\ndoes not have an efficient GeLU implementation.</li>\n<li><strong>use_stability_score:</strong>  Whether to use the stability score instead of the predicted score.</li>\n<li><strong>return_extra_metrics:</strong>  Whether to return a larger set of metrics.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span>,</span><span class=\"param\">\t<span class=\"n\">output_root</span>,</span><span class=\"param\">\t<span class=\"n\">opset</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">export_name</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_single_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gelu_approximate</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_stability_score</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_extra_metrics</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.model_export", "modulename": "micro_sam.bioimageio.model_export", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.model_export.DEFAULTS", "modulename": "micro_sam.bioimageio.model_export", "qualname": "DEFAULTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;authors&#x27;: [Author(affiliation=&#x27;University Goettingen&#x27;, email=None, orcid=None, name=&#x27;Anwai Archit&#x27;, github_user=&#x27;anwai98&#x27;), Author(affiliation=&#x27;University Goettingen&#x27;, email=None, orcid=None, name=&#x27;Constantin Pape&#x27;, github_user=&#x27;constantinpape&#x27;)], &#x27;description&#x27;: &#x27;Finetuned Segment Anything Model for Microscopy&#x27;, &#x27;cite&#x27;: [CiteEntry(text=&#x27;Archit et al. Segment Anything for Microscopy&#x27;, doi=&#x27;10.1101/2023.08.21.554208&#x27;, url=None)], &#x27;tags&#x27;: [&#x27;segment-anything&#x27;, &#x27;instance-segmentation&#x27;]}"}, {"fullname": "micro_sam.bioimageio.model_export.export_sam_model", "modulename": "micro_sam.bioimageio.model_export", "qualname": "export_sam_model", "kind": "function", "doc": "<p>Export SAM model to BioImage.IO model format.</p>\n\n<p>The exported model can be uploaded to <a href=\"https://bioimage.io/#/\">bioimage.io</a> and\nbe used in tools that support the BioImage.IO model format.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The image for generating test data.</li>\n<li><strong>label_image:</strong>  The segmentation correspoding to <code>image</code>.\nIt is used to derive prompt inputs for the model.</li>\n<li><strong>model_type:</strong>  The type of the SAM model.</li>\n<li><strong>name:</strong>  The name of the exported model.</li>\n<li><strong>output_path:</strong>  Where the exported model is saved.</li>\n<li><strong>checkpoint_path:</strong>  Optional checkpoint for loading the SAM model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">label_image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor", "modulename": "micro_sam.bioimageio.predictor_adaptor", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor", "kind": "class", "doc": "<p>Wrapper around the SamPredictor.</p>\n\n<p>This model supports the same functionality as SamPredictor and can provide mask segmentations\nfrom box, point or mask input prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The type of the model for the image encoder.\nCan be one of 'vit_b', 'vit_l', 'vit_h' or 'vit_t'.\nFor 'vit_t' support the 'mobile_sam' package has to be installed.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.__init__", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.sam", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.load_state_dict", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.load_state_dict", "kind": "function", "doc": "<p>Copy parameters and buffers from <code>state_dict</code> into this module and its descendants.</p>\n\n<p>If <code>strict</code> is <code>True</code>, then\nthe keys of <code>state_dict</code> must exactly match the keys returned\nby this module's <code>~torch.nn.Module.state_dict()</code> function.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>If <code>assign</code> is <code>True</code> the optimizer must be created after\nthe call to <code>load_state_dict</code> unless\n<code>~torch.__future__.get_swap_module_params_on_conversion()</code> is <code>True</code>.</p>\n\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state_dict (dict):</strong>  a dict containing parameters and\npersistent buffers.</li>\n<li><strong>strict (bool, optional):</strong>  whether to strictly enforce that the keys\nin <code>state_dict</code> match the keys returned by this module's\n<code>~torch.nn.Module.state_dict()</code> function. Default: <code>True</code></li>\n<li><strong>assign (bool, optional):</strong>  When <code>False</code>, the properties of the tensors\nin the current module are preserved while when <code>True</code>, the\nproperties of the Tensors in the state dict are preserved. The only\nexception is the <code>requires_grad</code> field of <code>~torch.nn.Parameter</code>s\nfor which the value from the module is preserved.\nDefault: <code>False</code></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p><code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:\n      * <strong>missing_keys</strong> is a list of str containing any keys that are expected\n          by this module but missing from the provided <code>state_dict</code>.\n      * <strong>unexpected_keys</strong> is a list of str containing the keys that are not\n          expected by this module but present in the provided <code>state_dict</code>.</p>\n</blockquote>\n\n<h6 id=\"note\">Note:</h6>\n\n<blockquote>\n  <p>If a parameter or buffer is registered as <code>None</code> and its corresponding key\n  exists in <code>state_dict</code>, <code>load_state_dict()</code> will raise a\n  <code>RuntimeError</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.forward", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  torch inputs of dimensions B x C x H x W</li>\n<li><strong>box_prompts:</strong>  box coordinates of dimensions B x OBJECTS x 4</li>\n<li><strong>point_prompts:</strong>  point coordinates of dimension B x OBJECTS x POINTS x 2</li>\n<li><strong>point_labels:</strong>  point labels of dimension B x OBJECTS x POINTS</li>\n<li><strong>mask_prompts:</strong>  mask prompts of dimension B x OBJECTS x 256 x 256</li>\n<li><strong>embeddings:</strong>  precomputed image embeddings B x 256 x 64 x 64</li>\n</ul>\n\n<p>Returns:</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">box_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation", "modulename": "micro_sam.evaluation", "kind": "module", "doc": "<p>Functionality for evaluating Segment Anything models on microscopy data.</p>\n"}, {"fullname": "micro_sam.evaluation.benchmark_datasets", "modulename": "micro_sam.evaluation.benchmark_datasets", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.evaluation.benchmark_datasets.LM_2D_DATASETS", "modulename": "micro_sam.evaluation.benchmark_datasets", "qualname": "LM_2D_DATASETS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;livecell&#x27;, &#x27;deepbacs&#x27;, &#x27;tissuenet&#x27;, &#x27;neurips_cellseg&#x27;, &#x27;dynamicnuclearnet&#x27;, &#x27;hpa&#x27;, &#x27;covid_if&#x27;, &#x27;pannuke&#x27;, &#x27;lizard&#x27;, &#x27;orgasegment&#x27;, &#x27;omnipose&#x27;, &#x27;dic_hepg2&#x27;]"}, {"fullname": "micro_sam.evaluation.benchmark_datasets.LM_3D_DATASETS", "modulename": "micro_sam.evaluation.benchmark_datasets", "qualname": "LM_3D_DATASETS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;plantseg_root&#x27;, &#x27;plantseg_ovules&#x27;, &#x27;gonuclear&#x27;, &#x27;mouse_embryo&#x27;, &#x27;embegseg&#x27;, &#x27;cellseg3d&#x27;]"}, {"fullname": "micro_sam.evaluation.benchmark_datasets.EM_2D_DATASETS", "modulename": "micro_sam.evaluation.benchmark_datasets", "qualname": "EM_2D_DATASETS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;mitolab_tem&#x27;]"}, {"fullname": "micro_sam.evaluation.benchmark_datasets.EM_3D_DATASETS", "modulename": "micro_sam.evaluation.benchmark_datasets", "qualname": "EM_3D_DATASETS", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;mitoem_rat&#x27;, &#x27;mitoem_human&#x27;, &#x27;platynereis_nuclei&#x27;, &#x27;lucchi&#x27;, &#x27;mitolab&#x27;, &#x27;nuc_mm_mouse&#x27;, &#x27;num_mm_zebrafish&#x27;, &#x27;uro_cell&#x27;, &#x27;sponge_em&#x27;, &#x27;platynereis_cilia&#x27;, &#x27;vnc&#x27;, &#x27;asem_mito&#x27;]"}, {"fullname": "micro_sam.evaluation.benchmark_datasets.DATASET_RETURNS_FOLDER", "modulename": "micro_sam.evaluation.benchmark_datasets", "qualname": "DATASET_RETURNS_FOLDER", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;deepbacs&#x27;: &#x27;*.tif&#x27;}"}, {"fullname": "micro_sam.evaluation.benchmark_datasets.DATASET_CONTAINER_KEYS", "modulename": "micro_sam.evaluation.benchmark_datasets", "qualname": "DATASET_CONTAINER_KEYS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;lucchi&#x27;: [&#x27;raw&#x27;, &#x27;labels&#x27;]}"}, {"fullname": "micro_sam.evaluation.benchmark_datasets.run_benchmark_evaluations", "modulename": "micro_sam.evaluation.benchmark_datasets", "qualname": "run_benchmark_evaluations", "kind": "function", "doc": "<p>Run evaluation for benchmarking Segment Anything models on microscopy datasets.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_folder:</strong>  The path to directory where all inputs will be stored and preprocessed.</li>\n<li><strong>dataset_choice:</strong>  The dataset choice.</li>\n<li><strong>model_type:</strong>  The model choice for SAM.</li>\n<li><strong>output_folder:</strong>  The path to directory where all outputs will be stored.</li>\n<li><strong>checkpoint_path:</strong>  The checkpoint path</li>\n<li><strong>run_amg:</strong>  Whether to run automatic segmentation in AMG mode.</li>\n<li><strong>retain:</strong>  Whether to retain certain parts of the benchmark runs.\nBy default, removes everything besides quantitative results.\nThere is the choice to retain 'data', 'crops', 'auto', or 'int'.</li>\n<li><strong>ignore_warnings:</strong>  Whether to ignore warnings.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dataset_choice</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">run_amg</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">retain</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_warnings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.evaluation", "modulename": "micro_sam.evaluation.evaluation", "kind": "module", "doc": "<p>Evaluation functionality for segmentation predictions from <code>micro_sam.evaluation.automatic_mask_generation</code>\nand <code>micro_sam.evaluation.inference</code>.</p>\n"}, {"fullname": "micro_sam.evaluation.evaluation.run_evaluation", "modulename": "micro_sam.evaluation.evaluation", "qualname": "run_evaluation", "kind": "function", "doc": "<p>Run evaluation for instance segmentation predictions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The list of paths to ground-truth images.</li>\n<li><strong>prediction_paths:</strong>  The list of paths with the instance segmentations to evaluate.</li>\n<li><strong>save_path:</strong>  Optional path for saving the results.</li>\n<li><strong>verbose:</strong>  Whether to print the progress.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A DataFrame that contains the evaluation results.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.evaluation.run_evaluation_for_iterative_prompting", "modulename": "micro_sam.evaluation.evaluation", "qualname": "run_evaluation_for_iterative_prompting", "kind": "function", "doc": "<p>Run evaluation for iterative prompt-based segmentation predictions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The list of paths to ground-truth images.</li>\n<li><strong>prediction_root:</strong>  The folder with the iterative prompt-based instance segmentations to evaluate.</li>\n<li><strong>experiment_folder:</strong>  The folder where all the experiment results are stored.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to evaluate on experiments with iterative prompting starting with box.</li>\n<li><strong>overwrite_results:</strong>  Whether to overwrite the results to update them with the new evaluation run.</li>\n<li><strong>use_masks:</strong>  Whether to use masks for iterative prompting.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A DataFrame that contains the evaluation results.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box_prompt</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">overwrite_results</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments", "modulename": "micro_sam.evaluation.experiments", "kind": "module", "doc": "<p>Predefined experiment settings for experiments with different prompt strategies.</p>\n"}, {"fullname": "micro_sam.evaluation.experiments.ExperimentSetting", "modulename": "micro_sam.evaluation.experiments", "qualname": "ExperimentSetting", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Dict"}, {"fullname": "micro_sam.evaluation.experiments.full_experiment_settings", "modulename": "micro_sam.evaluation.experiments", "qualname": "full_experiment_settings", "kind": "function", "doc": "<p>The full experiment settings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>use_boxes:</strong>  Whether to run the experiments with or without boxes.</li>\n<li><strong>positive_range:</strong>  The different number of positive points that will be used.\nBy defaul the values are set to [1, 2, 4, 8, 16].</li>\n<li><strong>negative_range:</strong>  The different number of negative points that will be used.\nBy defaul the values are set to [0, 1, 2, 4, 8, 16].</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of experiment settings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">positive_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">negative_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments.default_experiment_settings", "modulename": "micro_sam.evaluation.experiments", "qualname": "default_experiment_settings", "kind": "function", "doc": "<p>The three default experiment settings.</p>\n\n<p>For the default experiments we use a single positive prompt,\ntwo positive and four negative prompts and box prompts.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of experiment settings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments.get_experiment_setting_name", "modulename": "micro_sam.evaluation.experiments", "qualname": "get_experiment_setting_name", "kind": "function", "doc": "<p>Get the name for the given experiment setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>setting:</strong>  The experiment setting.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The name for this experiment setting.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">setting</span><span class=\"p\">:</span> <span class=\"n\">Dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference", "modulename": "micro_sam.evaluation.inference", "kind": "module", "doc": "<p>Inference with Segment Anything models and different prompt strategies.</p>\n"}, {"fullname": "micro_sam.evaluation.inference.precompute_all_embeddings", "modulename": "micro_sam.evaluation.inference", "qualname": "precompute_all_embeddings", "kind": "function", "doc": "<p>Precompute all image embeddings.</p>\n\n<p>To enable running different inference tasks in parallel afterwards.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the embeddings will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.precompute_all_prompts", "modulename": "micro_sam.evaluation.inference", "qualname": "precompute_all_prompts", "kind": "function", "doc": "<p>Precompute all point prompts.</p>\n\n<p>To enable running different inference tasks in parallel afterwards.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The file paths to the ground-truth segmentations.</li>\n<li><strong>prompt_save_dir:</strong>  The directory where the prompt files will be saved.</li>\n<li><strong>prompt_settings:</strong>  The settings for which the prompts will be computed.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_save_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_settings</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_inference_with_prompts", "modulename": "micro_sam.evaluation.inference", "qualname": "run_inference_with_prompts", "kind": "function", "doc": "<p>Run segment anything inference for multiple images using prompts derived from groundtruth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the image embddings will be saved or are already saved.</li>\n<li><strong>use_points:</strong>  Whether to use point prompts.</li>\n<li><strong>use_boxes:</strong>  Whether to use box prompts</li>\n<li><strong>n_positives:</strong>  The number of positive point prompts that will be sampled.</li>\n<li><strong>n_negativess:</strong>  The number of negative point prompts that will be sampled.</li>\n<li><strong>dilation:</strong>  The dilation factor for the radius around the ground-truth object\naround which points will not be sampled.</li>\n<li><strong>prompt_save_dir:</strong>  The directory where point prompts will be saved or are already saved.\nThis enables running multiple experiments in a reproducible manner.</li>\n<li><strong>batch_size:</strong>  The batch size used for batched prediction.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">n_positives</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_negatives</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_save_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">512</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_inference_with_iterative_prompting", "modulename": "micro_sam.evaluation.inference", "qualname": "run_inference_with_iterative_prompting", "kind": "function", "doc": "<p>Run Segment Anything inference for multiple images using prompts iteratively\nderived from model outputs and ground-truth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The Segment Anything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the image embeddings will be saved or are already saved.</li>\n<li><strong>prediction_dir:</strong>  The directory where the predictions from Segment Anything will be saved per iteration.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to use the first prompt as bounding box or a single point</li>\n<li><strong>dilation:</strong>  The dilation factor for the radius around the ground-truth object\naround which points will not be sampled.</li>\n<li><strong>batch_size:</strong>  The batch size used for batched predictions.</li>\n<li><strong>n_iterations:</strong>  The number of iterations for iterative prompting.</li>\n<li><strong>use_masks:</strong>  Whether to make use of logits from previous prompt-based segmentation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box_prompt</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">n_iterations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">use_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_amg", "modulename": "micro_sam.evaluation.inference", "qualname": "run_amg", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">peft_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cache_embeddings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.inference", "qualname": "run_instance_segmentation_with_decoder", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">peft_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cache_embeddings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation", "modulename": "micro_sam.evaluation.instance_segmentation", "kind": "module", "doc": "<p>Inference and evaluation for the automatic instance segmentation functionality.</p>\n"}, {"fullname": "micro_sam.evaluation.instance_segmentation.default_grid_search_values_amg", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "default_grid_search_values_amg", "kind": "function", "doc": "<p>Default grid-search parameter for AMG-based instance segmentation.</p>\n\n<p>Return grid search values for the two most important parameters:</p>\n\n<ul>\n<li><code>pred_iou_thresh</code>, the threshold for keeping objects according to the IoU predicted by the model.</li>\n<li><code>stability_score_thresh</code>, the theshold for keepong objects according to their stability.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>iou_thresh_values:</strong>  The values for <code>pred_iou_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>stability_score_values:</strong>  The values for <code>stability_score_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.default_grid_search_values_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "default_grid_search_values_instance_segmentation_with_decoder", "kind": "function", "doc": "<p>Default grid-search parameter for decoder-based instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>center_distance_threshold_values:</strong>  The values for <code>center_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>boundary_distance_threshold_values:</strong>  The values for <code>boundary_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>distance_smoothing_values:</strong>  The values for <code>distance_smoothing</code> used in the gridsearch.\nBy default values in the range from 1.0 to 2.0 with a stepsize of 0.1 will be used.</li>\n<li><strong>min_size_values:</strong>  The values for <code>min_size</code> used in the gridsearch.\nBy default the values 50, 100 and 200  are used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">center_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_grid_search", "kind": "function", "doc": "<p>Run grid search for automatic mask generation.</p>\n\n<p>The parameters and their respective value ranges for the grid search are specified via the\n'grid_search_values' argument. For example, to run a grid search over the parameters 'pred_iou_thresh'\nand 'stability_score_thresh', you can pass the following:</p>\n\n<pre><code>grid_search_values = {\n    \"pred_iou_thresh\": [0.6, 0.7, 0.8, 0.9],\n    \"stability_score_thresh\": [0.6, 0.7, 0.8, 0.9],\n}\n</code></pre>\n\n<p>All combinations of the parameters will be checked.</p>\n\n<p>You can use the functions <code>default_grid_search_values_instance_segmentation_with_decoder</code>\nor <code>default_grid_search_values_amg</code> to get the default grid search parameters for the two\nrespective instance segmentation methods.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>generate</code> function.</li>\n<li><strong>image_paths:</strong>  The input images for the grid search.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation for the grid search.</li>\n<li><strong>result_dir:</strong>  Folder to cache the evaluation results per image.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>fixed_generate_kwargs:</strong>  Fixed keyword arguments for the <code>generate</code> method of the segmenter.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the grid-search for individual images in a verbose mode.</li>\n<li><strong>image_key:</strong>  Key for loading the image data from a more complex file format like HDF5.\nIf not given a simple image format like tif is assumed.</li>\n<li><strong>gt_key:</strong>  Key for loading the ground-truth data from a more complex file format like HDF5.\nIf not given a simple image format like tif is assumed.</li>\n<li><strong>rois:</strong>  Region of interests to resetrict the evaluation to.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">image_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gt_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">slice</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_inference", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_inference", "kind": "function", "doc": "<p>Run inference for automatic mask generation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>image_paths:</strong>  The input images.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>prediction_dir:</strong>  Folder to save the predictions.</li>\n<li><strong>generate_kwargs:</strong>  The keyword arguments for the <code>generate</code> method of the segmenter.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.evaluate_instance_segmentation_grid_search", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "evaluate_instance_segmentation_grid_search", "kind": "function", "doc": "<p>Evaluate gridsearch results.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>result_dir:</strong>  The folder with the gridsearch results.</li>\n<li><strong>grid_search_parameters:</strong>  The names for the gridsearch parameters.</li>\n<li><strong>criterion:</strong>  The metric to use for determining the best parameters.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The best parameter setting.\n  The evaluation score for the best setting.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_parameters</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">criterion</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mSA&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.save_grid_search_best_params", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "save_grid_search_best_params", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">best_kwargs</span>, </span><span class=\"param\"><span class=\"n\">best_msa</span>, </span><span class=\"param\"><span class=\"n\">grid_search_result_dir</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search_and_inference", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_grid_search_and_inference", "kind": "function", "doc": "<p>Run grid search and inference for automatic mask generation.</p>\n\n<p>Please refer to the documentation of <code>run_instance_segmentation_grid_search</code>\nfor details on how to specify the grid search parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>generate</code> function.</li>\n<li><strong>val_image_paths:</strong>  The input images for the grid search.</li>\n<li><strong>val_gt_paths:</strong>  The ground-truth segmentation for the grid search.</li>\n<li><strong>test_image_paths:</strong>  The input images for inference.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>prediction_dir:</strong>  Folder to save the predictions.</li>\n<li><strong>experiment_folder:</strong>  Folder for caching best grid search parameters in 'results'.</li>\n<li><strong>result_dir:</strong>  Folder to cache the evaluation results per image.</li>\n<li><strong>fixed_generate_kwargs:</strong>  Fixed keyword arguments for the <code>generate</code> method of the segmenter.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell", "modulename": "micro_sam.evaluation.livecell", "kind": "module", "doc": "<p>Inference and evaluation for the <a href=\"https://www.nature.com/articles/s41592-021-01249-6\">LIVECell dataset</a> and\nthe different cell lines contained in it.</p>\n"}, {"fullname": "micro_sam.evaluation.livecell.CELL_TYPES", "modulename": "micro_sam.evaluation.livecell", "qualname": "CELL_TYPES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;A172&#x27;, &#x27;BT474&#x27;, &#x27;BV2&#x27;, &#x27;Huh7&#x27;, &#x27;MCF7&#x27;, &#x27;SHSY5Y&#x27;, &#x27;SkBr3&#x27;, &#x27;SKOV3&#x27;]"}, {"fullname": "micro_sam.evaluation.livecell.livecell_inference", "modulename": "micro_sam.evaluation.livecell", "qualname": "livecell_inference", "kind": "function", "doc": "<p>Run inference for livecell with a fixed prompt setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segment anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>use_points:</strong>  Whether to use point prompts.</li>\n<li><strong>use_boxes:</strong>  Whether to use box prompts.</li>\n<li><strong>n_positives:</strong>  The number of positive point prompts.</li>\n<li><strong>n_negatives:</strong>  The number of negative point prompts.</li>\n<li><strong>prompt_folder:</strong>  The folder where the prompts should be saved.</li>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">n_positives</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_negatives</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_precompute_embeddings", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_precompute_embeddings", "kind": "function", "doc": "<p>Run precomputation of val and test image embeddings for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_iterative_prompting", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_iterative_prompting", "kind": "function", "doc": "<p>Run inference on livecell with iterative prompting setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segment anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to use the first prompt as bounding box or a single point.</li>\n<li><strong>use_masks:</strong>  Whether to make use of logits from previous prompt-based segmentation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_amg", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_amg", "kind": "function", "doc": "<p>Run automatic mask generation grid-search and inference for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>iou_thresh_values:</strong>  The values for <code>pred_iou_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>stability_score_values:</strong>  The values for <code>stability_score_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path where the predicted images are stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_instance_segmentation_with_decoder", "kind": "function", "doc": "<p>Run automatic mask generation grid-search and inference for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>center_distance_threshold_values:</strong>  The values for <code>center_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>boundary_distance_threshold_values:</strong>  The values for <code>boundary_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>distance_smoothing_values:</strong>  The values for <code>distance_smoothing</code> used in the gridsearch.\nBy default values in the range from 1.0 to 2.0 with a stepsize of 0.1 will be used.</li>\n<li><strong>min_size_values:</strong>  The values for <code>min_size</code> used in the gridsearch.\nBy default the values 50, 100 and 200  are used.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path where the predicted images are stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">center_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_inference", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_inference", "kind": "function", "doc": "<p>Run LIVECell inference with command line tool.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_evaluation", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_evaluation", "kind": "function", "doc": "<p>Run LIVECell evaluation with command line tool.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "kind": "module", "doc": "<p>Functionality for qualitative comparison of Segment Anything models on microscopy data.</p>\n"}, {"fullname": "micro_sam.evaluation.model_comparison.generate_data_for_model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "generate_data_for_model_comparison", "kind": "function", "doc": "<p>Generate samples for qualitative model comparison.</p>\n\n<p>This precomputes the input for <code>model_comparison</code> and <code>model_comparison_with_napari</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loader:</strong>  The torch dataloader from which samples are drawn.</li>\n<li><strong>output_folder:</strong>  The folder where the samples will be saved.</li>\n<li><strong>model_type1:</strong>  The first model to use for comparison.\nThe value needs to be a valid model_type for <code>micro_sam.util.get_sam_model</code>.</li>\n<li><strong>model_type2:</strong>  The second model to use for comparison.\nThe value needs to be a valid model_type for <code>micro_sam.util.get_sam_model</code>.</li>\n<li><strong>n_samples:</strong>  The number of samples to draw from the dataloader.</li>\n<li><strong>checkpoint1:</strong>  Optional checkpoint for the first model.</li>\n<li><strong>checkpoint2:</strong>  Optional checkpoint for the second model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type1</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type2</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">model_type3</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint1</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint2</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint3</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison.model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "model_comparison", "kind": "function", "doc": "<p>Create images for a qualitative model comparision.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_folder:</strong>  The folder with the data precomputed by <code>generate_data_for_model_comparison</code>.</li>\n<li><strong>n_images_per_sample:</strong>  The number of images to generate per precomputed sample.</li>\n<li><strong>min_size:</strong>  The min size of ground-truth objects to take into account.</li>\n<li><strong>plot_folder:</strong>  The folder where to save the plots. If not given the plots will be displayed.</li>\n<li><strong>point_radius:</strong>  The radius of the point overlay.</li>\n<li><strong>outline_dilation:</strong>  The dilation factor of the outline overlay.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">n_images_per_sample</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">plot_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_radius</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">outline_dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">have_model3</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison.model_comparison_with_napari", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "model_comparison_with_napari", "kind": "function", "doc": "<p>Use napari to display the qualtiative comparison results for two models.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_folder:</strong>  The folder with the data precomputed by <code>generate_data_for_model_comparison</code>.</li>\n<li><strong>show_points:</strong>  Whether to show the results for point or for box prompts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">show_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.default_grid_search_values_multi_dimensional_segmentation", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "default_grid_search_values_multi_dimensional_segmentation", "kind": "function", "doc": "<p>Default grid-search parameters for multi-dimensional prompt-based instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>iou_threshold_values:</strong>  The values for <code>iou_threshold</code> used in the grid-search.\nBy default values in the range from 0.5 to 0.9 with a stepsize of 0.1 will be used.</li>\n<li><strong>projection_method_values:</strong>  The values for <code>projection</code> method used in the grid-search.\nBy default the values <code>mask</code>, <code>points</code>, <code>box</code>, <code>points_and_mask</code> and <code>single_point</code> are used.</li>\n<li><strong>box_extension_values:</strong>  The values for <code>box_extension</code> used in the grid-search.\nBy default values in the range from 0 to 0.25 with a stepsize of 0.025 will be used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">iou_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">projection_method_values</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension_values</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.segment_slices_from_ground_truth", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "segment_slices_from_ground_truth", "kind": "function", "doc": "<p>Segment all objects in a volume by prompt-based segmentation in one slice per object.</p>\n\n<p>This function first segments each object in the respective specified slice using interactive\n(prompt-based) segmentation functionality. Then it segments the particular object in the\nremaining slices in the volume.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>ground_truth:</strong>  The label volume with instance segmentations.</li>\n<li><strong>model_type:</strong>  Choice of segment anything model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the model checkpoint.</li>\n<li><strong>embedding_path:</strong>  Path to cache the computed embeddings.</li>\n<li><strong>save_path:</strong>  Path to store the segmentations.</li>\n<li><strong>iou_threshold:</strong>  The criterion to decide whether to link the objects in the consecutive slice's segmentation.</li>\n<li><strong>projection:</strong>  The projection (prompting) method to generate prompts for consecutive slices.</li>\n<li><strong>box_extension:</strong>  Extension factor for increasing the box size after projection.</li>\n<li><strong>device:</strong>  The selected device for computation.</li>\n<li><strong>interactive_seg_mode:</strong>  Method for guiding prompt-based instance segmentation.</li>\n<li><strong>verbose:</strong>  Whether to get the trace for projected segmentations.</li>\n<li><strong>return_segmentation:</strong>  Whether to return the segmented volume.</li>\n<li><strong>min_size:</strong>  The minimal size for evaluating an object in the ground-truth.\nThe size is measured within the central slice.</li>\n<li><strong>evaluation_metric:</strong>  The choice of supported metric to evaluate predictions.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">ground_truth</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">iou_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.8</span>,</span><span class=\"param\">\t<span class=\"n\">projection</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mask&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mf\">0.025</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">interactive_seg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;box&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_segmentation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">evaluation_metric</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;sa&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dice&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sa&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.run_multi_dimensional_segmentation_grid_search", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "run_multi_dimensional_segmentation_grid_search", "kind": "function", "doc": "<p>Run grid search for prompt-based multi-dimensional instance segmentation.</p>\n\n<p>The parameters and their respective value ranges for the grid search are specified via the\n<code>grid_search_values</code> argument. For example, to run a grid search over the parameters <code>iou_threshold</code>,\n<code>projection</code> and <code>box_extension</code>, you can pass the following:</p>\n\n<pre><code>grid_search_values = {\n    \"iou_threshold\": [0.5, 0.6, 0.7, 0.8, 0.9],\n    \"projection\": [\"mask\", \"box\", \"points\"],\n    \"box_extension\": [0, 0.1, 0.2, 0.3, 0.4, 0,5],\n}\n</code></pre>\n\n<p>All combinations of the parameters will be checked.\nIf passed None, the function <code>default_grid_search_values_multi_dimensional_segmentation</code> is used\nto get the default grid search parameters for the instance segmentation method.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>ground_truth:</strong>  The label volume with instance segmentations.</li>\n<li><strong>model_type:</strong>  Choice of segment anything model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the model checkpoint.</li>\n<li><strong>embedding_path:</strong>  Path to cache the computed embeddings.</li>\n<li><strong>result_dir:</strong>  Path to save the grid search results.</li>\n<li><strong>interactive_seg_mode:</strong>  Method for guiding prompt-based instance segmentation.</li>\n<li><strong>verbose:</strong>  Whether to get the trace for projected segmentations.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>segment_slices_from_ground_truth</code> function.</li>\n<li><strong>min_size:</strong>  The minimal size for evaluating an object in the ground-truth.\nThe size is measured within the central slice.</li>\n<li><strong>evaluation_metric:</strong>  The choice of metric for evaluating predictions.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">ground_truth</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">interactive_seg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;box&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">evaluation_metric</span><span class=\"p\">:</span> <span class=\"n\">Literal</span><span class=\"p\">[</span><span class=\"s1\">&#39;sa&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dice&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;sa&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.inference", "modulename": "micro_sam.inference", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.inference.batched_inference", "modulename": "micro_sam.inference", "qualname": "batched_inference", "kind": "function", "doc": "<p>Run batched inference for input prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>image:</strong>  The input image.</li>\n<li><strong>batch_size:</strong>  The batch size to use for inference.</li>\n<li><strong>boxes:</strong>  The box prompts. Array of shape N_PROMPTS x 4.\nThe bounding boxes are represented by [MIN_X, MIN_Y, MAX_X, MAX_Y].</li>\n<li><strong>points:</strong>  The point prompt coordinates. Array of shape N_PROMPTS x 1 x 2.\nThe points are represented by their coordinates [X, Y], which are given\nin the last dimension.</li>\n<li><strong>point_labels:</strong>  The point prompt labels. Array of shape N_PROMPTS x 1.\nThe labels are either 0 (negative prompt) or 1 (positive prompt).</li>\n<li><strong>multimasking:</strong>  Whether to predict with 3 or 1 mask.</li>\n<li><strong>embedding_path:</strong>  Cache path for the image embeddings.</li>\n<li><strong>return_instance_segmentation:</strong>  Whether to return a instance segmentation\nor the individual mask data.</li>\n<li><strong>segmentation_ids:</strong>  Fixed segmentation ids to assign to the masks\nderived from the prompts.</li>\n<li><strong>reduce_multimasking:</strong>  Whether to choose the most likely masks with\nhighest ious from multimasking</li>\n<li><strong>logits_masks:</strong>  The logits masks. Array of shape N_PROMPTS x 1 x 256 x 256.\nWhether to use the logits masks from previous segmentation.</li>\n<li><strong>verbose_embeddings:</strong>  Whether to show progress outputs of computing image embeddings.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predicted segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">boxes</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimasking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_instance_segmentation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reduce_multimasking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">logits_masks</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_embeddings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation", "modulename": "micro_sam.instance_segmentation", "kind": "module", "doc": "<p>Automated instance segmentation functionality.\nThe classes implemented here extend the automatic instance segmentation from Segment Anything:\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html</a></p>\n"}, {"fullname": "micro_sam.instance_segmentation.mask_data_to_segmentation", "modulename": "micro_sam.instance_segmentation", "qualname": "mask_data_to_segmentation", "kind": "function", "doc": "<p>Convert the output of the automatic mask generation to an instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>masks:</strong>  The outputs generated by AutomaticMaskGenerator or EmbeddingMaskGenerator.\nOnly supports output_mode=binary_mask.</li>\n<li><strong>with_background:</strong>  Whether the segmentation has background. If yes this function assures that the largest\nobject in the output will be mapped to zero (the background value).</li>\n<li><strong>min_object_size:</strong>  The minimal size of an object in pixels.</li>\n<li><strong>max_object_size:</strong>  The maximal size of an object in pixels.</li>\n<li><strong>label_masks:</strong>  Whether to apply connected components to the result before removing small objects.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">masks</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">min_object_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">max_object_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">label_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase", "kind": "class", "doc": "<p>Base class for the automatic mask generators.</p>\n", "bases": "abc.ABC"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.is_initialized", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.is_initialized", "kind": "variable", "doc": "<p>Whether the mask generator has already been initialized.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_list", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_list", "kind": "variable", "doc": "<p>The list of mask data after initialization.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_boxes", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_boxes", "kind": "variable", "doc": "<p>The list of crop boxes.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.original_size", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.original_size", "kind": "variable", "doc": "<p>The original image size.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.get_state", "kind": "function", "doc": "<p>Get the initialized state of the mask generator.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>State of the mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.set_state", "kind": "function", "doc": "<p>Set the state of the mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The state of the mask generator, e.g. from serialized state.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.clear_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.clear_state", "kind": "function", "doc": "<p>Clear the state of the mask generator.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>This class implements the same logic as\n<a href=\"https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py\">https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py</a>\nIt decouples the computationally expensive steps of generating masks from the cheap post-processing operation\nto filter these masks to enable grid search and interactively changing the post-processing.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">amg</span> <span class=\"o\">=</span> <span class=\"n\">AutomaticMaskGenerator</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">)</span>\n<span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>  <span class=\"c1\"># Initialize the masks, this takes care of all expensive computations.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">pred_iou_thresh</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the masks. This is fast and enables testing parameters</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>crop_n_layers:</strong>  If &gt;0, the mask prediction will be run again on crops of the image.</li>\n<li><strong>crop_overlap_ratio:</strong>  Sets the degree to which crops overlap.</li>\n<li><strong>crop_n_points_downscale_factor:</strong>  How the number of points is downsampled when predicting with crops.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n</ul>\n", "bases": "AMGBase"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_layers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">crop_overlap_ratio</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.3413333333333333</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_points_downscale_factor</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred_iou_thresh:</strong>  Filter threshold in [0, 1], using the mask quality predicted by the model.</li>\n<li><strong>stability_score_thresh:</strong>  Filter threshold in [0, 1], using the stability of the mask\nunder changes to the cutoff used to binarize the model prediction.</li>\n<li><strong>box_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks.</li>\n<li><strong>crop_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks between crops.</li>\n<li><strong>min_mask_region_area:</strong>  Minimal size for the predicted masks.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">pred_iou_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.88</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.95</span>,</span><span class=\"param\">\t<span class=\"n\">box_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">crop_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">min_mask_region_area</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>Implements the same functionality as <code>AutomaticMaskGenerator</code> but for tiled embeddings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n</ul>\n", "bases": "AutomaticMaskGenerator"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_shape:</strong>  The tile shape for embedding prediction.</li>\n<li><strong>halo:</strong>  The overlap of between tiles.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter", "kind": "class", "doc": "<p>Adapter to contain the UNETR decoder in a single module.</p>\n\n<p>To apply the decoder on top of pre-computed embeddings for\nthe segmentation functionality.\nSee also: <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unetr.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unetr.py</a></p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unetr</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.base", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.base", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.out_conv", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.out_conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv_out", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv_out", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.decoder_head", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.decoder_head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.final_activation", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.final_activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.postprocess_masks", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.postprocess_masks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.decoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv1", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv2", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv3", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv4", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv4", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.forward", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">input_shape</span>, </span><span class=\"param\"><span class=\"n\">original_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_unetr", "modulename": "micro_sam.instance_segmentation", "qualname": "get_unetr", "kind": "function", "doc": "<p>Get UNETR model for automatic instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_encoder:</strong>  The image encoder of the SAM model.\nThis is used as encoder by the UNETR too.</li>\n<li><strong>decoder_state:</strong>  Optional decoder state to initialize the weights\nof the UNETR decoder.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The UNETR model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_encoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">decoder_state</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "get_decoder", "kind": "function", "doc": "<p>Get decoder to predict outputs for automatic instance segmentation</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_encoder:</strong>  The image encoder of the SAM model.</li>\n<li><strong>decoder_state:</strong>  State to initialize the weights of the UNETR decoder.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The decoder for instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_encoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">decoder_state</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">DecoderAdapter</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_predictor_and_decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "get_predictor_and_decoder", "kind": "function", "doc": "<p>Load the SAM model (predictor) and instance segmentation decoder.</p>\n\n<p>This requires a checkpoint that contains the state for both predictor\nand decoder.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The type of the image encoder used in the SAM model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the checkpoint from which to load the data.</li>\n<li><strong>device:</strong>  The device.</li>\n<li><strong>lora_rank:</strong>  The rank for low rank adaptation of the attention layers.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The SAM predictor.\n  The decoder for instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">peft_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">DecoderAdapter</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a decoder.</p>\n\n<p>Implements the same interface as <code>AutomaticMaskGenerator</code>.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">segmenter</span> <span class=\"o\">=</span> <span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">,</span> <span class=\"n\">decoder</span><span class=\"p\">)</span>\n<span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>   <span class=\"c1\"># Predict the image embeddings and decoder outputs.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">center_distance_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the instance segmentation.</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>decoder:</strong>  The decoder to predict intermediate representations\nfor instance segmentation.</li>\n</ul>\n"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.is_initialized", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.is_initialized", "kind": "variable", "doc": "<p>Whether the mask generator has already been initialized.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to be verbose.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>center_distance_threshold:</strong>  Center distance predictions below this value will be\nused to find seeds (intersected with thresholded boundary distance predictions).</li>\n<li><strong>boundary_distance_threshold:</strong>  Boundary distance predictions below this value will be\nused to find seeds (intersected with thresholded center distance predictions).</li>\n<li><strong>foreground_smoothing:</strong>  Sigma value for smoothing the foreground predictions, to avoid\ncheckerboard artifacts in the prediction.</li>\n<li><strong>foreground_threshold:</strong>  Foreground predictions above this value will be used as foreground mask.</li>\n<li><strong>distance_smoothing:</strong>  Sigma value for smoothing the distance predictions.</li>\n<li><strong>min_size:</strong>  Minimal object size in the segmentation result.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in. Pass None to directly return the instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">center_distance_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_smoothing</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.6</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.get_state", "kind": "function", "doc": "<p>Get the initialized state of the instance segmenter.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Instance segmentation state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.set_state", "kind": "function", "doc": "<p>Set the state of the instance segmenter.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The instance segmentation state</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.clear_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.clear_state", "kind": "function", "doc": "<p>Clear the state of the instance segmenter.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledInstanceSegmentationWithDecoder", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledInstanceSegmentationWithDecoder", "kind": "class", "doc": "<p>Same as <code>InstanceSegmentationWithDecoder</code> but for tiled image embeddings.</p>\n", "bases": "InstanceSegmentationWithDecoder"}, {"fullname": "micro_sam.instance_segmentation.TiledInstanceSegmentationWithDecoder.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledInstanceSegmentationWithDecoder.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Dummy input to be compatible with other function signatures.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_amg", "modulename": "micro_sam.instance_segmentation", "qualname": "get_amg", "kind": "function", "doc": "<p>Get the automatic mask generator class.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>is_tiled:</strong>  Whether tiled embeddings are used.</li>\n<li><strong>decoder:</strong>  Decoder to predict instacne segmmentation.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The automatic mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">is_tiled</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models", "modulename": "micro_sam.models", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.build_sam", "modulename": "micro_sam.models.build_sam", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.build_sam.build_sam_vit_h", "modulename": "micro_sam.models.build_sam", "qualname": "build_sam_vit_h", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_multimask_outputs</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">image_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.build_sam.build_sam", "modulename": "micro_sam.models.build_sam", "qualname": "build_sam", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_multimask_outputs</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">image_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.build_sam.build_sam_vit_l", "modulename": "micro_sam.models.build_sam", "qualname": "build_sam_vit_l", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_multimask_outputs</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">image_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.build_sam.build_sam_vit_b", "modulename": "micro_sam.models.build_sam", "qualname": "build_sam_vit_b", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">num_multimask_outputs</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">image_size</span><span class=\"o\">=</span><span class=\"mi\">1024</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.build_sam.sam_model_registry", "modulename": "micro_sam.models.build_sam", "qualname": "sam_model_registry", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;default&#x27;: &lt;function build_sam_vit_h&gt;, &#x27;vit_h&#x27;: &lt;function build_sam_vit_h&gt;, &#x27;vit_l&#x27;: &lt;function build_sam_vit_l&gt;, &#x27;vit_b&#x27;: &lt;function build_sam_vit_b&gt;}"}, {"fullname": "micro_sam.models.peft_sam", "modulename": "micro_sam.models.peft_sam", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery", "kind": "class", "doc": "<p>Operates on the attention layers for performing low-rank adaptation.</p>\n\n<p>(Inspired from: <a href=\"https://github.com/JamesQFreeman/Sam_LoRA/\">https://github.com/JamesQFreeman/Sam_LoRA/</a>)</p>\n\n<p>In SAM, it is implemented as:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">qkv</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">dim</span> <span class=\"o\">*</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"n\">qkv_bias</span><span class=\"p\">)</span>\n<span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n<span class=\"n\">qkv</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">qkv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">B</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_heads</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">head_dim</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">permute</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">qkv</span><span class=\"o\">.</span><span class=\"n\">unbind</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>rank:</strong>  The rank of the decomposition matrices for updating weights in each attention layer.</li>\n<li><strong>block:</strong>  The chosen attention blocks for implementing lora.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">rank</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.qkv_proj", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.qkv_proj", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.dim", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.alpha", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.rank", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.rank", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.w_a_linear_q", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.w_a_linear_q", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.w_b_linear_q", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.w_b_linear_q", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.w_a_linear_v", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.w_a_linear_v", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.w_b_linear_v", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.w_b_linear_v", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.reset_parameters", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.reset_parameters", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.LoRASurgery.forward", "modulename": "micro_sam.models.peft_sam", "qualname": "LoRASurgery.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery", "kind": "class", "doc": "<p>Operates on the attention layers for performing factorized attention.</p>\n\n<p>(Inspired from: <a href=\"https://github.com/cchen-cc/MA-SAM/blob/main/MA-SAM/sam_fact_tt_image_encoder.py\">https://github.com/cchen-cc/MA-SAM/blob/main/MA-SAM/sam_fact_tt_image_encoder.py</a>)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>rank:</strong>  The rank of the decomposition matrices for updating weights in each attention layer.</li>\n<li><strong>block:</strong>  The chosen attention blocks for implementing fact.</li>\n<li><strong>dropout:</strong>  The dropout rate for the factorized attention.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">rank</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.qkv_proj", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.qkv_proj", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.dim", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.q_FacTs", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.q_FacTs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.v_FacTs", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.v_FacTs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.dropout", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.dropout", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.FacTu", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.FacTu", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.FacTv", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.FacTv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.FacTSurgery.forward", "modulename": "micro_sam.models.peft_sam", "qualname": "FacTSurgery.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.ScaleShiftLayer", "modulename": "micro_sam.models.peft_sam", "qualname": "ScaleShiftLayer", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.peft_sam.ScaleShiftLayer.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "ScaleShiftLayer.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">layer</span>, </span><span class=\"param\"><span class=\"n\">dim</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.ScaleShiftLayer.layer", "modulename": "micro_sam.models.peft_sam", "qualname": "ScaleShiftLayer.layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.ScaleShiftLayer.scale", "modulename": "micro_sam.models.peft_sam", "qualname": "ScaleShiftLayer.scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.ScaleShiftLayer.shift", "modulename": "micro_sam.models.peft_sam", "qualname": "ScaleShiftLayer.shift", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.ScaleShiftLayer.forward", "modulename": "micro_sam.models.peft_sam", "qualname": "ScaleShiftLayer.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.SSFSurgery", "modulename": "micro_sam.models.peft_sam", "qualname": "SSFSurgery", "kind": "class", "doc": "<p>Operates on all layers in the transformer block for adding learnable scale and shift parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>rank:</strong>  This parameter is not used in <code>SSFSurgery</code>. This is kept here for consistency.</li>\n<li><strong>block:</strong>  The chosen attention blocks for implementing ssf.</li>\n<li><strong>dim:</strong>  The input dimensions determining the shape of scale and shift parameters.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.peft_sam.SSFSurgery.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "SSFSurgery.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">rank</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.SSFSurgery.block", "modulename": "micro_sam.models.peft_sam", "qualname": "SSFSurgery.block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.SSFSurgery.forward", "modulename": "micro_sam.models.peft_sam", "qualname": "SSFSurgery.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.SelectiveSurgery", "modulename": "micro_sam.models.peft_sam", "qualname": "SelectiveSurgery", "kind": "class", "doc": "<p>Base class for selectively allowing gradient updates for certain parameters.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.peft_sam.SelectiveSurgery.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "SelectiveSurgery.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.SelectiveSurgery.block", "modulename": "micro_sam.models.peft_sam", "qualname": "SelectiveSurgery.block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.SelectiveSurgery.allow_gradient_update_for_parameters", "modulename": "micro_sam.models.peft_sam", "qualname": "SelectiveSurgery.allow_gradient_update_for_parameters", "kind": "function", "doc": "<p>This function decides the parameter attributes to match for allowing gradient updates.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prefix:</strong>  Matches the part of parameter name in front.</li>\n<li><strong>suffix:</strong>  Matches the part of parameter name at the end.</li>\n<li><strong>infix:</strong>  Matches parts of parameter name occuring in between.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">prefix</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">suffix</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">infix</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.SelectiveSurgery.forward", "modulename": "micro_sam.models.peft_sam", "qualname": "SelectiveSurgery.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer", "kind": "class", "doc": "<p>Adds AdaptFormer Module in place of the MLP Layers</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>rank:</strong>  The rank is not used in this class but kept here for consistency.</li>\n<li><strong>block:</strong>  The chosen encoder block for implementing AdaptFormer.</li>\n<li><strong>alpha:</strong>  A parameters that scales the Adapter path. Can be either learnable or some fixed value.</li>\n<li><strong>dropout:</strong>  The dropout rate for the dropout layer between down and up projection layer.</li>\n<li><strong>projection_size:</strong>  The size of the projection layer.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">rank</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">alpha</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;learnable_scalar&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">projection_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.mlp_proj", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.mlp_proj", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.n_embd", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.n_embd", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.projection_size", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.projection_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.dropout", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.dropout", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.down_proj", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.down_proj", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.non_linear_func", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.non_linear_func", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.up_proj", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.up_proj", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.AdaptFormer.forward", "modulename": "micro_sam.models.peft_sam", "qualname": "AdaptFormer.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.peft_sam.AttentionSurgery", "modulename": "micro_sam.models.peft_sam", "qualname": "AttentionSurgery", "kind": "class", "doc": "<p>Child class for allowing gradient updates for parameters in attention layers.</p>\n", "bases": "SelectiveSurgery"}, {"fullname": "micro_sam.models.peft_sam.AttentionSurgery.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "AttentionSurgery.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.BiasSurgery", "modulename": "micro_sam.models.peft_sam", "qualname": "BiasSurgery", "kind": "class", "doc": "<p>Child class for allowing gradient updates for bias parameters.</p>\n", "bases": "SelectiveSurgery"}, {"fullname": "micro_sam.models.peft_sam.BiasSurgery.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "BiasSurgery.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.LayerNormSurgery", "modulename": "micro_sam.models.peft_sam", "qualname": "LayerNormSurgery", "kind": "class", "doc": "<p>Child class for allowing gradient updates in normalization layers.</p>\n", "bases": "SelectiveSurgery"}, {"fullname": "micro_sam.models.peft_sam.LayerNormSurgery.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "LayerNormSurgery.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.PEFT_Sam", "modulename": "micro_sam.models.peft_sam", "qualname": "PEFT_Sam", "kind": "class", "doc": "<p>Wraps the Segment Anything model's image encoder to different parameter efficient finetuning methods.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model:</strong>  The Segment Anything model.</li>\n<li><strong>rank:</strong>  The rank for low-rank adaptation.</li>\n<li><strong>peft_module:</strong>  Wrapper to operate on the image encoder blocks for the PEFT method.</li>\n<li><strong>attention_layers_to_update:</strong>  Which specific layers we apply PEFT methods to.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.peft_sam.PEFT_Sam.__init__", "modulename": "micro_sam.models.peft_sam", "qualname": "PEFT_Sam.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">modeling</span><span class=\"o\">.</span><span class=\"n\">sam</span><span class=\"o\">.</span><span class=\"n\">Sam</span>,</span><span class=\"param\">\t<span class=\"n\">rank</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tpeft_module: torch.nn.modules.module.Module = &lt;class &#x27;micro_sam.models.peft_sam.LoRASurgery&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">attention_layers_to_update</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">module_kwargs</span></span>)</span>"}, {"fullname": "micro_sam.models.peft_sam.PEFT_Sam.peft_module", "modulename": "micro_sam.models.peft_sam", "qualname": "PEFT_Sam.peft_module", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.PEFT_Sam.peft_blocks", "modulename": "micro_sam.models.peft_sam", "qualname": "PEFT_Sam.peft_blocks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.PEFT_Sam.sam", "modulename": "micro_sam.models.peft_sam", "qualname": "PEFT_Sam.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.peft_sam.PEFT_Sam.forward", "modulename": "micro_sam.models.peft_sam", "qualname": "PEFT_Sam.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batched_input</span>, </span><span class=\"param\"><span class=\"n\">multimask_output</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.sam_3d_wrapper", "modulename": "micro_sam.models.sam_3d_wrapper", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.get_sam_3d_model", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "get_sam_3d_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">n_classes</span>,</span><span class=\"param\">\t<span class=\"n\">image_size</span>,</span><span class=\"param\">\t<span class=\"n\">lora_rank</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">freeze_encoder</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;vit_b&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.sam_3d_wrapper.Sam3DWrapper", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "Sam3DWrapper", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.sam_3d_wrapper.Sam3DWrapper.__init__", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "Sam3DWrapper.__init__", "kind": "function", "doc": "<p>Initializes the Sam3DWrapper object.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sam_model:</strong>  The Sam model to be wrapped.</li>\n<li><strong>freeze_encoder:</strong>  Whether to freeze the image encoder.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">sam_model</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">modeling</span><span class=\"o\">.</span><span class=\"n\">sam</span><span class=\"o\">.</span><span class=\"n\">Sam</span>, </span><span class=\"param\"><span class=\"n\">freeze_encoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span>)</span>"}, {"fullname": "micro_sam.models.sam_3d_wrapper.Sam3DWrapper.sam_model", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "Sam3DWrapper.sam_model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.Sam3DWrapper.freeze_encoder", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "Sam3DWrapper.freeze_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.Sam3DWrapper.forward", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "Sam3DWrapper.forward", "kind": "function", "doc": "<p>Predict 3D masks for the current inputs.</p>\n\n<p>Unlike original SAM this model only supports automatic segmentation and does not support prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batched_input:</strong>  A list over input images, each a dictionary with the following keys.\n'image': The image as a torch tensor in 3xDxHxW format. Already transformed for the input to the model.\n'original_size': The original size of the image (HxW) before transformation.</li>\n<li><strong>multimask_output:</strong>  Wheterh to predict with the multi- or single-mask head of the maks decoder.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A list over input images, where each element is as dictionary with the following keys:\n      'masks': Mask prediction for this object.\n      'iou_predictions': IOU score prediction for this object.\n      'low_res_masks': Low resolution mask prediction for this object.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batched_input</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.sam_3d_wrapper.ImageEncoderViT3DWrapper", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "ImageEncoderViT3DWrapper", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.sam_3d_wrapper.ImageEncoderViT3DWrapper.__init__", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "ImageEncoderViT3DWrapper.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_encoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">num_heads</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">12</span>,</span><span class=\"param\">\t<span class=\"n\">embed_dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">768</span></span>)</span>"}, {"fullname": "micro_sam.models.sam_3d_wrapper.ImageEncoderViT3DWrapper.image_encoder", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "ImageEncoderViT3DWrapper.image_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.ImageEncoderViT3DWrapper.img_size", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "ImageEncoderViT3DWrapper.img_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.ImageEncoderViT3DWrapper.forward", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "ImageEncoderViT3DWrapper.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">d_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.__init__", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">block</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">dim</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">num_heads</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\tnorm_layer: Type[torch.nn.modules.module.Module] = &lt;class &#x27;torch.nn.modules.normalization.LayerNorm&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">adapter_channels</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">384</span></span>)</span>"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.block", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.block", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_channels", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_linear_down", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_linear_down", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_linear_up", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_linear_up", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_conv", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_act", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_act", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_norm", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_norm", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_linear_down_2", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_linear_down_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_linear_up_2", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_linear_up_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_conv_2", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_conv_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_act_2", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_act_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.adapter_norm_2", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.adapter_norm_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.sam_3d_wrapper.NDBlockWrapper.forward", "modulename": "micro_sam.models.sam_3d_wrapper", "qualname": "NDBlockWrapper.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">d_size</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.get_simple_sam_3d_model", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "get_simple_sam_3d_model", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">n_classes</span>,</span><span class=\"param\">\t<span class=\"n\">image_size</span>,</span><span class=\"param\">\t<span class=\"n\">lora_rank</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">freeze_encoder</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;vit_b&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock.__init__", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span>,</span><span class=\"param\">\t<span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;nearest&#39;</span></span>)</span>"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock.conv1", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock.conv1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock.conv2", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock.conv2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock.downsample", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock.downsample", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock.leakyrelu", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock.leakyrelu", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock.up", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock.up", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.BasicBlock.forward", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "BasicBlock.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SegmentationHead", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SegmentationHead", "kind": "class", "doc": "<p>A sequential container.</p>\n\n<p>Modules will be added to it in the order they are passed in the\nconstructor. Alternatively, an <code>OrderedDict</code> of modules can be\npassed in. The <code>forward()</code> method of <code>Sequential</code> accepts any\ninput and forwards it to the first module it contains. It then\n\"chains\" outputs to inputs sequentially for each subsequent module,\nfinally returning the output of the last module.</p>\n\n<p>The value a <code>Sequential</code> provides over manually calling a sequence\nof modules is that it allows treating the whole container as a\nsingle module, such that performing a transformation on the\n<code>Sequential</code> applies to each of the modules it stores (which are\neach a registered submodule of the <code>Sequential</code>).</p>\n\n<p>What's the difference between a <code>Sequential</code> and a\n<code>torch.nn.ModuleList</code>? A <code>ModuleList</code> is exactly what it\nsounds like--a list for storing <code>Module</code> s! On the other hand,\nthe layers in a <code>Sequential</code> are connected in a cascading way.</p>\n\n<p>Example::</p>\n\n<pre><code># Using Sequential to create a small model. When `model` is run,\n# input will first be passed to `Conv2d(1,20,5)`. The output of\n# `Conv2d(1,20,5)` will be used as the input to the first\n# `ReLU`; the output of the first `ReLU` will become the input\n# for `Conv2d(20,64,5)`. Finally, the output of\n# `Conv2d(20,64,5)` will be used as input to the second `ReLU`\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n</code></pre>\n", "bases": "torch.nn.modules.container.Sequential"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SegmentationHead.__init__", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SegmentationHead.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">in_channels</span>,</span><span class=\"param\">\t<span class=\"n\">out_channels</span>,</span><span class=\"param\">\t<span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>,</span><span class=\"param\">\t<span class=\"n\">bias</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SegmentationHead.conv_pred", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SegmentationHead.conv_pred", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SegmentationHead.segmentation_head", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SegmentationHead.segmentation_head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SegmentationHead.forward", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SegmentationHead.forward", "kind": "function", "doc": "<p>Define the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SimpleSam3DWrapper", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SimpleSam3DWrapper", "kind": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to()</code>, etc.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>As per the example above, an <code>__init__()</code> call to the parent class\nmust be made before assignment on the child.</p>\n\n</div>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SimpleSam3DWrapper.__init__", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SimpleSam3DWrapper.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">sam</span>, </span><span class=\"param\"><span class=\"n\">num_classes</span>, </span><span class=\"param\"><span class=\"n\">freeze_encoder</span></span>)</span>"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SimpleSam3DWrapper.sam", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SimpleSam3DWrapper.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SimpleSam3DWrapper.freeze_encoder", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SimpleSam3DWrapper.freeze_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SimpleSam3DWrapper.decoders", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SimpleSam3DWrapper.decoders", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SimpleSam3DWrapper.out_conv", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SimpleSam3DWrapper.out_conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.models.simple_sam_3d_wrapper.SimpleSam3DWrapper.forward", "modulename": "micro_sam.models.simple_sam_3d_wrapper", "qualname": "SimpleSam3DWrapper.forward", "kind": "function", "doc": "<p>Predict 3D masks for the current inputs.</p>\n\n<p>Unlike original SAM this model only supports automatic segmentation and does not support prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batched_input:</strong>  A list over input images, each a dictionary with the following keys.L\n'image': The image as a torch tensor in 3xDxHxW format. Already transformed for the input to the model.</li>\n<li><strong>multimask_output:</strong>  Wheterh to predict with the multi- or single-mask head of the maks decoder.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A list over input images, where each element is as dictionary with the following keys:\n      'masks': Mask prediction for this object.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batched_input</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation", "modulename": "micro_sam.multi_dimensional_segmentation", "kind": "module", "doc": "<p>Multi-dimensional segmentation with segment anything.</p>\n"}, {"fullname": "micro_sam.multi_dimensional_segmentation.PROJECTION_MODES", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "PROJECTION_MODES", "kind": "variable", "doc": "<p></p>\n", "default_value": "(&#x27;box&#x27;, &#x27;mask&#x27;, &#x27;points&#x27;, &#x27;points_and_mask&#x27;, &#x27;single_point&#x27;)"}, {"fullname": "micro_sam.multi_dimensional_segmentation.segment_mask_in_volume", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "segment_mask_in_volume", "kind": "function", "doc": "<p>Segment an object mask in in volumetric data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The initial segmentation for the object.</li>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>image_embeddings:</strong>  The precomputed image embeddings for the volume.</li>\n<li><strong>segmented_slices:</strong>  List of slices for which this object has already been segmented.</li>\n<li><strong>stop_lower:</strong>  Whether to stop at the lowest segmented slice.</li>\n<li><strong>stop_upper:</strong>  Wheter to stop at the topmost segmented slice.</li>\n<li><strong>iou_threshold:</strong>  The IOU threshold for continuing segmentation across 3d.</li>\n<li><strong>projection:</strong>  The projection method to use. One of 'box', 'mask', 'points', 'points_and_mask' or 'single point'.\nPass a dictionary to choose the excact combination of projection modes.</li>\n<li><strong>update_progress:</strong>  Callback to update an external progress bar.</li>\n<li><strong>box_extension:</strong>  Extension factor for increasing the box size after projection.</li>\n<li><strong>verbose:</strong>  Whether to print details about the segmentation steps.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Array with the volumetric segmentation.\n  Tuple with the first and last segmented slice.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">segmented_slices</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">stop_lower</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">stop_upper</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">iou_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">projection</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">update_progress</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation.merge_instance_segmentation_3d", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "merge_instance_segmentation_3d", "kind": "function", "doc": "<p>Merge stacked 2d instance segmentations into a consistent 3d segmentation.</p>\n\n<p>Solves a multicut problem based on the overlap of objects to merge across z.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>slice_segmentation:</strong>  The stacked segmentation across the slices.\nWe assume that the segmentation is labeled consecutive across z.</li>\n<li><strong>beta:</strong>  The bias term for the multicut. Higher values lead to a larger\ndegree of over-segmentation and vice versa.</li>\n<li><strong>with_background:</strong>  Whether this is a segmentation problem with background.\nIn that case all edges connecting to the background are set to be repulsive.</li>\n<li><strong>gap_closing:</strong>  If given, gaps in the segmentation are closed with a binary closing\noperation. The value is used to determine the number of iterations for the closing.</li>\n<li><strong>min_z_extent:</strong>  Require a minimal extent in z for the segmented objects.\nThis can help to prevent segmentation artifacts.</li>\n<li><strong>verbose:</strong>  Verbosity flag.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The merged segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">slice_segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gap_closing</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_z_extent</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation.automatic_3d_segmentation", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "automatic_3d_segmentation", "kind": "function", "doc": "<p>Segment volume in 3d.</p>\n\n<p>First segments slices individually in 2d and then merges them across 3d\nbased on overlap of objects between slices.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>predictor:</strong>  The SAM model.</li>\n<li><strong>segmentor:</strong>  The instance segmentation class.</li>\n<li><strong>embedding_path:</strong>  The path to save pre-computed embeddings.</li>\n<li><strong>with_background:</strong>  Whether the segmentation has background.</li>\n<li><strong>gap_closing:</strong>  If given, gaps in the segmentation are closed with a binary closing\noperation. The value is used to determine the number of iterations for the closing.</li>\n<li><strong>min_z_extent:</strong>  Require a minimal extent in z for the segmented objects.\nThis can help to prevent segmentation artifacts.</li>\n<li><strong>tile_shape:</strong>  Shape of the tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>verbose:</strong>  Verbosity flag.</li>\n<li><strong>kwargs:</strong>  Keyword arguments for the 'generate' method of the 'segmentor'.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">segmentor</span><span class=\"p\">:</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gap_closing</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_z_extent</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state", "modulename": "micro_sam.precompute_state", "kind": "module", "doc": "<p>Precompute image embeddings and automatic mask generator state for image data.</p>\n"}, {"fullname": "micro_sam.precompute_state.cache_amg_state", "modulename": "micro_sam.precompute_state", "qualname": "cache_amg_state", "kind": "function", "doc": "<p>Compute and cache or load the state for the automatic mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n<li><strong>save_path:</strong>  The embedding save path. The AMG state will be stored in 'save_path/amg_state.pickle'.</li>\n<li><strong>verbose:</strong>  Whether to run the computation verbose.</li>\n<li><strong>i:</strong>  The index for which to cache the state.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The automatic mask generator class with the cached state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">raw</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state.cache_is_state", "modulename": "micro_sam.precompute_state", "qualname": "cache_is_state", "kind": "function", "doc": "<p>Compute and cache or load the state for the automatic mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>decoder:</strong>  The instance segmentation decoder.</li>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n<li><strong>save_path:</strong>  The embedding save path. The AMG state will be stored in 'save_path/amg_state.pickle'.</li>\n<li><strong>verbose:</strong>  Whether to run the computation verbose.</li>\n<li><strong>i:</strong>  The index for which to cache the state.</li>\n<li><strong>skip_load:</strong>  Skip loading the state if it is precomputed.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation class with the cached state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">raw</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">skip_load</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state.precompute_state", "modulename": "micro_sam.precompute_state", "qualname": "precompute_state", "kind": "function", "doc": "<p>Precompute the image embeddings and other optional state for the input image(s).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_path:</strong>  The input image file(s). Can either be a single image file (e.g. tif or png),\na container file (e.g. hdf5 or zarr) or a folder with images files.\nIn case of a container file the argument <code>key</code> must be given. In case of a folder\nit can be given to provide a glob pattern to subselect files from the folder.</li>\n<li><strong>output_path:</strong>  The output path where the embeddings and other state will be saved.</li>\n<li><strong>pattern:</strong>  Glob pattern to select files in a folder. The embeddings will be computed\nfor each of these files. To select all files in a folder pass \"*\".</li>\n<li><strong>model_type:</strong>  The SegmentAnything model to use. Will use the standard vit_l model by default.</li>\n<li><strong>checkpoint_path:</strong>  Path to a checkpoint for a custom model.</li>\n<li><strong>key:</strong>  The key to the input file. This is needed for contaner files (e.g. hdf5 or zarr)\nor to load several images as 3d volume. Provide a glob pattern, e.g. \"*.tif\", for this case.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic instance segmentation\nin addition to the image embeddings.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation", "modulename": "micro_sam.prompt_based_segmentation", "kind": "module", "doc": "<p>Functions for prompt-based segmentation with Segment Anything.</p>\n"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_points", "kind": "function", "doc": "<p>Segmentation from point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points:</strong>  The point prompts given in the image coordinate system.</li>\n<li><strong>labels:</strong>  The labels (positive or negative) associated with the points.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>use_best_multimask:</strong>  Whether to use multimask output and then choose the best mask.\nBy default this is used for a single positive point and not otherwise.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_best_multimask</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_mask", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_mask", "kind": "function", "doc": "<p>Segmentation from a mask prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>mask:</strong>  The mask used to derive prompts.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>use_box:</strong>  Whether to derive the bounding box prompt from the mask.</li>\n<li><strong>use_mask:</strong>  Whether to use the mask itself as prompt.</li>\n<li><strong>use_points:</strong>  Whether to derive point prompts from the mask.</li>\n<li><strong>original_size:</strong>  Full image shape. Use this if the mask that is being passed\ndownsampled compared to the original image.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>box_extension:</strong>  Relative factor used to enlarge the bounding box prompt.</li>\n<li><strong>box:</strong>  Precomputed bounding box.</li>\n<li><strong>points:</strong>  Precomputed point prompts.</li>\n<li><strong>labels:</strong>  Positive/negative labels corresponding to the point prompts.</li>\n<li><strong>use_single_point:</strong>  Whether to derive just a single point from the mask.\nIn case use_points is true.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">original_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_logits</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_single_point</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box", "kind": "function", "doc": "<p>Segmentation from a box prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>box_extension:</strong>  Relative factor used to enlarge the bounding box prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box_and_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box_and_points", "kind": "function", "doc": "<p>Segmentation from a box prompt and point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>points:</strong>  The point prompts, given in the image coordinates system.</li>\n<li><strong>labels:</strong>  The point labels, either positive or negative.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_generators", "modulename": "micro_sam.prompt_generators", "kind": "module", "doc": "<p>Classes for generating prompts from ground-truth segmentation masks.\nFor training or evaluation of prompt-based segmentation.</p>\n"}, {"fullname": "micro_sam.prompt_generators.PromptGeneratorBase", "modulename": "micro_sam.prompt_generators", "qualname": "PromptGeneratorBase", "kind": "class", "doc": "<p>PromptGeneratorBase is an interface to implement specific prompt generators.</p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator", "kind": "class", "doc": "<p>Generate point and/or box prompts from an instance segmentation.</p>\n\n<p>You can use this class to derive prompts from an instance segmentation, either for\nevaluation purposes or for training Segment Anything on custom data.\nIn order to use this generator you need to precompute the bounding boxes and center\ncoordiantes of the instance segmentation, using e.g. <code>util.get_centers_and_bounding_boxes</code>.</p>\n\n<p>Here's an example for how to use this class:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># Initialize generator for 1 positive and 4 negative point prompts.</span>\n<span class=\"n\">prompt_generator</span> <span class=\"o\">=</span> <span class=\"n\">PointAndBoxPromptGenerator</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">dilation_strength</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Precompute the bounding boxes for the given segmentation</span>\n<span class=\"n\">bounding_boxes</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">get_centers_and_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">segmentation</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># generate point prompts for the objects with ids 1, 2 and 3</span>\n<span class=\"n\">seg_ids</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"n\">object_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">segmentation</span> <span class=\"o\">==</span> <span class=\"n\">seg_id</span> <span class=\"k\">for</span> <span class=\"n\">seg_id</span> <span class=\"ow\">in</span> <span class=\"n\">seg_ids</span><span class=\"p\">])[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n<span class=\"n\">this_bounding_boxes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">bounding_boxes</span><span class=\"p\">[</span><span class=\"n\">seg_id</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">seg_id</span> <span class=\"ow\">in</span> <span class=\"n\">seg_ids</span><span class=\"p\">]</span>\n<span class=\"n\">point_coords</span><span class=\"p\">,</span> <span class=\"n\">point_labels</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">prompt_generator</span><span class=\"p\">(</span><span class=\"n\">object_mask</span><span class=\"p\">,</span> <span class=\"n\">this_bounding_boxes</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>n_positive_points:</strong>  The number of positive point prompts to generate per mask.</li>\n<li><strong>n_negative_points:</strong>  The number of negative point prompts to generate per mask.</li>\n<li><strong>dilation_strength:</strong>  The factor by which the mask is dilated before generating prompts.</li>\n<li><strong>get_point_prompts:</strong>  Whether to generate point prompts.</li>\n<li><strong>get_box_prompts:</strong>  Whether to generate box prompts.</li>\n</ul>\n", "bases": "PromptGeneratorBase"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.__init__", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_positive_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_strength</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">get_point_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_box_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_positive_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_positive_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_negative_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_negative_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.dilation_strength", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.dilation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_box_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_box_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_point_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_point_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.IterativePromptGenerator", "modulename": "micro_sam.prompt_generators", "qualname": "IterativePromptGenerator", "kind": "class", "doc": "<p>Generate point prompts from an instance segmentation iteratively.</p>\n", "bases": "PromptGeneratorBase"}, {"fullname": "micro_sam.sam_annotator", "modulename": "micro_sam.sam_annotator", "kind": "module", "doc": "<p>The interactive annotation tools.</p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.Annotator2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "Annotator2d", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.Annotator2d.__init__", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "Annotator2d.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.annotator_2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "annotator_2d", "kind": "function", "doc": "<p>Start the 2d annotation tool for a given image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The image data.</li>\n<li><strong>embedding_path:</strong>  Filepath where to save the embeddings.</li>\n<li><strong>segmentation_result:</strong>  An initial segmentation to load.\nThis can be used to correct segmentations with Segment Anything or to save and load progress.\nThe segmentation will be loaded as the 'committed_objects' layer.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_result</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.annotator_3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.Annotator3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "Annotator3d", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.Annotator3d.__init__", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "Annotator3d.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.annotator_3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "annotator_3d", "kind": "function", "doc": "<p>Start the 3d annotation tool for a given image volume.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The volumetric image data.</li>\n<li><strong>embedding_path:</strong>  Filepath for saving the precomputed embeddings.</li>\n<li><strong>segmentation_result:</strong>  An initial segmentation to load.\nThis can be used to correct segmentations with Segment Anything or to save and load progress.\nThe segmentation will be loaded as the 'committed_objects' layer.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_result</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.AnnotatorTracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "AnnotatorTracking", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.AnnotatorTracking.__init__", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "AnnotatorTracking.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.annotator_tracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "annotator_tracking", "kind": "function", "doc": "<p>Start the tracking annotation tool fora given timeseries.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>embedding_path:</strong>  Filepath for saving the precomputed embeddings.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.image_series_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "image_series_annotator", "kind": "function", "doc": "<p>Run the annotation tool for a series of images (supported for both 2d and 3d images).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>images:</strong>  List of the file paths or list of (set of) slices for the images to be annotated.</li>\n<li><strong>output_folder:</strong>  The folder where the segmentation results are saved.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>embedding_path:</strong>  Filepath where to save the embeddings.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>is_volumetric:</strong>  Whether to use the 3d annotator.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n<li><strong>skip_segmented:</strong>  Whether to skip images that were already segmented.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">images</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_volumetric</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">skip_segmented</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.image_folder_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "image_folder_annotator", "kind": "function", "doc": "<p>Run the 2d annotation tool for a series of images in a folder.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_folder:</strong>  The folder with the images to be annotated.</li>\n<li><strong>output_folder:</strong>  The folder where the segmentation results are saved.</li>\n<li><strong>pattern:</strong>  The glob patter for loading files from <code>input_folder</code>.\nBy default all files will be loaded.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for <code>micro_sam.sam_annotator.image_series_annotator</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;*&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator", "kind": "class", "doc": "<p>QWidget(parent: typing.Optional[QWidget] = None, flags: Union[Qt.WindowFlags, Qt.WindowType] = Qt.WindowFlags())</p>\n", "bases": "micro_sam.sam_annotator._widgets._WidgetBase"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator.__init__", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span>, </span><span class=\"param\"><span class=\"n\">parent</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator.run_button", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator.run_button", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.training_ui", "modulename": "micro_sam.sam_annotator.training_ui", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget", "kind": "class", "doc": "<p>QWidget(parent: typing.Optional[QWidget] = None, flags: Union[Qt.WindowFlags, Qt.WindowType] = Qt.WindowFlags())</p>\n", "bases": "micro_sam.sam_annotator._widgets._WidgetBase"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget.__init__", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">parent</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget.run_button", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget.run_button", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.util", "modulename": "micro_sam.sam_annotator.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.util.point_layer_to_prompts", "modulename": "micro_sam.sam_annotator.util", "qualname": "point_layer_to_prompts", "kind": "function", "doc": "<p>Extract point prompts for SAM from a napari point layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer:</strong>  The point layer from which to extract the prompts.</li>\n<li><strong>i:</strong>  Index for the data (required for 3d or timeseries data).</li>\n<li><strong>track_id:</strong>  Id of the current track (required for tracking data).</li>\n<li><strong>with_stop_annotation:</strong>  Whether a single negative point will be interpreted\nas stop annotation or just returned as normal prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The point coordinates for the prompts.\n  The labels (positive or negative / 1 or 0) for the prompts.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">track_id</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_stop_annotation</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.shape_layer_to_prompts", "modulename": "micro_sam.sam_annotator.util", "qualname": "shape_layer_to_prompts", "kind": "function", "doc": "<p>Extract prompts for SAM from a napari shape layer.</p>\n\n<p>Extracts the bounding box for 'rectangle' shapes and the bounding box and corresponding mask\nfor 'ellipse' and 'polygon' shapes.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prompt_layer:</strong>  The napari shape layer.</li>\n<li><strong>shape:</strong>  The image shape.</li>\n<li><strong>i:</strong>  Index for the data (required for 3d or timeseries data).</li>\n<li><strong>track_id:</strong>  Id of the current track (required for tracking data).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The box prompts.\n  The mask prompts.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">Shapes</span>,</span><span class=\"param\">\t<span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">track_id</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.prompt_layer_to_state", "modulename": "micro_sam.sam_annotator.util", "qualname": "prompt_layer_to_state", "kind": "function", "doc": "<p>Get the state of the track from a point layer for a given timeframe.</p>\n\n<p>Only relevant for annotator_tracking.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prompt_layer:</strong>  The napari layer.</li>\n<li><strong>i:</strong>  Timeframe of the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The state of this frame (either \"division\" or \"track\").</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">prompt_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>, </span><span class=\"param\"><span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.prompt_layers_to_state", "modulename": "micro_sam.sam_annotator.util", "qualname": "prompt_layers_to_state", "kind": "function", "doc": "<p>Get the state of the track from a point layer and shape layer for a given timeframe.</p>\n\n<p>Only relevant for annotator_tracking.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>point_layer:</strong>  The napari point layer.</li>\n<li><strong>box_layer:</strong>  The napari box layer.</li>\n<li><strong>i:</strong>  Timeframe of the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The state of this frame (either \"division\" or \"track\").</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">point_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>,</span><span class=\"param\">\t<span class=\"n\">box_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">Shapes</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data", "modulename": "micro_sam.sample_data", "kind": "module", "doc": "<p>Sample microscopy data.</p>\n\n<p>You can change the download location for sample data and model weights\nby setting the environment variable: MICROSAM_CACHEDIR</p>\n\n<p>By default sample data is downloaded to a folder named 'micro_sam/sample_data'\ninside your default cache directory, eg:\n    * Mac: ~/Library/Caches/<AppName>\n    * Unix: ~/.cache/<AppName> or the value of the XDG_CACHE_HOME environment variable, if defined.\n    * Windows: C:\\Users&lt;user>\\AppData\\Local&lt;AppAuthor>&lt;AppName>\\Cache</p>\n"}, {"fullname": "micro_sam.sample_data.fetch_image_series_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_image_series_example_data", "kind": "function", "doc": "<p>Download the sample images for the image series annotator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_image_series", "modulename": "micro_sam.sample_data", "qualname": "sample_data_image_series", "kind": "function", "doc": "<p>Provides image series example image to napari.</p>\n\n<p>Opens as three separate image layers in napari (one per image in series).\nThe third image in the series has a different size and modality.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_wholeslide_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_wholeslide_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads part of a whole-slide image from the NeurIPS Cell Segmentation Challenge.\nSee <a href=\"https://neurips22-cellseg.grand-challenge.org/\">https://neurips22-cellseg.grand-challenge.org/</a> for details on the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_wholeslide", "modulename": "micro_sam.sample_data", "qualname": "sample_data_wholeslide", "kind": "function", "doc": "<p>Provides wholeslide 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_livecell_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_livecell_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the LiveCELL dataset.\nSee <a href=\"https://doi.org/10.1038/s41592-021-01249-6\">https://doi.org/10.1038/s41592-021-01249-6</a> for details on the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_livecell", "modulename": "micro_sam.sample_data", "qualname": "sample_data_livecell", "kind": "function", "doc": "<p>Provides livecell 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_hela_2d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_hela_2d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the HeLa CTC dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_hela_2d", "modulename": "micro_sam.sample_data", "qualname": "sample_data_hela_2d", "kind": "function", "doc": "<p>Provides HeLa 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_3d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_3d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 3d annotator.</p>\n\n<p>This downloads the Lucchi++ datasets from <a href=\"https://casser.io/connectomics/\">https://casser.io/connectomics/</a>.\nIt is a dataset for mitochondria segmentation in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_3d", "modulename": "micro_sam.sample_data", "qualname": "sample_data_3d", "kind": "function", "doc": "<p>Provides Lucchi++ 3d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_tracking_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_tracking_example_data", "kind": "function", "doc": "<p>Download the sample data for the tracking annotator.</p>\n\n<p>This data is the cell tracking challenge dataset DIC-C2DH-HeLa.\nCell tracking challenge webpage: <a href=\"http://data.celltrackingchallenge.net\">http://data.celltrackingchallenge.net</a>\nHeLa cells on a flat glass\nDr. G. van Cappellen. Erasmus Medical Center, Rotterdam, The Netherlands\nTraining dataset: <a href=\"http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip</a> (37 MB)\nChallenge dataset: <a href=\"http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip</a> (41 MB)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_tracking", "modulename": "micro_sam.sample_data", "qualname": "sample_data_tracking", "kind": "function", "doc": "<p>Provides tracking example dataset to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_tracking_segmentation_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_tracking_segmentation_data", "kind": "function", "doc": "<p>Download groundtruth segmentation for the tracking example data.</p>\n\n<p>This downloads the groundtruth segmentation for the image data from <code>fetch_tracking_example_data</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_segmentation", "modulename": "micro_sam.sample_data", "qualname": "sample_data_segmentation", "kind": "function", "doc": "<p>Provides segmentation example dataset to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.synthetic_data", "modulename": "micro_sam.sample_data", "qualname": "synthetic_data", "kind": "function", "doc": "<p>Create synthetic image data and segmentation for training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_nucleus_3d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_nucleus_3d_example_data", "kind": "function", "doc": "<p>Download the sample data for 3d segmentation of nuclei.</p>\n\n<p>This data contains a small crop from a volume from the publication\n\"Efficient automatic 3D segmentation of cell nuclei for high-content screening\"\n<a href=\"https://doi.org/10.1186/s12859-022-04737-4\">https://doi.org/10.1186/s12859-022-04737-4</a></p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training", "modulename": "micro_sam.training", "kind": "module", "doc": "<p>Functionality for training Segment Anything.</p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer", "modulename": "micro_sam.training.joint_sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer", "kind": "class", "doc": "<p>Trainer class for jointly training the Segment Anything model with an additional convolutional decoder.</p>\n\n<p>This class is inherited from <code>SamTrainer</code>.\nCheck out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/training/sam_trainer.py\">https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/training/sam_trainer.py</a>\nfor details on its implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>unetr:</strong>  The UNet-style model with vision transformer as the image encoder.\nRequired to perform automatic instance segmentation.</li>\n<li><strong>instance_loss:</strong>  The loss to compare the predictions (for instance segmentation) and the targets.</li>\n<li><strong>instance_metric:</strong>  The metric to compare the predictions and the targets.</li>\n<li><strong>kwargs:</strong>  The keyword arguments of the <code>SamTrainer</code> (and <code>DefaultTrainer</code>) class.</li>\n</ul>\n", "bases": "micro_sam.training.sam_trainer.SamTrainer"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.__init__", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">unetr</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">instance_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">instance_metric</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.unetr", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.unetr", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.instance_loss", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.instance_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.instance_metric", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.instance_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.save_checkpoint", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.save_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">current_metric</span>, </span><span class=\"param\"><span class=\"n\">best_metric</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">extra_save_dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.load_checkpoint", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.load_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.sam_trainer", "modulename": "micro_sam.training.sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer", "kind": "class", "doc": "<p>Trainer class for training the Segment Anything model.</p>\n\n<p>This class is derived from <code>torch_em.trainer.DefaultTrainer</code>.\nCheck out <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py</a>\nfor details on its usage and implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>convert_inputs:</strong>  The class that converts outputs of the dataloader to the expected input format of SAM.\nThe class <code>micro_sam.training.util.ConvertToSamInputs</code> can be used here.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iteration steps for which the masks predicted for one object are updated.\nIn each sub-iteration new point prompts are sampled where the model was wrong.</li>\n<li><strong>n_objects_per_batch:</strong>  If not given, we compute the loss for all objects in a sample.\nOtherwise the loss computation is limited to n_objects_per_batch, and the objects are randomly sampled.</li>\n<li><strong>mse_loss:</strong>  The regression loss to compare the IoU predicted by the model with the true IoU.</li>\n<li><strong>prompt_generator:</strong>  The iterative prompt generator which takes care of the iterative prompting logic for training</li>\n<li><strong>mask_prob:</strong>  The probability of using the mask inputs in the iterative prompting (per <code>n_sub_iteration</code>)</li>\n<li><strong>mask_loss:</strong>  The loss to compare the predicted masks and the targets.</li>\n<li><strong>kwargs:</strong>  The keyword arguments of the DefaultTrainer super class.</li>\n</ul>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.__init__", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">convert_inputs</span>,</span><span class=\"param\">\t<span class=\"n\">n_sub_iteration</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_objects_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mse_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">=</span> <span class=\"n\">MSELoss</span><span class=\"p\">()</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_generator</span><span class=\"p\">:</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">prompt_generators</span><span class=\"o\">.</span><span class=\"n\">PromptGeneratorBase</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">prompt_generators</span><span class=\"o\">.</span><span class=\"n\">IterativePromptGenerator</span> <span class=\"nb\">object</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">mask_loss</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.convert_inputs", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.convert_inputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.mse_loss", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.mse_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.n_objects_per_batch", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.n_objects_per_batch", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.n_sub_iteration", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.n_sub_iteration", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.prompt_generator", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.prompt_generator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.mask_prob", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.mask_prob", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer", "modulename": "micro_sam.training.semantic_sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.CustomDiceLoss", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "CustomDiceLoss", "kind": "class", "doc": "<p>Loss for computing dice over one-hot labels.</p>\n\n<p>Expects prediction and target with <code>num_classes</code> channels: the number of classes for semantic segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>num_classes:</strong>  The number of classes for semantic segmentation (including background class).</li>\n<li><strong>softmax:</strong>  Whether to use softmax over the predictions.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.training.semantic_sam_trainer.CustomDiceLoss.__init__", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "CustomDiceLoss.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">int</span>, </span><span class=\"param\"><span class=\"n\">softmax</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span>)</span>"}, {"fullname": "micro_sam.training.semantic_sam_trainer.CustomDiceLoss.num_classes", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "CustomDiceLoss.num_classes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.CustomDiceLoss.dice_loss", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "CustomDiceLoss.dice_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.CustomDiceLoss.softmax", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "CustomDiceLoss.softmax", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.SemanticSamTrainer", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "SemanticSamTrainer", "kind": "class", "doc": "<p>Trainer class for training the Segment Anything model for semantic segmentation.</p>\n\n<p>This class is derived from <code>torch_em.trainer.DefaultTrainer</code>.\nCheck out <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py</a>\nfor details on its usage and implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>convert_inputs:</strong>  The class that converts outputs of the dataloader to the expected input format of SAM.\nThe class <code>micro_sam.training.util.ConvertToSemanticSamInputs</code> can be used here.</li>\n<li><strong>num_classes:</strong>  The number of classes for semantic segmentation (including the background class).</li>\n<li><strong>dice_weight:</strong>  The weighing for the dice loss in the combined dice-cross entropy loss function.</li>\n<li><strong>kwargs:</strong>  The keyword arguments of the DefaultTrainer super class.</li>\n</ul>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "micro_sam.training.semantic_sam_trainer.SemanticSamTrainer.__init__", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "SemanticSamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">convert_inputs</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dice_weight</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.semantic_sam_trainer.SemanticSamTrainer.convert_inputs", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "SemanticSamTrainer.convert_inputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.SemanticSamTrainer.num_classes", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "SemanticSamTrainer.num_classes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.SemanticSamTrainer.compute_ce_loss", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "SemanticSamTrainer.compute_ce_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.SemanticSamTrainer.dice_weight", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "SemanticSamTrainer.dice_weight", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.semantic_sam_trainer.SemanticMapsSamTrainer", "modulename": "micro_sam.training.semantic_sam_trainer", "qualname": "SemanticMapsSamTrainer", "kind": "class", "doc": "<p>Trainer class for training the Segment Anything model for semantic segmentation.</p>\n\n<p>This class is derived from <code>torch_em.trainer.DefaultTrainer</code>.\nCheck out <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py</a>\nfor details on its usage and implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>convert_inputs:</strong>  The class that converts outputs of the dataloader to the expected input format of SAM.\nThe class <code>micro_sam.training.util.ConvertToSemanticSamInputs</code> can be used here.</li>\n<li><strong>num_classes:</strong>  The number of classes for semantic segmentation (including the background class).</li>\n<li><strong>dice_weight:</strong>  The weighing for the dice loss in the combined dice-cross entropy loss function.</li>\n<li><strong>kwargs:</strong>  The keyword arguments of the DefaultTrainer super class.</li>\n</ul>\n", "bases": "SemanticSamTrainer"}, {"fullname": "micro_sam.training.simple_sam_trainer", "modulename": "micro_sam.training.simple_sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.simple_sam_trainer.SimpleSamTrainer", "modulename": "micro_sam.training.simple_sam_trainer", "qualname": "SimpleSamTrainer", "kind": "class", "doc": "<p>Trainer class for creating a simple SAM trainer for limited prompt-based segmentation.</p>\n\n<p>This class is inherited from <code>SamTrainer</code>.\nCheck out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/training/sam_trainer.py\">https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/training/sam_trainer.py</a>\nfor details on its implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>use_points:</strong>  Whether to use point prompts for interactive segmentation.</li>\n<li><strong>use_box:</strong>  Whether to use box prompts for interactive segmentation.</li>\n<li><strong>kwargs:</strong>  The keyword arguments of the <code>SamTrainer</code> (and <code>DefaultTrainer</code>) class.</li>\n</ul>\n", "bases": "micro_sam.training.sam_trainer.SamTrainer"}, {"fullname": "micro_sam.training.simple_sam_trainer.SimpleSamTrainer.__init__", "modulename": "micro_sam.training.simple_sam_trainer", "qualname": "SimpleSamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">use_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.simple_sam_trainer.SimpleSamTrainer.use_points", "modulename": "micro_sam.training.simple_sam_trainer", "qualname": "SimpleSamTrainer.use_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.simple_sam_trainer.SimpleSamTrainer.use_box", "modulename": "micro_sam.training.simple_sam_trainer", "qualname": "SimpleSamTrainer.use_box", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.simple_sam_trainer.MedSAMTrainer", "modulename": "micro_sam.training.simple_sam_trainer", "qualname": "MedSAMTrainer", "kind": "class", "doc": "<p>Trainer class for replicating the trainer of MedSAM (https://arxiv.org/abs/2304.12306).</p>\n\n<p>This class is inherited from <code>SimpleSamTrainer</code>.\nCheck out\n<a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/training/simple_sam_trainer.py\">https://github.com/computational-cell-analytics/micro-sam/blob/master/micro_sam/training/simple_sam_trainer.py</a>\nfor details on its implementation.</p>\n", "bases": "SimpleSamTrainer"}, {"fullname": "micro_sam.training.simple_sam_trainer.MedSAMTrainer.__init__", "modulename": "micro_sam.training.simple_sam_trainer", "qualname": "MedSAMTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.trainable_sam", "modulename": "micro_sam.training.trainable_sam", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM", "kind": "class", "doc": "<p>Wrapper to make the SegmentAnything model trainable.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sam:</strong>  The SegmentAnything Model.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.__init__", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">sam</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">modeling</span><span class=\"o\">.</span><span class=\"n\">sam</span><span class=\"o\">.</span><span class=\"n\">Sam</span></span>)</span>"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.sam", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.transform", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.preprocess", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.preprocess", "kind": "function", "doc": "<p>Resize, normalize pixel values and pad to a square input.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  The input tensor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The resized, normalized and padded tensor.\n  The shape of the image after resizing.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.image_embeddings_oft", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.image_embeddings_oft", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batched_inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.forward", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.forward", "kind": "function", "doc": "<p>Forward pass.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batched_inputs:</strong>  The batched input images and prompts.</li>\n<li><strong>image_embeddings:</strong>  The precompute image embeddings. If not passed then they will be computed.</li>\n<li><strong>multimask_output:</strong>  Whether to predict mutiple or just a single mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predicted segmentation masks and iou values.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batched_inputs</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training", "modulename": "micro_sam.training.training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.training.FilePath", "modulename": "micro_sam.training.training", "qualname": "FilePath", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Union[str, os.PathLike]"}, {"fullname": "micro_sam.training.training.train_sam", "modulename": "micro_sam.training.training", "qualname": "train_sam", "kind": "function", "doc": "<p>Run training for a SAM model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>name:</strong>  The name of the model to be trained.\nThe checkpoint and logs wil have this name.</li>\n<li><strong>model_type:</strong>  The type of the SAM model.</li>\n<li><strong>train_loader:</strong>  The dataloader for training.</li>\n<li><strong>val_loader:</strong>  The dataloader for validation.</li>\n<li><strong>n_epochs:</strong>  The number of epochs to train for.</li>\n<li><strong>early_stopping:</strong>  Enable early stopping after this number of epochs\nwithout improvement.</li>\n<li><strong>n_objects_per_batch:</strong>  The number of objects per batch used to compute\nthe loss for interative segmentation. If None all objects will be used,\nif given objects will be randomly sub-sampled.</li>\n<li><strong>checkpoint_path:</strong>  Path to checkpoint for initializing the SAM model.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train additional UNETR decoder\nfor automatic instance segmentation.</li>\n<li><strong>freeze:</strong>  Specify parts of the model that should be frozen, namely:\nimage_encoder, prompt_encoder and mask_decoder\nBy default nothing is frozen and the full model is updated.</li>\n<li><strong>device:</strong>  The device to use for training.</li>\n<li><strong>lr:</strong>  The learning rate.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iterative prompts per training iteration.</li>\n<li><strong>save_root:</strong>  Optional root directory for saving the checkpoints and logs.\nIf not given the current working directory is used.</li>\n<li><strong>mask_prob:</strong>  The probability for using a mask as input in a given training sub-iteration.</li>\n<li><strong>n_iterations:</strong>  The number of iterations to use for training. This will over-ride n_epochs if given.</li>\n<li><strong>scheduler_class:</strong>  The learning rate scheduler to update the learning rate.\nBy default, torch.optim.lr_scheduler.ReduceLROnPlateau is used.</li>\n<li><strong>scheduler_kwargs:</strong>  The learning rate scheduler parameters.\nIf passed None, the chosen default parameters are used in ReduceLROnPlateau.</li>\n<li><strong>save_every_kth_epoch:</strong>  Save checkpoints after every kth epoch separately.</li>\n<li><strong>pbar_signals:</strong>  Controls for napari progress bar.</li>\n<li><strong>optimizer_class:</strong>  The optimizer class.\nBy default, torch.optim.AdamW is used.</li>\n<li><strong>peft_kwargs:</strong>  Keyword arguments for the PEFT wrapper class.</li>\n<li><strong>verify_n_labels_in_loader:</strong>  The number of labels to verify out of the train and validation dataloaders.\nBy default, 50 batches of labels are verified from the dataloaders.</li>\n<li><strong>model_kwargs:</strong>  Additional keyword arguments for the <code>util.get_sam_model</code>.</li>\n<li><strong>ignore_warnings:</strong>  Whether to ignore raised warnings.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">n_epochs</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">n_objects_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">freeze</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-05</span>,</span><span class=\"param\">\t<span class=\"n\">n_sub_iteration</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">save_root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">n_iterations</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\tscheduler_class: Optional[torch.optim.lr_scheduler._LRScheduler] = &lt;class &#x27;torch.optim.lr_scheduler.ReduceLROnPlateau&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">scheduler_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_every_kth_epoch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_signals</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">PyQt5</span><span class=\"o\">.</span><span class=\"n\">QtCore</span><span class=\"o\">.</span><span class=\"n\">QObject</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\toptimizer_class: Optional[torch.optim.optimizer.Optimizer] = &lt;class &#x27;torch.optim.adamw.AdamW&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">peft_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ignore_warnings</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">verify_n_labels_in_loader</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">50</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">model_kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.default_sam_dataset", "modulename": "micro_sam.training.training", "qualname": "default_sam_dataset", "kind": "function", "doc": "<p>Create a PyTorch Dataset for training a SAM model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>raw_paths:</strong>  The path(s) to the image data used for training.\nCan either be multiple 2D images or volumetric data.</li>\n<li><strong>raw_key:</strong>  The key for accessing the image data. Internal filepath for hdf5-like input\nor a glob pattern for selecting multiple files.</li>\n<li><strong>label_paths:</strong>  The path(s) to the label data used for training.\nCan either be multiple 2D images or volumetric data.</li>\n<li><strong>label_key:</strong>  The key for accessing the label data. Internal filepath for hdf5-like input\nor a glob pattern for selecting multiple files.</li>\n<li><strong>patch_shape:</strong>  The shape for training patches.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train with additional segmentation decoder.</li>\n<li><strong>with_channels:</strong>  Whether the image data has RGB channels.</li>\n<li><strong>sampler:</strong>  A sampler to reject batches according to a given criterion.</li>\n<li><strong>raw_transform:</strong>  Transformation applied to the image data.\nIf not given the data will be cast to 8bit.</li>\n<li><strong>n_samples:</strong>  The number of samples for this dataset.</li>\n<li><strong>is_train:</strong>  Whether this dataset is used for training or validation.</li>\n<li><strong>min_size:</strong>  Minimal object size. Smaller objects will be filtered.</li>\n<li><strong>max_sampling_attempts:</strong>  Number of sampling attempts to make from a dataset.</li>\n<li><strong>kwargs:</strong>  Additional keyword arguments for <code>torch_em.default_segmentation_dataset</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">raw_transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Callable</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>,</span><span class=\"param\">\t<span class=\"n\">max_sampling_attempts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.default_sam_loader", "modulename": "micro_sam.training.training", "qualname": "default_sam_loader", "kind": "function", "doc": "<p>Create a PyTorch DataLoader for training a SAM model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>kwargs:</strong>  Keyword arguments for <code>micro_sam.training.default_sam_dataset</code> or for the PyTorch DataLoader.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The DataLoader.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.CONFIGURATIONS", "modulename": "micro_sam.training.training", "qualname": "CONFIGURATIONS", "kind": "variable", "doc": "<p>Best training configurations for given hardware resources.</p>\n", "default_value": "{&#x27;Minimal&#x27;: {&#x27;model_type&#x27;: &#x27;vit_t&#x27;, &#x27;n_objects_per_batch&#x27;: 4, &#x27;n_sub_iteration&#x27;: 4}, &#x27;CPU&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;, &#x27;n_objects_per_batch&#x27;: 10}, &#x27;gtx1080&#x27;: {&#x27;model_type&#x27;: &#x27;vit_t&#x27;, &#x27;n_objects_per_batch&#x27;: 5}, &#x27;rtx5000&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;, &#x27;n_objects_per_batch&#x27;: 10}, &#x27;V100&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;}, &#x27;A100&#x27;: {&#x27;model_type&#x27;: &#x27;vit_h&#x27;}}"}, {"fullname": "micro_sam.training.training.train_sam_for_configuration", "modulename": "micro_sam.training.training", "qualname": "train_sam_for_configuration", "kind": "function", "doc": "<p>Run training for a SAM model with the configuration for a given hardware resource.</p>\n\n<p>Selects the best training settings for the given configuration.\nThe available configurations are listed in <code>CONFIGURATIONS</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>name:</strong>  The name of the model to be trained.\nThe checkpoint and logs wil have this name.</li>\n<li><strong>configuration:</strong>  The configuration (= name of hardware resource).</li>\n<li><strong>train_loader:</strong>  The dataloader for training.</li>\n<li><strong>val_loader:</strong>  The dataloader for validation.</li>\n<li><strong>checkpoint_path:</strong>  Path to checkpoint for initializing the SAM model.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train additional UNETR decoder\nfor automatic instance segmentation.</li>\n<li><strong>model_type:</strong>  Over-ride the default model type.\nThis can be used to use one of the micro_sam models as starting point\ninstead of a default sam model.</li>\n<li><strong>kwargs:</strong>  Additional keyword parameters that will be passed to <code>train_sam</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util", "modulename": "micro_sam.training.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.identity", "modulename": "micro_sam.training.util", "qualname": "identity", "kind": "function", "doc": "<p>Identity transformation.</p>\n\n<p>This is a helper function to skip data normalization when finetuning SAM.\nData normalization is performed within the model and should thus be skipped as\na preprocessing step in training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.require_8bit", "modulename": "micro_sam.training.util", "qualname": "require_8bit", "kind": "function", "doc": "<p>Transformation to require 8bit input data range (0-255).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.get_trainable_sam_model", "modulename": "micro_sam.training.util", "qualname": "get_trainable_sam_model", "kind": "function", "doc": "<p>Get the trainable sam model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The segment anything model that should be finetuned.\nThe weights of this model will be used for initialization, unless a\ncustom weight file is passed via <code>checkpoint_path</code>.</li>\n<li><strong>device:</strong>  The device to use for training.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the model weights.</li>\n<li><strong>freeze:</strong>  Specify parts of the model that should be frozen, namely: image_encoder, prompt_encoder and mask_decoder\nBy default nothing is frozen and the full model is updated.</li>\n<li><strong>return_state:</strong>  Whether to return the full checkpoint state.</li>\n<li><strong>peft_kwargs:</strong>  Keyword arguments for the PEFT wrapper class.</li>\n<li><strong>flexible_load_checkpoint:</strong>  Whether to adjust mismatching params while loading pretrained checkpoints.</li>\n<li><strong>model_kwargs:</strong>  Additional keyword arguments for the <code>util.get_sam_model</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The trainable segment anything model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">freeze</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">peft_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">flexible_load_checkpoint</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">model_kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">trainable_sam</span><span class=\"o\">.</span><span class=\"n\">TrainableSAM</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs", "kind": "class", "doc": "<p>Convert outputs of data loader to the expected batched inputs of the SegmentAnything model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>transform:</strong>  The transformation to resize the prompts. Should be the same transform used in the\nmodel to resize the inputs. If <code>None</code> the prompts will not be resized.</li>\n<li><strong>dilation_strength:</strong>  The dilation factor.\nIt determines a \"safety\" border from which prompts are not sampled to avoid ambiguous prompts\ndue to imprecise groundtruth masks.</li>\n<li><strong>box_distortion_factor:</strong>  Factor for distorting the box annotations derived from the groundtruth masks.</li>\n</ul>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.__init__", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ResizeLongestSide</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_strength</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">box_distortion_factor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.dilation_strength", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.dilation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.transform", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.box_distortion_factor", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.box_distortion_factor", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ConvertToSemanticSamInputs", "modulename": "micro_sam.training.util", "qualname": "ConvertToSemanticSamInputs", "kind": "class", "doc": "<p>Convert outputs of data loader to the expected batched inputs of the SegmentAnything model\nfor semantic segmentation.</p>\n"}, {"fullname": "micro_sam.training.util.normalize_to_8bit", "modulename": "micro_sam.training.util", "qualname": "normalize_to_8bit", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">raw</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.__init__", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">desired_shape</span>, </span><span class=\"param\"><span class=\"n\">do_rescaling</span><span class=\"o\">=</span><span class=\"kc\">False</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;constant&#39;</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.desired_shape", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.desired_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.padding", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.do_rescaling", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.do_rescaling", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.__init__", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">desired_shape</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;constant&#39;</span>, </span><span class=\"param\"><span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">0</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.desired_shape", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.desired_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.padding", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.min_size", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.min_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.util", "modulename": "micro_sam.util", "kind": "module", "doc": "<p>Helper functions for downloading Segment Anything models and predicting image embeddings.</p>\n"}, {"fullname": "micro_sam.util.get_cache_directory", "modulename": "micro_sam.util", "qualname": "get_cache_directory", "kind": "function", "doc": "<p>Get micro-sam cache directory location.</p>\n\n<p>Users can set the MICROSAM_CACHEDIR environment variable for a custom cache directory.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.microsam_cachedir", "modulename": "micro_sam.util", "qualname": "microsam_cachedir", "kind": "function", "doc": "<p>Return the micro-sam cache directory.</p>\n\n<p>Returns the top level cache directory for micro-sam models and sample data.</p>\n\n<p>Every time this function is called, we check for any user updates made to\nthe MICROSAM_CACHEDIR os environment variable since the last time.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.models", "modulename": "micro_sam.util", "qualname": "models", "kind": "function", "doc": "<p>Return the segmentation models registry.</p>\n\n<p>We recreate the model registry every time this function is called,\nso any user changes to the default micro-sam cache directory location\nare respected.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_device", "modulename": "micro_sam.util", "qualname": "get_device", "kind": "function", "doc": "<p>Get the torch device.</p>\n\n<p>If no device is passed the default device for your system is used.\nElse it will be checked if the device you have passed is supported.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>device:</strong>  The input device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The device.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_sam_model", "modulename": "micro_sam.util", "qualname": "get_sam_model", "kind": "function", "doc": "<p>Get the SegmentAnything Predictor.</p>\n\n<p>This function will download the required model or load it from the cached weight file.\nThis location of the cache can be changed by setting the environment variable: MICROSAM_CACHEDIR.\nThe name of the requested model can be set via <code>model_type</code>.\nSee <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>\nfor an overview of the available models</p>\n\n<p>Alternatively this function can also load a model from weights stored in a local filepath.\nThe corresponding file path is given via <code>checkpoint_path</code>. In this case <code>model_type</code>\nmust be given as the matching encoder architecture, e.g. \"vit_b\" if the weights are for\na SAM model with vit_b encoder.</p>\n\n<p>By default the models are downloaded to a folder named 'micro_sam/models'\ninside your default cache directory, eg:</p>\n\n<ul>\n<li>Mac: ~/Library/Caches/<AppName></li>\n<li>Unix: ~/.cache/<AppName> or the value of the XDG_CACHE_HOME environment variable, if defined.</li>\n<li>Windows: C:\\Users&lt;user>\\AppData\\Local&lt;AppAuthor>&lt;AppName>\\Cache\nSee the pooch.os_cache() documentation for more details:\n<a href=\"https://www.fatiando.org/pooch/latest/api/generated/pooch.os_cache.html\">https://www.fatiando.org/pooch/latest/api/generated/pooch.os_cache.html</a></li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The SegmentAnything model to use. Will use the standard vit_h model by default.\nTo get a list of all available model names you can call <code>get_model_names</code>.</li>\n<li><strong>device:</strong>  The device for the model. If none is given will use GPU if available.</li>\n<li><strong>checkpoint_path:</strong>  The path to a file with weights that should be used instead of using the\nweights corresponding to <code>model_type</code>. If given, <code>model_type</code> must match the architecture\ncorresponding to the weight file. E.g. if you use weights for SAM with vit_b encoder\nthen <code>model_type</code> must be given as \"vit_b\".</li>\n<li><strong>return_sam:</strong>  Return the sam model object as well as the predictor.</li>\n<li><strong>return_state:</strong>  Return the unpickled checkpoint state.</li>\n<li><strong>peft_kwargs:</strong>  Keyword arguments for th PEFT wrapper class.</li>\n<li><strong>flexible_load_checkpoint:</strong>  Whether to adjust mismatching params while loading pretrained checkpoints.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segment anything predictor.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_sam</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">peft_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">flexible_load_checkpoint</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">model_kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.export_custom_sam_model", "modulename": "micro_sam.util", "qualname": "export_custom_sam_model", "kind": "function", "doc": "<p>Export a finetuned segment anything model to the standard model format.</p>\n\n<p>The exported model can be used by the interactive annotation tools in <code>micro_sam.annotator</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint_path:</strong>  The path to the corresponding checkpoint if not in the default model folder.</li>\n<li><strong>model_type:</strong>  The SegmentAnything model type corresponding to the checkpoint (vit_h, vit_b, vit_l or vit_t).</li>\n<li><strong>save_path:</strong>  Where to save the exported model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_model_names", "modulename": "micro_sam.util", "qualname": "get_model_names", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">Iterable</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.precompute_image_embeddings", "modulename": "micro_sam.util", "qualname": "precompute_image_embeddings", "kind": "function", "doc": "<p>Compute the image embeddings (output of the encoder) for the input.</p>\n\n<p>If 'save_path' is given the embeddings will be loaded/saved in a zarr container.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>input_:</strong>  The input data. Can be 2 or 3 dimensional, corresponding to an image, volume or timeseries.</li>\n<li><strong>save_path:</strong>  Path to save the embeddings in a zarr container.</li>\n<li><strong>lazy_loading:</strong>  Whether to load all embeddings into memory or return an\nobject to load them on demand when required. This only has an effect if 'save_path' is given\nand if the input is 3 dimensional.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data. If not given will be deduced from the input data.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>verbose:</strong>  Whether to be verbose in the computation.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The image embeddings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">input_</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.set_precomputed", "modulename": "micro_sam.util", "qualname": "set_precomputed", "kind": "function", "doc": "<p>Set the precomputed image embeddings for a predictor.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_embeddings:</strong>  The precomputed image embeddings computed by <code>precompute_image_embeddings</code>.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_id:</strong>  Index for the tile. This is required if the embeddings are tiled.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predictor with set features.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.compute_iou", "modulename": "micro_sam.util", "qualname": "compute_iou", "kind": "function", "doc": "<p>Compute the intersection over union of two masks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask1:</strong>  The first mask.</li>\n<li><strong>mask2:</strong>  The second mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The intersection over union of the two masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mask1</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>, </span><span class=\"param\"><span class=\"n\">mask2</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_centers_and_bounding_boxes", "modulename": "micro_sam.util", "qualname": "get_centers_and_bounding_boxes", "kind": "function", "doc": "<p>Returns the center coordinates of the foreground instances in the ground-truth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The segmentation.</li>\n<li><strong>mode:</strong>  Determines the functionality used for computing the centers.</li>\n<li>If 'v', the object's eccentricity centers computed by vigra are used.</li>\n<li>If 'p' the object's centroids computed by skimage are used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dictionary that maps object ids to the corresponding centroid.\n  A dictionary that maps object_ids to the corresponding bounding box.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;v&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.load_image_data", "modulename": "micro_sam.util", "qualname": "load_image_data", "kind": "function", "doc": "<p>Helper function to load image data from file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The filepath to the image data.</li>\n<li><strong>key:</strong>  The internal filepath for complex data formats like hdf5.</li>\n<li><strong>lazy_loading:</strong>  Whether to lazyly load data. Only supported for n5 and zarr data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The image data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.segmentation_to_one_hot", "modulename": "micro_sam.util", "qualname": "segmentation_to_one_hot", "kind": "function", "doc": "<p>Convert the segmentation to one-hot encoded masks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The segmentation.</li>\n<li><strong>segmentation_ids:</strong>  Optional subset of ids that will be used to subsample the masks.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The one-hot encoded masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_block_shape", "modulename": "micro_sam.util", "qualname": "get_block_shape", "kind": "function", "doc": "<p>Get a suitable block shape for chunking a given shape.</p>\n\n<p>The primary use for this is determining chunk sizes for\nzarr arrays or block shapes for parallelization.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>shape:</strong>  The image or volume shape.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The block shape.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization", "modulename": "micro_sam.visualization", "kind": "module", "doc": "<p>Functionality for visualizing image embeddings.</p>\n"}, {"fullname": "micro_sam.visualization.compute_pca", "modulename": "micro_sam.visualization", "qualname": "compute_pca", "kind": "function", "doc": "<p>Compute the pca projection of the embeddings to visualize them as RGB image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings:</strong>  The embeddings. For example predicted by the SAM image encoder.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>PCA of the embeddings, mapped to the pixels.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization.project_embeddings_for_visualization", "modulename": "micro_sam.visualization", "qualname": "project_embeddings_for_visualization", "kind": "function", "doc": "<p>Project image embeddings to pixel-wise PCA.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PCA of the embeddings.\n  The scale factor for resizing to the original image size.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();