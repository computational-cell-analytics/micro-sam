window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "micro_sam", "modulename": "micro_sam", "kind": "module", "doc": "<h1 id=\"segment-anything-for-microscopy\">Segment Anything for Microscopy</h1>\n\n<p>Segment Anything for Microscopy implements automatic and interactive annotation for microscopy data. It is built on top of <a href=\"https://segment-anything.com/\">Segment Anything</a> by Meta AI and specializes it for microscopy and other bio-imaging data.\nIts core components are:</p>\n\n<ul>\n<li>The <code>micro_sam</code> tools for interactive data annotation, built as <a href=\"https://napari.org/stable/\">napari</a> plugin.</li>\n<li>The <code>micro_sam</code> library to apply Segment Anything to 2d and 3d data or fine-tune it on your data.</li>\n<li>The <code>micro_sam</code> models that are fine-tuned on publicly available microscopy data and that are available on <a href=\"https://bioimage.io/#/\">BioImage.IO</a>.</li>\n</ul>\n\n<p>Based on these components <code>micro_sam</code> enables fast interactive and automatic annotation for microscopy data, like interactive cell segmentation from bounding boxes:</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d04cb158-9f5b-4460-98cd-023c4f19cccd\" alt=\"box-prompts\" /></p>\n\n<p><code>micro_sam</code> is now available as stable version 1.0 and we will not change its user interface significantly in the foreseeable future.\nWe are still working on improving and extending its functionality. The current roadmap includes:</p>\n\n<ul>\n<li>Releasing more and better finetuned models.</li>\n<li>Integrating parameter efficient training and compressed models for faster fine-tuning.</li>\n<li>Improving the 3D segmentation and tracking functionality.</li>\n</ul>\n\n<p>If you run into any problems or have questions please <a href=\"\">open an issue</a> or reach out via <a href=\"https://forum.image.sc/\">image.sc</a> using the tag <code>micro-sam</code>.</p>\n\n<h2 id=\"quickstart\">Quickstart</h2>\n\n<p>You can install <code>micro_sam</code> via mamba:</p>\n\n<pre><code>$ mamba install -c conda-forge micro_sam\n</code></pre>\n\n<p>We also provide installers for Windows and Linux. For more details on the available installation options check out <a href=\"#installation\">the installation section</a>.</p>\n\n<p>After installing <code>micro_sam</code> you can start napari and select the annotation tool you want to use from <code>Plugins-&gt;Segment Anything for Microscopy</code>. Check out the <a href=\"TODO\">quickstart tutorial video</a> for a short introduction and  <a href=\"#annotation-tools\">the annotation tool section</a> for details.</p>\n\n<p>The <code>micro_sam</code> python library can be imported via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>It is explained in more detail <a href=\"#using-the-python-library\">here</a>.</p>\n\n<p>We provide different finetuned models for microscopy that can be used within our tools or any other tool that supports Segment Anything. See <a href=\"#finetuned-models\">finetuned models</a> for details on the available models.\nYou can also train models on your own data, see <a href=\"#training-your-own-model\">here for details</a>.</p>\n\n<h2 id=\"citation\">Citation</h2>\n\n<p>If you are using <code>micro_sam</code> in your research please cite</p>\n\n<ul>\n<li>Our <a href=\"https://doi.org/10.1101/2023.08.21.554208\">preprint</a></li>\n<li>and the original <a href=\"https://arxiv.org/abs/2304.02643\">Segment Anything publication</a>.</li>\n<li>If you use a <code>vit-tiny</code> models please also cite <a href=\"https://arxiv.org/abs/2306.14289\">Mobile SAM</a>.</li>\n</ul>\n\n<h1 id=\"installation\">Installation</h1>\n\n<p>There are three ways to install <code>micro_sam</code>:</p>\n\n<ul>\n<li><a href=\"#from-mamba\">From mamba</a> is the recommended way if you want to use all functionality.</li>\n<li><a href=\"#from-source\">From source</a> for setting up a development environment to use the development version and to change and contribute to our software.</li>\n<li><a href=\"#from-installer\">From installer</a> to install it without having to use mamba (supported platforms: Windows and Linux, only for CPU users). </li>\n</ul>\n\n<p>You can find more information on the installation and how to troubleshoot it in <a href=\"installation-questions\">the FAQ section</a>.</p>\n\n<h2 id=\"from-mamba\">From mamba</h2>\n\n<p><a href=\"https://mamba.readthedocs.io/en/latest/\">mamba</a> is a drop-in replacement for conda, but much faster.\nWhile the steps below may also work with <code>conda</code>, we highly recommend using <code>mamba</code>.\nYou can follow the instructions <a href=\"https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html\">here</a> to install <code>mamba</code>.</p>\n\n<p><strong>IMPORTANT</strong>: Make sure to avoid installing anything in the base environment.</p>\n\n<p><code>micro_sam</code> can be installed in an existing environment via:</p>\n\n<pre><code>$ mamba install -c conda-forge micro_sam\n</code></pre>\n\n<p>or you can create a new environment (here called <code>micro-sam</code>) with it via:</p>\n\n<pre><code>$ mamba create -c conda-forge -n micro-sam micro_sam\n</code></pre>\n\n<p>if you want to use the GPU you need to install PyTorch from the <code>pytorch</code> channel instead of <code>conda-forge</code>. For example:</p>\n\n<pre><code>$ mamba create -c pytorch -c nvidia -c conda-forge micro_sam pytorch pytorch-cuda=12.1\n</code></pre>\n\n<p>You may need to change this command to install the correct CUDA version for your system, see <a href=\"https://pytorch.org/\">https://pytorch.org/</a> for details.</p>\n\n<h2 id=\"from-source\">From source</h2>\n\n<p>To install <code>micro_sam</code> from source, we recommend to first set up an environment with the necessary requirements:</p>\n\n<ul>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_gpu.yaml\">environment_gpu.yaml</a>: sets up an environment with GPU support.</li>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_cpu.yaml\">environment_cpu.yaml</a>: sets up an environment with CPU support.</li>\n</ul>\n\n<p>To create one of these environments and install <code>micro_sam</code> into it follow these steps</p>\n\n<ol>\n<li>Clone the repository:</li>\n</ol>\n\n<pre><code>$ git clone https://github.com/computational-cell-analytics/micro-sam\n</code></pre>\n\n<ol start=\"2\">\n<li>Enter it:</li>\n</ol>\n\n<pre><code>$ cd micro-sam\n</code></pre>\n\n<ol start=\"3\">\n<li>Create the GPU or CPU environment:</li>\n</ol>\n\n<pre><code>$ mamba env create -f &lt;ENV_FILE&gt;.yaml\n</code></pre>\n\n<ol start=\"4\">\n<li>Activate the environment:</li>\n</ol>\n\n<pre><code>$ mamba activate sam\n</code></pre>\n\n<ol start=\"5\">\n<li>Install <code>micro_sam</code>:</li>\n</ol>\n\n<pre><code>$ pip install -e .\n</code></pre>\n\n<h2 id=\"from-installer\">From installer</h2>\n\n<p>We also provide installers for Linux and Windows:</p>\n\n<ul>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/nrNBuHr9ncJqid6\">Linux</a></li>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/kZmpAIBDmUSu4e9\">Windows</a>\n<!---</li>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/7YupGgACw9SHy2P\">Mac</a>\n--></li>\n</ul>\n\n<p>The installers will not enable you to use a GPU, so if you have one then please consider installing <code>micro_sam</code> via <a href=\"#from-mamba\">mamba</a> instead. They will also not enable using the python library.</p>\n\n<p><strong>Linux Installer:</strong></p>\n\n<p>To use the installer:</p>\n\n<ul>\n<li>Unpack the zip file you have downloaded.</li>\n<li>Make the installer executable: <code>$ chmod +x micro_sam-0.2.0post1-Linux-x86_64.sh</code></li>\n<li>Run the installer: <code>$./micro_sam-0.2.0post1-Linux-x86_64.sh$</code> \n<ul>\n<li>You can select where to install <code>micro_sam</code> during the installation. By default it will be installed in <code>$HOME/micro_sam</code>.</li>\n<li>The installer will unpack all <code>micro_sam</code> files to the installation directory.</li>\n</ul></li>\n<li>After the installation you can start the annotator with the command <code>.../micro_sam/bin/micro_sam.annotator</code>.\n<ul>\n<li>To make it easier to run the annotation tool you can add <code>.../micro_sam/bin</code> to your <code>PATH</code> or set a softlink to <code>.../micro_sam/bin/micro_sam.annotator</code>.</li>\n</ul></li>\n</ul>\n\n<p><strong>Windows Installer:</strong></p>\n\n<ul>\n<li>Unpack the zip file you have downloaded.</li>\n<li>Run the installer by double clicking on it.</li>\n<li>Choose installation type: <code>Just Me(recommended)</code> or <code>All Users(requires admin privileges)</code>.</li>\n<li>Choose installation path. By default it will be installed in <code>C:\\Users\\&lt;Username&gt;\\micro_sam</code> for <code>Just Me</code> installation or in <code>C:\\ProgramData\\micro_sam</code> for <code>All Users</code>.\n<ul>\n<li>The installer will unpack all micro_sam files to the installation directory.</li>\n</ul></li>\n<li>After the installation you can start the annotator by double clicking on <code>.\\micro_sam\\Scripts\\micro_sam.annotator.exe</code> or  with the command <code>.\\micro_sam\\Scripts\\micro_sam.annotator.exe</code> from the Command Prompt.</li>\n</ul>\n\n<p><!---\n<strong>Mac Installer:</strong></p>\n\n<p>To use the Mac installer you will need to enable installing unsigned applications. Please follow <a href=\"https://disable-gatekeeper.github.io/\">the instructions for 'Disabling Gatekeeper for one application only' here</a>.</p>\n\n<p>Alternative link on how to disable gatekeeper.\n<a href=\"https://www.makeuseof.com/how-to-disable-gatekeeper-mac/\">https://www.makeuseof.com/how-to-disable-gatekeeper-mac/</a></p>\n\n<p>TODO detailed instruction\n--></p>\n\n<h1 id=\"annotation-tools\">Annotation Tools</h1>\n\n<p><code>micro_sam</code> provides applications for fast interactive 2d segmentation, 3d segmentation and tracking.\nSee an example for interactive cell segmentation in phase-contrast microscopy (left), interactive segmentation\nof mitochondria in volume EM (middle) and interactive tracking of cells (right).</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d5ee2080-ab08-4716-b4c4-c169b4ed29f5\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/dfca3d9b-dba5-440b-b0f9-72a0683ac410\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/aefbf99f-e73a-4125-bb49-2e6592367a64\" width=\"256\"></p>\n\n<p>The annotation tools can be started from the napari plugin menu, the command line or from python scripts.\nThey are built as napari plugin and make use of existing napari functionality wherever possible. If you are not familiar with napari yet, <a href=\"https://napari.org/stable/tutorials/fundamentals/quick_start.html\">start here</a>.\nThe <code>micro_sam</code> tools mainly use <a href=\"https://napari.org/stable/howtos/layers/points.html\">the point layer</a>, <a href=\"https://napari.org/stable/howtos/layers/shapes.html\">shape layer</a> and <a href=\"https://napari.org/stable/howtos/layers/labels.html\">label layer</a>.</p>\n\n<p>The annotation tools are explained in detail below. We also provide <a href=\"TODO\">video tutorials</a>.</p>\n\n<p>The annotation tools can be started from the napari plugin menu:\n<img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/napari-plugin.png\" width=\"768\"></p>\n\n<p>You can find additional information on the annotation tools <a href=\"usage-question\">in the FAQ section</a>.</p>\n\n<h2 id=\"annotator-2d\">Annotator 2D</h2>\n\n<p>The 2d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Annotator 2d</code> in the plugin menu.</li>\n<li>running <code>$ micro_sam.annotator_2d</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_2d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py\">examples/annotator_2d.py</a> for details. </li>\n</ul>\n\n<p>The user interface of the 2d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/2d-annotator-menu.png\" width=\"1024\"></p>\n\n<p>It contains the following elements:</p>\n\n<ol>\n<li>The napari layers for the segmentations and prompts:\n<ul>\n<li><code>prompts</code>: shape layer that is used to provide box prompts to SegmentAnything. Annotations can be given as rectangle (box prompt in the image), ellipse or polygon.</li>\n<li><code>point_prompts</code>: point layer that is used to provide point prompts to SegmentAnything. Positive prompts (green points) for marking the object you want to segment, negative prompts (red points) for marking the outside of the object.</li>\n<li><code>committed_objects</code>: label layer with the objects that have already been segmented.</li>\n<li><code>auto_segmentation</code>: label layer with the results from automatic instance segmentation.</li>\n<li><code>current_object</code>: label layer for the object(s) you're currently segmenting.</li>\n</ul></li>\n<li>The embedding menu. For selecting the image to process, the Segment Anything model that is used and computing the image embeddings with the model. The <code>Embedding Settings</code> contain advanced settings for loading cached embeddings from file or using tiled embeddings.</li>\n<li>The prompt menu for changing whether the currently selected point is a positive or a negative prompt. This can also be done by pressing <code>T</code>.</li>\n<li>The menu for interactive segmentation. Clicking <code>Segment Object</code> (or pressing <code>S</code>) will run segmentation for the current prompts. The result is displayed in <code>current_object</code>. Activating <code>batched</code> enables segmentation of multiple objects with point prompts. In this case an object will be segmented per positive prompt.</li>\n<li>The menu for automatic segmentation. Clicking <code>Automatic Segmentation</code> will segment all objects n the image. The results will be displayed in the <code>auto_segmentation</code> layer. We support two different methods for automatic segmentation: automatic mask generation (supported for all models) and instance segmentation with an additional decoder (only supported for our models).\nChanging the parameters under <code>Automatic Segmentation Settings</code> controls the segmentation results, check the tooltips for details.</li>\n<li>The menu for commiting the segmentation. When clicking <code>Commit</code> (or pressing <code>C</code>) the result from the selected layer (either <code>current_object</code> or <code>auto_segmentation</code>) will be transferred from the respective layer to <code>committed_objects</code>.\nWhen <code>commit_path</code> is given the results will automatically be saved there.</li>\n<li>The menu for clearing the current annotations. Clicking <code>Clear Annotations</code> (or pressing <code>Shift + C</code>) will clear the current annotations and the current segmentation.</li>\n</ol>\n\n<p>Note that point prompts and box prompts can be combined. When you're using point prompts you can only segment one object at a time, unless the <code>batched</code> mode is activated. With box prompts you can segment several objects at once, both in the normal and <code>batched</code> mode.</p>\n\n<p>Check out <a href=\"TODO\">this video</a> for a tutorial for this tool.</p>\n\n<h2 id=\"annotator-3d\">Annotator 3D</h2>\n\n<p>The 3d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Annotator 3d</code> in the plugin menu.</li>\n<li>running <code>$ micro_sam.annotator_3d</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_3d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_3d.py\">examples/annotator_3d.py</a> for details.</li>\n</ul>\n\n<p>The user interface of the 3d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/3d-annotator-menu.png\" width=\"1024\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the segmentations and prompts.</li>\n<li>The embedding menu.</li>\n<li>The prompt menu.</li>\n<li>The menu for interactive segmentation.</li>\n<li>The menu for interactive 3d segmentation. Clicking <code>Segment All Slices</code> (or <code>Shift + S</code>) will extend the segmentation for the current object across the volume by projecting prompts across slices. The parameters for prompt projection can be set in <code>Segmentation Settings</code>, please refer to the tooltips for details.</li>\n<li>The menu for automatic segmentation. The overall functionality is the same as <a href=\"#annotator-2d\">for the 2d annotator</a>. To segment the full volume <code>Apply to Volume</code> needs to be checked, otherwise only the current slice will be segmented. Note that 3D segmentation can take quite long without a GPU.</li>\n<li>The menu for committing the current object.</li>\n<li>The menu for clearing the current annotations. If <code>all slices</code> is set all annotations will be cleared, otherwise they are only cleared for the current slice.</li>\n</ol>\n\n<p>Note that you can only segment one object at a time using the interactive segmentation functionality with this tool.</p>\n\n<p>Check out <a href=\"TODO\">this video</a> for a tutorial for the 3d annotation tool.</p>\n\n<h2 id=\"annotator-tracking\">Annotator Tracking</h2>\n\n<p>The tracking annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Annotator Tracking</code> in the plugin menu.</li>\n<li>running <code>$ micro_sam.annotator_tracking</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_tracking</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_tracking.py\">examples/annotator_tracking.py</a> for details. </li>\n</ul>\n\n<p>The user interface of the tracking annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/tracking-annotator-menu.png\" width=\"1024\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the segmentations and prompts. Same as for <a href=\"#annotator-2d\">the 2d segmentation app</a> but without the <code>auto_segmentation</code> layer.</li>\n<li>The embedding menu.</li>\n<li>The prompt menu.</li>\n<li>The menu with tracking settings: <code>track_state</code> is used to indicate that the object you are tracking is dividing in the current frame. <code>track_id</code> is used to select which of the tracks after division you are following.</li>\n<li>The menu for interactive segmentation.</li>\n<li>The menu for interactive tracking menu. Click <code>Track Object</code> (or press <code>Shift + S</code>) to segment the current object across time.</li>\n<li>The menu for committing the current tracking result.</li>\n<li>The menu for clearing the current annotations.</li>\n</ol>\n\n<p>Note that the tracking annotator only supports 2d image data, volumetric data is not supported. We also do not support automatic tracking yet.</p>\n\n<p>Check out <a href=\"TODO\">this video</a> for a tutorial for how to use the tracking annotation tool.</p>\n\n<h2 id=\"image-series-annotator\">Image Series Annotator</h2>\n\n<p>The image series annotation tool enables running the <a href=\"#annotator-2d\">2d annotator</a> or <a href=\"#annotator-3d\">2d annotator</a> for multiple images that are saved within an folder. This makes it convenient to annotate many images without having to close the tool. It can be started by</p>\n\n<ul>\n<li>clicking <code>Image Series Annotator</code> in the plugin menu.</li>\n<li>running <code>$ micro_sam.image_series_annotator</code> in the command line.</li>\n<li>calling <code>micro_sam.sam_annotator.image_series_annotator</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/image_series_annotator.py\">examples/image_series_annotator.py</a> for details. </li>\n</ul>\n\n<p>When starting this tool via the plugin menu the following interface opens:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/series-menu.png\" width=\"512\"></p>\n\n<p>You can select the folder where your image data is saved with <code>Input Folder</code>. The annotation results will be saved in <code>Output Folder</code>.\nYou can specify a rule for loading only a subset of images via <code>pattern</code>, for example <code>*.tif</code> to only load tif images. Set <code>is_volumetric</code> if the data you want to annotate is 3d. The rest of the options are settings for the image embedding computation and are the same as for the embedding menu (see above).\nOnce you click <code>Annotate Images</code> the images from the folder you have specified will be loaded and the annotation tool is started for them.</p>\n\n<p>This menu will not open if you start the image series annotator from the command line or via python. In this case the input folder and other settings are passed as parameters instead.</p>\n\n<p>Check out <a href=\"TODO\">this video</a> for a tutorial for how to use the image series annotator.</p>\n\n<h2 id=\"finetuning-ui\">Finetuning UI</h2>\n\n<p>We also provide a graphical tool for finetuning models on your own data. It can be started by clicking <code>Finetuning</code> in the plugin menu.</p>\n\n<p><strong>Note:</strong> if you know a bit of python programming we recommend to use a script for model finetuning instead. This will give you more options to configure the training. See <a href=\"training-your-own-model\">these instructions</a> for details.</p>\n\n<p>When starting this tool via the plugin menu the following interface opens:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/finetuning-menu.png\" width=\"512\"></p>\n\n<p>You can select the image data via <code>Path to images</code>. We can either load images from a folder or select a single file for training. By providing <code>Image data key</code> you can either provide a pattern for selecting files from a folder or provide an internal filepath for hdf5, zarr or similar fileformats.</p>\n\n<p>You can select the label data via <code>Path to labels</code> and <code>Label data key</code>, following the same logic as for the image data. We expect label masks stored in the same size as the image data for training. You can for example use annotations created with one of the <code>micro_sam</code> annotation tools for this, they are stored in the correct format!</p>\n\n<p>The <code>Configuration</code> option allows you to choose the hardware configuration for training. We try to automatically select the correct setting for your system, but it can also be changed. Please refer to the tooltips for the other parameters.</p>\n\n<h1 id=\"using-the-python-library\">Using the Python Library</h1>\n\n<p>The python library can be imported via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>This library extends the <a href=\"https://github.com/facebookresearch/segment-anything\">Segment Anything library</a> and</p>\n\n<ul>\n<li>implements functions to apply Segment Anything to 2d and 3d data in <code>micro_sam.prompt_based_segmentation</code>.</li>\n<li>provides improved automatic instance segmentation functionality in <code>micro_sam.instance_segmentation</code>.</li>\n<li>implements training functionality that can be used for finetuning on your own data in <code>micro_sam.training</code>.</li>\n<li>provides functionality for quantitative and qualitative evaluation of Segment Anything models in <code>micro_sam.evaluation</code>.</li>\n</ul>\n\n<p>You can import these sub-modules via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam.prompt_based_segmentation</span>\n<span class=\"kn\">import</span> <span class=\"nn\">micro_sam.instance_segmentation</span>\n<span class=\"c1\"># etc.</span>\n</code></pre>\n</div>\n\n<p>This functionality is used to implement the interactive annotation tools in <code>micro_sam.sam_annotator</code> and can be used as a standalone python library.\nWe provide jupyter notebooks that demonstrate how to use it <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/notebooks\">here</a>. You can find the full library documentation by scrolling to the end of this page. </p>\n\n<h2 id=\"training-your-own-model\">Training your own model</h2>\n\n<p>We reimplement the training logic described in the <a href=\"https://arxiv.org/abs/2304.02643\">Segment Anything publication</a> to enable finetuning on custom data.\nWe use this functionality to provide the <a href=\"#finetuned-models\">finetuned microscopy models</a> and it can also be used to train models on your own data.\nIn fact the best results can be expected when finetuning on your own data, and we found that it does not require much annotated training data to get siginficant improvements in model performance.\nSo a good strategy is to annotate a few images with one of the provided models using our interactive annotation tools and, if the model is not working as good as required for your use-case, finetune on the annotated data.\n<!--\nTODO: provide link to the paper with results on how much data is needed\n--></p>\n\n<p>The training logic is implemented in <code>micro_sam.training</code> and is based on <a href=\"https://github.com/constantinpape/torch-em\">torch-em</a>. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/micro-sam-finetuning.ipynb\">the finetuning notebook</a> to see how to use it.</p>\n\n<p>We also support training an additional decoder for automatic instance segmentation. This yields better results than the automatic mask generation of segment anything and is significantly faster.\nThe notebook explains how to activate training it together with the rest of SAM and how to then use it.</p>\n\n<p>More advanced examples, including quantitative and qualitative evaluation, of finetuned models can be found in <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/finetuning\">finetuning</a>, which contains the code for training and evaluating <a href=\"finetuned-models\">our models</a>. You can find further information on model training in the <a href=\"fine-tuning-questions\">FAQ section</a>.</p>\n\n<h1 id=\"finetuned-models\">Finetuned models</h1>\n\n<p>In addition to the original Segment Anything models, we provide models that are finetuned on microscopy data.\nThe additional models are available in the <a href=\"https://bioimage.io/#/\">bioimage.io modelzoo</a> and are also hosted on zenodo.</p>\n\n<p>We currently offer the following models:</p>\n\n<ul>\n<li><code>vit_h</code>: Default Segment Anything model with vit-h backbone.</li>\n<li><code>vit_l</code>: Default Segment Anything model with vit-l backbone.</li>\n<li><code>vit_b</code>: Default Segment Anything model with vit-b backbone.</li>\n<li><code>vit_t</code>: Segment Anything model with vit-tiny backbone. From the <a href=\"https://arxiv.org/abs/2306.14289\">Mobile SAM publication</a>. </li>\n<li><code>vit_l_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with vit-l backbone. (<a href=\"TODO\">zenodo</a>, <a href=\"TODO\">bioimage.io</a>)</li>\n<li><code>vit_b_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with vit-b backbone. (<a href=\"https://zenodo.org/doi/10.5281/zenodo.11103797\">zenodo</a>, <a href=\"TODO\">diplomatic-bug on bioimage.io</a>)</li>\n<li><code>vit_t_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with vit-t backbone. (<a href=\"TODO\">zenodo</a>, <a href=\"TODO\">bioimage.io</a>)</li>\n<li><code>vit_l_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with vit-l backbone. (<a href=\"TODO\">zenodo</a>, <a href=\"TODO\">bioimage.io</a>)</li>\n<li><code>vit_b_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with vit-b backbone. (<a href=\"TODO\">zenodo</a>, <a href=\"TODO\">bioimage.io</a>)</li>\n<li><code>vit_t_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with vit-t backbone. (<a href=\"TODO\">zenodo</a>, <a href=\"TODO\">bioimage.io</a>)</li>\n</ul>\n\n<p>See the two figures below of the improvements through the finetuned model for LM and EM data. </p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/lm_comparison.png\" width=\"768\"></p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/em_comparison.png\" width=\"768\"></p>\n\n<p>You can select which model to use for annotation by selecting the corresponding name in the embedding menu:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/model-type-selector.png\" width=\"256\"></p>\n\n<p>To use a specific model in the python library you need to pass the corresponding name as value to the <code>model_type</code> parameter exposed by all relevant functions.\nSee for example the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L62\">2d annotator example</a>.</p>\n\n<h2 id=\"choosing-a-model\">Choosing a Model</h2>\n\n<p>As a rule of thumb:</p>\n\n<ul>\n<li>Use the <code>vit_l_lm</code> or <code>vit_b_lm</code> model for segmenting cells or nuclei in light microscopy. The larger model (<code>vit_l_lm</code>) yields a bit better segmentation quality, especially for automatic segmentation, but needs more computational resources.</li>\n<li>Use the <code>vit_l_em_organelles</code> or <code>vit_b_em_organelles</code> models for segmenting mitochondria, nuclei or other  roundish organelles in electron microscopy.</li>\n<li>For other use-cases use one of the default models.</li>\n<li>The <code>vit_t_...</code> models run much faster than other models, but yield inferior quality for many applications. It can still make sense to try them for your use-case if your working on a laptop and want to annotate many images or volumetric data. </li>\n</ul>\n\n<p>See also the figures above for examples where the finetuned models work better than the default models.\nWe are working on further improving these models and adding new models for other biomedical imaging domains.</p>\n\n<h2 id=\"older-models\">Older Models</h2>\n\n<p>Previous versions of our models are available on zenodo:</p>\n\n<ul>\n<li><a href=\"https://zenodo.org/records/10524894\">vit_b_em_boundaries</a>: for segmenting compartments delineated by boundaries such as cells or neurites in EM.</li>\n<li><a href=\"https://zenodo.org/records/10524828\">vit_b_em_organelles</a>: for segmenting mitochondria, nuclei or other organelles in EM.</li>\n<li><a href=\"https://zenodo.org/records/10524791\">vit_b_lm</a>: for segmenting cells and nuclei in LM.</li>\n<li><a href=\"https://zenodo.org/records/8250291\">vit_h_em</a>: for general EM segmentation.</li>\n<li><a href=\"https://zenodo.org/records/8250299\">vit_h_lm</a>: for general LM segmentation.</li>\n</ul>\n\n<p>We do not recommend to use these models since our new models improve upon them significantly. But we provide the links here in case they are needed to reproduce older segmentation workflows.</p>\n\n<h1 id=\"faq\">FAQ</h1>\n\n<p>Here we provide frequently asked questions and common issues.\nIf you encounter a problem or question not addressed here feel free to <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues\">open an issue</a> or to ask your question on <a href=\"https://forum.image.sc/\">image.sc</a> with the tag <code>micro-sam</code>.</p>\n\n<h2 id=\"installation-questions\">Installation questions</h2>\n\n<h3 id=\"1-how-to-install-micro_sam\">1. How to install <code>micro_sam</code>?</h3>\n\n<p>The <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation\">installation</a> for <code>micro_sam</code> is supported in three ways: <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-mamba\">from mamba</a> (recommended), <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-source\">from source</a> and <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#from-installer\">from installers</a>. Check out our <a href=\"TODO\">tutorial video</a> to get started with <code>micro_sam</code>, briefly walking you through the installation process and how to start the tool.</p>\n\n<h3 id=\"2-i-cannot-install-micro_sam-using-the-installer-i-am-getting-some-errors\">2. I cannot install <code>micro_sam</code> using the installer, I am getting some errors.</h3>\n\n<p>The installer should work out-of-the-box on Windows and Linux platforms. Please open an issue to report the error you encounter.</p>\n\n<blockquote>\n  <p>NOTE: The installers enable using <code>micro_sam</code> without mamba or conda. However, we recommend the installation from mamba / from source to use all its features seamlessly. Specifically, the installers currently only support the CPU and won't enable you to use the GPU (if you have one). </p>\n</blockquote>\n\n<h3 id=\"3-what-is-the-minimum-system-requirement-for-micro_sam\">3. What is the minimum system requirement for <code>micro_sam</code>?</h3>\n\n<p>From our experience, the <code>micro_sam</code> annotation tools work seamlessly on most laptop or workstation CPUs and with &gt; 8GB RAM.\nYou might encounter some slowness for $leq$ 8GB RAM. The resources <code>micro_sam</code>'s annotation tools have been tested on are:</p>\n\n<ul>\n<li>Windows:\n<ul>\n<li>Windows 10 Pro, Intel i5 7th Gen, 8GB RAM</li>\n</ul></li>\n<li>Linux:\n<ul>\n<li>Ubuntu 22.04, Intel i7 12th Gen, 32GB RAM</li>\n</ul></li>\n<li>Mac:\n<ul>\n<li>macOS Sonoma 14.4.1\n<ul>\n<li>M1 Chip, 8GB RAM</li>\n<li>M3 Max Chip, 36GB RAM</li>\n</ul></li>\n</ul></li>\n</ul>\n\n<p>Having a GPU will significantly speed up the annotation tools and especially the model finetuning.</p>\n\n<h3 id=\"4-what-is-the-recommended-pytorch-version\">4. What is the recommended PyTorch version?</h3>\n\n<p><code>micro_sam</code> has been tested mostly with CUDA 12.1 and PyTorch [2.1.1, 2.2.0]. However, the tool and the library is not constrained to a specific PyTorch or CUDA version. So it should work fine with the standard PyTorch installation for your system.</p>\n\n<h3 id=\"5-i-am-missing-a-few-packages-eg-modulenotfounderror-no-module-named-elfio-what-should-i-do\">5. I am missing a few packages (eg. <code>ModuleNotFoundError: No module named 'elf.io</code>). What should I do?</h3>\n\n<p>With the latest release 1.0.0, the installation from mamba and source should take care of this and install all the relevant packages for you.\nSo please reinstall <code>micro_sam</code>.</p>\n\n<h3 id=\"6-can-i-install-micro_sam-using-pip\">6. Can I install <code>micro_sam</code> using pip?</h3>\n\n<p>The installation is not supported via pip.</p>\n\n<h3 id=\"7-i-get-the-following-error-importerror-cannot-import-name-unetr-from-torch_emmodel\">7. I get the following error: <code>importError: cannot import name 'UNETR' from 'torch_em.model'</code>.</h3>\n\n<p>It's possible that you have an older version of <code>torch-em</code> installed. Similar errors could often be raised from other libraries, the reasons being: a) Outdated packages installed, or b) Some non-existent module being called. If the source of such error is from <code>micro_sam</code>, then <code>a)</code> is most likely the reason . We recommend installing the latest version following the <a href=\"https://github.com/constantinpape/torch-em?tab=readme-ov-file#installation\">installation instructions</a>.</p>\n\n<h2 id=\"usage-questions\">Usage questions</h2>\n\n<p><!---\nTODO provide relevant links here.\n--></p>\n\n<h3 id=\"1-i-have-some-micropscopy-images-can-i-use-the-annotator-tool-for-segmenting-them\">1. I have some micropscopy images. Can I use the annotator tool for segmenting them?</h3>\n\n<p>Yes, you can use the annotator tool for:</p>\n\n<ul>\n<li>Segmenting objects in 2d images (using automatic and/or interactive segmentation).</li>\n<li>Segmenting objects in 3d volumes (using automatic and/or interactive segmentation for the entire object(s)).</li>\n<li>Tracking objects over time in time-series data.</li>\n<li>Segmenting objects in a series of 2d / 3d images.</li>\n<li>(OPTIONAL) You can finetune the Segment Anything / <code>micro_sam</code> models on your own microscopy data, in case the provided models do not suffice your needs. One caveat: You need to annotate a few objects before-hand (<code>micro_sam</code> has the potential of improving interactive segmentation with only a few annotated objects) to proceed with the supervised finetuning procedure.</li>\n</ul>\n\n<h3 id=\"2-which-model-should-i-use-for-my-data\">2. Which model should I use for my data?</h3>\n\n<p>We currently provide three different kind of models: the default models <code>vit_h</code>, <code>vit_l</code>, <code>vit_b</code> and <code>vit_t</code>; the models for light microscopy <code>vit_l_lm</code>, <code>vit_b_lm</code> and <code>vit_t_lm</code>; the models for electron microscopy <code>vit_l_em_organelles</code>, <code>vit_b_em_organelles</code> and <code>vit_t_em_organelles</code>.\nYou should first try the model that best fits the segmentation task your interested in, a <code>lm</code> model for cell or nucleus segmentation in light microscopy or a <code>em_organelles</code> model for segmenting nuclei, mitochondria or other roundish organelles in electron microscopy.\nIf your segmentation problem does not meet these descriptions, or if these models don't work well, you should try one of the default models instead.\nThe letter after <code>vit</code> denotes the size of the image encoder in SAM, <code>h</code> (huge) being the largest and <code>t</code> (tiny) the smallest. The smaller models are faster but may yield worse results. We recommend to either use a <code>vit_l</code> or <code>vit_b</code> model, they offer the best trade-off between speed and segmentation quality.\nYou can find more information on model choice <a href=\"#choosing-a-model\">here</a>.</p>\n\n<h3 id=\"3-i-have-high-resolution-microscopy-images-micro_sam-does-not-seem-to-work\">3. I have high-resolution microscopy images, 'micro_sam' does not seem to work.</h3>\n\n<p>The Segment Anything model expects inputs of shape 1024 x 1024 pixels. Inputs that do not match this size will be internally resized to match it. Hence, applying Segment Anything to a much larger image will often lead to inferior results, or somethimes not work at all. To address this, <code>micro_sam</code> implements tiling: cutting up the input image into tiles of a fixed size (with a fixed overlap) and running Segment Anything for the individual tiles. You can activate tiling with the <code>tile_shape</code> parameter, which determines the size of the inner tile and <code>halo</code>, which determines the size of the additional overlap.</p>\n\n<ul>\n<li>If you are using the <code>micro_sam</code> annotation tools, you can specify the values for the <code>tile_shape</code> and <code>halo</code> via the <code>tile_x</code>, <code>tile_y</code>, <code>halo_x</code> and <code>halo_y</code> parameters in the <code>Embedding Settings</code> drop-down menu.</li>\n<li>If you are using the <code>micro_sam</code> library in a python script, you can pass them as tuples, e.g. <code>tile_shape=(1024, 1024), halo=(256, 256)</code>. See also the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L47-L63\">wholeslide annotator example</a>.</li>\n<li>If you are using the command line functionality, you can pass them via the options <code>--tile_shape 1024 1024 --halo 256 256</code>.</li>\n</ul>\n\n<blockquote>\n  <p>NOTE: It's recommended to choose the <code>halo</code> so that it is larger than half of the maximal radius of the objects you want to segment.</p>\n</blockquote>\n\n<h3 id=\"4-the-computation-of-image-embeddings-takes-very-long-in-napari\">4. The computation of image embeddings takes very long in napari.</h3>\n\n<p><code>micro_sam</code> pre-computes the image embeddings produced by the vision transformer backbone in Segment Anything, and (optionally) store them on disc. I fyou are using a CPU, this step can take a while for 3d data or time-series (you will see a progress bar in the command-line interface / on the bootom right of napari). If you have access to a GPU without graphical interface (e.g. via a local computer cluster or a cloud provider), you can also pre-compute the embeddings there and then copy them over to your laptop / local machine to speed this up.</p>\n\n<ul>\n<li>You can use the command <code>micro_sam.precompute_embeddings</code> for this (it is installed with the rest of the software). You can specify the location of the precomputed embeddings via the <code>embedding_path</code> argument.</li>\n<li>You can cache the computed embedding in the napari tool (to avoid recomputing the embeddings again) by passing the path to store the embeddings in the <code>embeddings_save_path</code> option in the <code>Embedding Settings</code> drop-down. You can later load the precomputed image embeddings by entering the path to the stored embeddings there as well.</li>\n</ul>\n\n<h3 id=\"5-can-i-use-micro_sam-on-a-cpu\">5. Can I use <code>micro_sam</code> on a CPU?</h3>\n\n<p>Most other processing steps that are very fast even on a CPU, the automatic segmentation step for the default Segment Anything models (typically called as the \"Segment Anything\" feature or AMG - Automatic Mask Generation) takes several minutes without a GPU (depending on the image size). For large volumes and time-series, segmenting an object interactively in 3d / tracking across time can take a couple of seconds with a CPU (it is very fast with a GPU).</p>\n\n<blockquote>\n  <p>HINT: All the tutorial videos have been created on CPU resources.</p>\n</blockquote>\n\n<h3 id=\"6-i-generated-some-segmentations-from-another-tool-can-i-use-it-as-a-starting-point-in-micro_sam\">6. I generated some segmentations from another tool, can I use it as a starting point in <code>micro_sam</code>?</h3>\n\n<p>You can save and load the results from the <code>committed_objects</code> layer to correct segmentations you obtained from another tool (e.g. CellPose) or  save intermediate annotation results. The results can be saved via <code>File</code> -> <code>Save Selected Layers (s) ...</code> in the napari menu-bar on top (see the tutorial videos for details). They can be loaded again by specifying the corresponding location via the <code>segmentation_result</code> parameter in the CLI or python script (2d and 3d segmentation).\nIf you are using an annotation tool you can load the segmentation you want to edit as segmentation layer and renae it to <code>committed_objects</code>.</p>\n\n<h3 id=\"7-i-am-using-micro_sam-for-segmenting-objects-i-would-like-to-report-the-steps-for-reproducability-how-can-this-be-done\">7. I am using <code>micro_sam</code> for segmenting objects. I would like to report the steps for reproducability. How can this be done?</h3>\n\n<p>The annotation steps and segmentation results can be saved to a zarr file by providing the <code>commit_path</code> in the <code>commit</code> widget. This file will contain all relevant information to reproduce the segmentation.</p>\n\n<blockquote>\n  <p>NOTE: This feature is still under development and we have not implemented rerunning the segmentation from this file yet. See <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/408\">this issue</a> for details.</p>\n</blockquote>\n\n<h3 id=\"8-i-want-to-segment-complex-objects-both-the-default-segment-anything-models-and-the-micro_sam-generalist-models-do-not-work-for-my-data-what-should-i-do\">8. I want to segment complex objects. Both the default Segment Anything models and the <code>micro_sam</code> generalist models do not work for my data. What should I do?</h3>\n\n<p><code>micro_sam</code> supports interactive annotation using positive and negative point prompts, box prompts and polygon drawing. You can combine multiple types of prompts to improve the segmentation quality. In case the aforementioned suggestions do not work as desired, <code>micro_sam</code> also supports finetuning a model on your data (see the next section). We recommend the following: a) Check which of the provided models performs relatively good on your data, b) Choose the best model as the starting point to train your own specialist model for the desired segmentation task.</p>\n\n<h3 id=\"9-i-am-using-the-annotation-tool-and-napari-outputs-the-following-error-while-emmitting-signal-an-error-ocurred-in-callback-this-is-not-a-bug-in-psygnal-see-above-for-details\">9. I am using the annotation tool and napari outputs the following error: <code>While emmitting signal ... an error ocurred in callback ... This is not a bug in psygnal. See ... above for details.</code></h3>\n\n<p>These messages occur when an internal error happens in <code>micro_sam</code>. In most cases this is due to inconsistent annotations and you can fix them by clearing the annotations.\nWe want to remove these errors, so we would be very grateful if you can <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues\">open an issue</a> and describe the steps you did when encountering it.</p>\n\n<h3 id=\"10-the-objects-are-not-segmented-in-my-3d-data-using-the-interactive-annotation-tool\">10. The objects are not segmented in my 3d data using the interactive annotation tool.</h3>\n\n<p>The first thing to check is: a) make sure you are using the latest version of <code>micro_sam</code> (pull the latest commit from master if your installation is from source, or update the installation from conda / mamba using <code>mamba update micro_sam</code>), and b) try out the steps from the <a href=\"TODO\">3d annotator tutorial video</a> to verify if this shows the same behaviour (or the same errors) as you faced. For 3d images, it's important to pass the inputs in the python axis convention, ZYX.\nc) try using a different model and change the projection mode for 3d segmentation. This is also explained in the video.</p>\n\n<h3 id=\"11-i-have-very-small-or-fine-grained-structures-in-my-high-resolution-microscopic-images-can-i-use-micro_sam-to-annotate-them\">11. I have very small or fine-grained structures in my high-resolution microscopic images. Can I use <code>micro_sam</code> to annotate them?</h3>\n\n<p>Segment Anything does not work well for very small or fine-grained objects (e.g. filaments). In these cases, you could try to use tiling to improve results (see <a href=\"#3-i-have-high-resolution-large-tomograms-micro-sam-does-not-seem-to-work\">Point 3</a> above for details).</p>\n\n<h3 id=\"12-napari-seems-to-be-very-slow-for-large-images\">12. napari seems to be very slow for large images.</h3>\n\n<p>Editing (drawing / erasing) very large 2d images or 3d volumes is known to be slow at the moment, as the objects in the layers are stored in-memory. See the related <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/39\">issue</a>.</p>\n\n<h3 id=\"13-while-computing-the-embeddings-and-or-automatic-segmentation-a-window-stating-napari-is-not-responding-pops-up\">13. While computing the embeddings (and / or automatic segmentation), a window stating: <code>\"napari\" is not responding.</code> pops up.</h3>\n\n<p>This can happen for long running computations. You just need to wait a bit longer and the computation will finish.</p>\n\n<h2 id=\"fine-tuning-questions\">Fine-tuning questions</h2>\n\n<h3 id=\"1-i-have-a-microscopy-dataset-i-would-like-to-fine-tune-segment-anything-for-is-it-possible-using-micro_sam\">1. I have a microscopy dataset I would like to fine-tune Segment Anything for. Is it possible using 'micro_sam'?</h3>\n\n<p>Yes, you can fine-tune Segment Anything on your own dataset. Here's how you can do it:</p>\n\n<ul>\n<li>Check out the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/micro-sam-finetuning.ipynb\">tutorial notebook</a> on how to fine-tune Segment Anything with our <code>micro_sam.training</code> library.</li>\n<li>Or check the <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning\">examples</a> for additional scripts that demonstrate finetuning.</li>\n<li>If you are not familiar with coding in python at all then you can also use the <a href=\"finetuning-ui\">graphical interface for finetuning</a>. But we recommend using a script for more flexibility and reproducibility.</li>\n</ul>\n\n<h3 id=\"2-i-would-like-to-fine-tune-segment-anything-on-open-source-cloud-services-eg-kaggle-notebooks-is-it-possible\">2. I would like to fine-tune Segment Anything on open-source cloud services (e.g. Kaggle Notebooks), is it possible?</h3>\n\n<p>Yes, you can fine-tune Segment Anything on your custom datasets on Kaggle (and <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#using-micro_sam-on-band\">BAND</a>). Check out our <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/micro-sam-finetuning.ipynb\">tutorial notebook</a> for this.</p>\n\n<h3 id=\"3-what-kind-of-annotations-do-i-need-to-finetune-segment-anything\">3. What kind of annotations do I need to finetune Segment Anything?</h3>\n\n<p>TODO: explain instance segmentation labels, that you can get them by annotation with micro_sam, and dense vs. sparse annotation (for training without / with decoder)</p>\n\n<h3 id=\"4-i-have-finetuned-segment-anything-on-my-microscopy-data-how-can-i-use-it-for-annotating-new-images\">4. I have finetuned Segment Anything on my microscopy data. How can I use it for annotating new images?</h3>\n\n<p>You can load your finetuned model by entering the path to its checkpoint in the <code>custom_weights_path</code> field in the <code>Embedding Settings</code> drop-down menu.\nIf you are using the python library or CLI you can specify this path with the <code>checkpoint_path</code> parameter.</p>\n\n<h3 id=\"5-what-is-the-background-of-the-new-ais-automatic-instance-segmentation-feature-in-micro_sam\">5. What is the background of the new AIS (Automatic Instance Segmentation) feature in <code>micro_sam</code>?</h3>\n\n<p><code>micro_sam</code> introduces a new segmentation decoder to the Segment Anything backbone, for enabling faster and accurate automatic instance segmentation, by predicting the <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/transform/label.py#L284\">distances to the object center and boundary</a> as well as predicting foregrund, and performing <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/util/segmentation.py#L122\">seeded watershed-based postprocessing</a> to obtain the instances.\n<!--- This text is a bit confusing, not sure if we need to mention anything else here, leaving it for reference.\nHowever, it's a flexible wrap around the Segment Anything model, which provides the users to either fine-tune the Segment Anything model as it is, or the choice to fine-tune the Segment Anything model with an additional instance segmentation decoder (see the <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning#example-for-model-finetuning\">example</a> for finetuning with both the objectives). The finetuned models provided by <code>micro_sam</code> use the AIS feature for improving the segmentation experience for the light microscopy and electron microscopy domains.\n--></p>\n\n<h3 id=\"6-i-have-a-nvidia-rtx-4090ti-gpu-with-24gb-vram-can-i-finetune-segment-anything\">6. I have a NVIDIA RTX 4090Ti GPU with 24GB VRAM. Can I finetune Segment Anything?</h3>\n\n<p>Finetuning Segment Anything is possible in most consumer-grade GPU and CPU resources (but training being a lot slower on the CPU). For the mentioned resource, it should be possible to finetune a ViT Base (also abbreviated as <code>vit_b</code>) by reducing the number of objects per image to 15.\nThis parameter has the biggest impact on the VRAM consumption and quality of the finetuned model.\nYou can find an overview of the resources we have tested for finetuning <a href=\"TODO\">here</a>.\nWe also provide a the convenience function <code>micro_sam.training.train_sam_for_configuration</code> that selects the best training settings for these configuration. This function is also used by the finetuning UI.</p>\n\n<h3 id=\"7-i-want-to-create-a-dataloader-for-my-data-for-finetuning-segment-anything\">7. I want to create a dataloader for my data, for finetuning Segment Anything.</h3>\n\n<p>Thanks to <code>torch-em</code>, a) Creating PyTorch datasets and dataloaders using the python library is convenient and supported for various data formats and data structures.\nSee the <a href=\"https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\">tutorial notebook</a> on how to create dataloaders using <code>torch-em</code> and the <a href=\"https://github.com/constantinpape/torch-em/blob/main/doc/datasets_and_dataloaders.md\">documentation</a> for details on creating your own datasets and dataloaders; and b) finetuning using the <code>napari</code> tool eases the aforementioned process, by allowing you to add the input parameters (path to the directory for inputs and labels etc.) directly in the tool.</p>\n\n<blockquote>\n  <p>NOTE: If you have images with large input shapes with a sparse density of instance segmentations, we recommend using <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/data/sampler.py\"><code>sampler</code></a> for choosing the patches with valid segmentation for the finetuning purpose (see the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/finetuning/specialists/training/light_microscopy/plantseg_root_finetuning.py#L29\">example</a> for PlantSeg (Root) specialist model in <code>micro_sam</code>).</p>\n</blockquote>\n\n<h3 id=\"8-how-can-i-evaluate-a-model-i-have-finetuned\">8. How can I evaluate a model I have finetuned?</h3>\n\n<p>TODO: move the content of <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/doc/bioimageio/validation.md\">https://github.com/computational-cell-analytics/micro-sam/blob/master/doc/bioimageio/validation.md</a> here.</p>\n\n<h1 id=\"contribution-guide\">Contribution Guide</h1>\n\n<ul>\n<li><a href=\"#discuss-your-ideas\">Discuss your ideas</a></li>\n<li><a href=\"#clone-the-repository\">Clone the repository</a></li>\n<li><a href=\"#create-your-development-environment\">Create your development environment</a></li>\n<li><a href=\"#make-your-changes\">Make your changes</a></li>\n<li><a href=\"#testing\">Testing</a>\n<ul>\n<li><a href=\"#run-the-tests\">Run the tests</a></li>\n<li><a href=\"#writing-your-own-tests\">Writing your own tests</a></li>\n</ul></li>\n<li><a href=\"#open-a-pull-request\">Open a pull request</a></li>\n<li><a href=\"#optional-build-the-documentation\">Optional: Build the documentation</a></li>\n<li><a href=\"#optional-benchmark-performance\">Optional: Benchmark performance</a>\n<ul>\n<li><a href=\"#run-the-benchmark-script\">Run the benchmark script</a></li>\n<li><a href=\"#line-profiling\">Line profiling</a></li>\n<li><a href=\"#snakeviz-visualization\">Snakeviz visualization</a></li>\n<li><a href=\"#memory-profiling-with-memray\">Memory profiling with memray</a></li>\n</ul></li>\n</ul>\n\n<h2 id=\"discuss-your-ideas\">Discuss your ideas</h2>\n\n<p>We welcome new contributions!</p>\n\n<p>First, discuss your idea by opening a <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/new\">new issue</a> in micro-sam.</p>\n\n<p>This allows you to ask questions, and have the current developers make suggestions about the best way to implement your ideas.</p>\n\n<p>You may also find it helpful to look at this <a href=\"#for-developers\">developer guide</a>, which explains the organization of the micro-sam code.</p>\n\n<h2 id=\"clone-the-repository\">Clone the repository</h2>\n\n<p>We use <a href=\"https://git-scm.com/\">git</a> for version control.</p>\n\n<p>Clone the repository, and checkout the development branch:</p>\n\n<pre><code>git clone https://github.com/computational-cell-analytics/micro-sam.git\ncd micro-sam\ngit checkout dev\n</code></pre>\n\n<h2 id=\"create-your-development-environment\">Create your development environment</h2>\n\n<p>We use <a href=\"https://docs.conda.io/en/latest/\">conda</a> to <a href=\"https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\">manage our environments</a>. If you don't have this already, install <a href=\"https://docs.conda.io/projects/miniconda/en/latest/\">miniconda</a> or <a href=\"https://mamba.readthedocs.io/en/latest/\">mamba</a> to get started.</p>\n\n<p>Now you can create the environment, install user and develoepr dependencies, and micro-sam as an editable installation:</p>\n\n<pre><code>conda env create environment-gpu.yml\nconda activate sam\npython -m pip install requirements-dev.txt\npython -m pip install -e .\n</code></pre>\n\n<h2 id=\"make-your-changes\">Make your changes</h2>\n\n<p>Now it's time to make your code changes.</p>\n\n<p>Typically, changes are made branching off from the development branch. Checkout <code>dev</code> and then create a new branch to work on your changes.</p>\n\n<pre><code>git checkout dev\ngit checkout -b my-new-feature\n</code></pre>\n\n<p>We use <a href=\"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\">google style python docstrings</a> to create documentation for all new code.</p>\n\n<p>You may also find it helpful to look at this <a href=\"#for-developers\">developer guide</a>, which explains the organization of the micro-sam code.</p>\n\n<h2 id=\"testing\">Testing</h2>\n\n<h3 id=\"run-the-tests\">Run the tests</h3>\n\n<p>The tests for micro-sam are run with <a href=\"https://docs.pytest.org/en/7.4.x/\">pytest</a></p>\n\n<p>To run the tests:</p>\n\n<pre><code>pytest\n</code></pre>\n\n<h3 id=\"writing-your-own-tests\">Writing your own tests</h3>\n\n<p>If you have written new code, you will need to write tests to go with it.</p>\n\n<h4 id=\"unit-tests\">Unit tests</h4>\n\n<p>Unit tests are the preferred style of tests for user contributions. Unit tests check small, isolated parts of the code for correctness. If your code is too complicated to write unit tests easily, you may need to consider breaking it up into smaller functions that are easier to test.</p>\n\n<h4 id=\"tests-involving-napari\">Tests involving napari</h4>\n\n<p>In cases where tests <em>must</em> use the napari viewer, <a href=\"https://napari.org/stable/plugins/test_deploy.html#tips-for-testing-napari-plugins\">these tips might be helpful</a> (in particular, the <code>make_napari_viewer_proxy</code> fixture).</p>\n\n<p>These kinds of tests should be used only in limited circumstances. Developers are <a href=\"https://napari.org/stable/plugins/test_deploy.html#prefer-smaller-unit-tests-when-possible\">advised to prefer smaller unit tests, and avoid integration tests</a> wherever possible.</p>\n\n<h4 id=\"code-coverage\">Code coverage</h4>\n\n<p>Pytest uses the <a href=\"https://pytest-cov.readthedocs.io/en/latest/\">pytest-cov</a> plugin to automatically determine which lines of code are covered by tests.</p>\n\n<p>A short summary report is printed to the terminal output whenever you run pytest. The full results are also automatically written to a file named <code>coverage.xml</code>.</p>\n\n<p>The <a href=\"https://marketplace.visualstudio.com/items?itemName=ryanluker.vscode-coverage-gutters\">Coverage Gutters VSCode extension</a> is useful for visualizing which parts of the code need better test coverage. PyCharm professional <a href=\"https://www.jetbrains.com/pycharm/guide/tips/spot-coverage-in-gutter/\">has a similar feature</a>, and you may be able to find similar tools for your preferred editor.</p>\n\n<p>We also use <a href=\"https://app.codecov.io/gh/computational-cell-analytics/micro-sam\">codecov.io</a> to display the code coverage results from our Github Actions continuous integration.</p>\n\n<h2 id=\"open-a-pull-request\">Open a pull request</h2>\n\n<p>Once you've made changes to the code and written some tests to go with it, you are ready to <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests\">open a pull request</a>. You can <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-requests\">mark your pull request as a draft</a> if you are still working on it, and still get the benefit of discussing the best approach with maintainers.</p>\n\n<p>Remember that typically changes to micro-sam are made branching off from the development branch. So, you will need to open your pull request to merge back into the <code>dev</code> branch <a href=\"https://github.com/computational-cell-analytics/micro-sam/compare/dev...dev\">like this</a>.</p>\n\n<h2 id=\"optional-build-the-documentation\">Optional: Build the documentation</h2>\n\n<p>We use <a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> to build the documentation.</p>\n\n<p>To build the documentation locally, run this command:</p>\n\n<pre><code>python build_doc.py\n</code></pre>\n\n<p>This will start a local server and display the HTML documentation. Any changes you make to the documentation will be updated in real time (you may need to refresh your browser to see the changes).</p>\n\n<p>If you want to save the HTML files, append <code>--out</code> to the command, like this:</p>\n\n<pre><code>python build_doc.py --out\n</code></pre>\n\n<p>This will save the HTML files into a new directory named <code>tmp</code>.</p>\n\n<p>You can add content to the documentation in two ways:</p>\n\n<ol>\n<li>By adding or updating <a href=\"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\">google style python docstrings</a> in the micro-sam code.\n<ul>\n<li><a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> will automatically find and include docstrings in the documentation.</li>\n</ul></li>\n<li>By adding or editing markdown files in the micro-sam <code>doc</code> directory.\n<ul>\n<li>If you add a new markdown file to the documentation, you must tell <a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> that it exists by adding a line to the <code>micro_sam/__init__.py</code> module docstring (eg: <code>.. include:: ../doc/my_amazing_new_docs_page.md</code>). Otherwise it will not be included in the final documentation build!</li>\n</ul></li>\n</ol>\n\n<h2 id=\"optional-benchmark-performance\">Optional: Benchmark performance</h2>\n\n<p>There are a number of options you can use to benchmark performance, and identify problems like slow run times or high memory use in micro-sam.</p>\n\n<ul>\n<li><a href=\"#run-the-benchmark-script\">Run the benchmark script</a></li>\n<li><a href=\"#line-profiling\">Line profiling</a></li>\n<li><a href=\"#snakeviz-visualization\">Snakeviz visualization</a></li>\n<li><a href=\"#memory-profiling-with-memray\">Memory profiling with memray</a></li>\n</ul>\n\n<h3 id=\"run-the-benchmark-script\">Run the benchmark script</h3>\n\n<p>There is a performance benchmark script available in the micro-sam repository at <code>development/benchmark.py</code>.</p>\n\n<p>To run the benchmark script:</p>\n\n<pre><code>python development/benchmark.py --model_type vit_t --device cpu`\n</code></pre>\n\n<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>\n\n<pre><code>python development/benchmark.py --help\n</code></pre>\n\n<h3 id=\"line-profiling\">Line profiling</h3>\n\n<p>For more detailed line by line performance results, we can use <a href=\"https://github.com/pyutils/line_profiler\">line-profiler</a>.</p>\n\n<blockquote>\n  <p><a href=\"https://github.com/pyutils/line_profiler\">line_profiler</a> is a module for doing line-by-line profiling of functions. kernprof is a convenient script for running either line_profiler or the Python standard library's cProfile or profile modules, depending on what is available.</p>\n</blockquote>\n\n<p>To do line-by-line profiling:</p>\n\n<ol>\n<li>Ensure you have line profiler installed: <code>python -m pip install line_profiler</code></li>\n<li>Add <code>@profile</code> decorator to any function in the call stack</li>\n<li>Run <code>kernprof -lv benchmark.py --model_type vit_t --device cpu</code></li>\n</ol>\n\n<p>For more details about how to use line-profiler and kernprof, see <a href=\"https://kernprof.readthedocs.io/en/latest/\">the documentation</a>.</p>\n\n<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>\n\n<pre><code>python development/benchmark.py --help\n</code></pre>\n\n<h3 id=\"snakeviz-visualization\">Snakeviz visualization</h3>\n\n<p>For more detailed visualizations of profiling results, we use <a href=\"https://jiffyclub.github.io/snakeviz/\">snakeviz</a>.</p>\n\n<blockquote>\n  <p>SnakeViz is a browser based graphical viewer for the output of Python\u2019s cProfile module</p>\n</blockquote>\n\n<ol>\n<li>Ensure you have snakeviz installed: <code>python -m pip install snakeviz</code></li>\n<li>Generate profile file: <code>python -m cProfile -o program.prof benchmark.py --model_type vit_h --device cpu</code></li>\n<li>Visualize profile file: <code>snakeviz program.prof</code></li>\n</ol>\n\n<p>For more details about how to use snakeviz, see <a href=\"https://jiffyclub.github.io/snakeviz/\">the documentation</a>.</p>\n\n<h3 id=\"memory-profiling-with-memray\">Memory profiling with memray</h3>\n\n<p>If you need to investigate memory use specifically, we use <a href=\"https://github.com/bloomberg/memray\">memray</a>.</p>\n\n<blockquote>\n  <p>Memray is a memory profiler for Python. It can track memory allocations in Python code, in native extension modules, and in the Python interpreter itself. It can generate several different types of reports to help you analyze the captured memory usage data. While commonly used as a CLI tool, it can also be used as a library to perform more fine-grained profiling tasks.</p>\n</blockquote>\n\n<p>For more details about how to use memray, see <a href=\"https://bloomberg.github.io/memray/getting_started.html\">the documentation</a>.</p>\n\n<h1 id=\"using-micro_sam-on-band\">Using micro_sam on BAND</h1>\n\n<p>BAND is a service offered by EMBL Heidelberg that gives access to a virtual desktop for image analysis tasks. It is free to use and <code>micro_sam</code> is installed there.\nIn order to use BAND and start <code>micro_sam</code> on it follow these steps:</p>\n\n<h2 id=\"start-band\">Start BAND</h2>\n\n<ul>\n<li>Go to <a href=\"https://band.embl.de/\">https://band.embl.de/</a> and click <strong>Login</strong>. If you have not used BAND before you will need to register for BAND. Currently you can only sign up via a google account.</li>\n<li>Launch a BAND desktop with sufficient resources. It's particularly important to select a GPU. The settings from the image below are a good choice.</li>\n<li>Go to the desktop by clicking <strong>GO TO DESKTOP</strong> in the <strong>Running Desktops</strong> menu. See also the screenshot below.</li>\n</ul>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/f965fce2-b924-4fc8-871b-f3201e502138\" alt=\"image\" /></p>\n\n<h2 id=\"start-micro_sam-in-band\">Start micro_sam in BAND</h2>\n\n<ul>\n<li>Select <strong>Applications->Image Analysis->uSAM</strong> (see screenshot)\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5daeafb3-119b-4104-8708-aab2960cb21c\" alt=\"image\" /></li>\n<li>This will open the micro_sam menu, where you can select the tool you want to use (see screenshot). Note: this may take a few minutes.\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/900ce0b9-4cf8-418c-94f1-e99ac7bc0086\" alt=\"image\" /></li>\n<li>For testing if the tool works, it's best to use the <strong>2d annotator</strong> first.\n<ul>\n<li>You can find an example image to use here: <code>/scratch/cajal-connectomics/hela-2d-image.png</code>. Select it via <strong>Select image</strong>. (see screenshot)\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5fbd1c53-2ba1-47d4-ae50-dfab890ac9d3\" alt=\"image\" /></li>\n</ul></li>\n<li>Then press <strong>2d annotator</strong> and the tool will start.</li>\n</ul>\n\n<h2 id=\"transfering-data-to-band\">Transfering data to BAND</h2>\n\n<p>To copy data to and from BAND you can use any cloud storage, e.g. ownCloud, dropbox or google drive. For this, it's important to note that copy and paste, which you may need for accessing links on BAND, works a bit different in BAND:</p>\n\n<ul>\n<li>To copy text into BAND you first need to copy it on your computer (e.g. via selecting it + <code>ctrl + c</code>).</li>\n<li>Then go to the browser window with BAND and press <code>ctrl + shift + alt</code>. This will open a side window where you can paste your text via <code>ctrl + v</code>.</li>\n<li>Then select the text in this window and copy it via <code>ctrl + c</code>.</li>\n<li>Now you can close the side window via <code>ctrl + shift + alt</code> and paste the text in band via <code>ctrl + v</code></li>\n</ul>\n\n<p>The video below shows how to copy over a link from owncloud and then download the data on BAND using copy and paste:</p>\n\n<p><a href=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287\">https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287</a></p>\n"}, {"fullname": "micro_sam.bioimageio", "modulename": "micro_sam.bioimageio", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.model_export", "modulename": "micro_sam.bioimageio.model_export", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.model_export.DEFAULTS", "modulename": "micro_sam.bioimageio.model_export", "qualname": "DEFAULTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;authors&#x27;: [Author(affiliation=&#x27;University Goettingen&#x27;, email=None, orcid=None, name=&#x27;Anwai Archit&#x27;, github_user=&#x27;anwai98&#x27;), Author(affiliation=&#x27;University Goettingen&#x27;, email=None, orcid=None, name=&#x27;Constantin Pape&#x27;, github_user=&#x27;constantinpape&#x27;)], &#x27;description&#x27;: &#x27;Finetuned Segment Anything Model for Microscopy&#x27;, &#x27;cite&#x27;: [CiteEntry(text=&#x27;Archit et al. Segment Anything for Microscopy&#x27;, doi=&#x27;10.1101/2023.08.21.554208&#x27;, url=None)], &#x27;tags&#x27;: [&#x27;segment-anything&#x27;, &#x27;instance-segmentation&#x27;]}"}, {"fullname": "micro_sam.bioimageio.model_export.export_sam_model", "modulename": "micro_sam.bioimageio.model_export", "qualname": "export_sam_model", "kind": "function", "doc": "<p>Export SAM model to BioImage.IO model format.</p>\n\n<p>The exported model can be uploaded to <a href=\"https://bioimage.io/#/\">bioimage.io</a> and\nbe used in tools that support the BioImage.IO model format.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The image for generating test data.</li>\n<li><strong>label_image:</strong>  The segmentation correspoding to <code>image</code>.\nIt is used to derive prompt inputs for the model.</li>\n<li><strong>model_type:</strong>  The type of the SAM model.</li>\n<li><strong>name:</strong>  The name of the exported model.</li>\n<li><strong>output_path:</strong>  Where the exported model is saved.</li>\n<li><strong>checkpoint_path:</strong>  Optional checkpoint for loading the SAM model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">label_image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor", "modulename": "micro_sam.bioimageio.predictor_adaptor", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor", "kind": "class", "doc": "<p>Wrapper around the SamPredictor.</p>\n\n<p>This model supports the same functionality as SamPredictor and can provide mask segmentations\nfrom box, point or mask input prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The type of the model for the image encoder.\nCan be one of 'vit_b', 'vit_l', 'vit_h' or 'vit_t'.\nFor 'vit_t' support the 'mobile_sam' package has to be installed.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.__init__", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.sam", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.load_state_dict", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.load_state_dict", "kind": "function", "doc": "<p>Copies parameters and buffers from <code>state_dict</code> into\nthis module and its descendants. If <code>strict</code> is <code>True</code>, then\nthe keys of <code>state_dict</code> must exactly match the keys returned\nby this module's <code>~torch.nn.Module.state_dict()</code> function.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>If <code>assign</code> is <code>True</code> the optimizer must be created after\nthe call to <code>load_state_dict</code>.</p>\n\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state_dict (dict):</strong>  a dict containing parameters and\npersistent buffers.</li>\n<li><strong>strict (bool, optional):</strong>  whether to strictly enforce that the keys\nin <code>state_dict</code> match the keys returned by this module's\n<code>~torch.nn.Module.state_dict()</code> function. Default: <code>True</code></li>\n<li><strong>assign (bool, optional):</strong>  whether to assign items in the state\ndictionary to their corresponding keys in the module instead\nof copying them inplace into the module's current parameters and buffers.\nWhen <code>False</code>, the properties of the tensors in the current\nmodule are preserved while when <code>True</code>, the properties of the\nTensors in the state dict are preserved.\nDefault: <code>False</code></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p><code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:\n      * <strong>missing_keys</strong> is a list of str containing the missing keys\n      * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>\n</blockquote>\n\n<h6 id=\"note\">Note:</h6>\n\n<blockquote>\n  <p>If a parameter or buffer is registered as <code>None</code> and its corresponding key\n  exists in <code>state_dict</code>, <code>load_state_dict()</code> will raise a\n  <code>RuntimeError</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.forward", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  torch inputs of dimensions B x C x H x W</li>\n<li><strong>box_prompts:</strong>  box coordinates of dimensions B x OBJECTS x 4</li>\n<li><strong>point_prompts:</strong>  point coordinates of dimension B x OBJECTS x POINTS x 2</li>\n<li><strong>point_labels:</strong>  point labels of dimension B x OBJECTS x POINTS</li>\n<li><strong>mask_prompts:</strong>  mask prompts of dimension B x OBJECTS x 256 x 256</li>\n<li><strong>embeddings:</strong>  precomputed image embeddings B x 256 x 64 x 64</li>\n</ul>\n\n<p>Returns:</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">box_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation", "modulename": "micro_sam.evaluation", "kind": "module", "doc": "<p>Functionality for evaluating Segment Anything models on microscopy data.</p>\n"}, {"fullname": "micro_sam.evaluation.evaluation", "modulename": "micro_sam.evaluation.evaluation", "kind": "module", "doc": "<p>Evaluation functionality for segmentation predictions from <code>micro_sam.evaluation.automatic_mask_generation</code>\nand <code>micro_sam.evaluation.inference</code>.</p>\n"}, {"fullname": "micro_sam.evaluation.evaluation.run_evaluation", "modulename": "micro_sam.evaluation.evaluation", "qualname": "run_evaluation", "kind": "function", "doc": "<p>Run evaluation for instance segmentation predictions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The list of paths to ground-truth images.</li>\n<li><strong>prediction_paths:</strong>  The list of paths with the instance segmentations to evaluate.</li>\n<li><strong>save_path:</strong>  Optional path for saving the results.</li>\n<li><strong>verbose:</strong>  Whether to print the progress.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A DataFrame that contains the evaluation results.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.evaluation.run_evaluation_for_iterative_prompting", "modulename": "micro_sam.evaluation.evaluation", "qualname": "run_evaluation_for_iterative_prompting", "kind": "function", "doc": "<p>Run evaluation for iterative prompt-based segmentation predictions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The list of paths to ground-truth images.</li>\n<li><strong>prediction_root:</strong>  The folder with the iterative prompt-based instance segmentations to evaluate.</li>\n<li><strong>experiment_folder:</strong>  The folder where all the experiment results are stored.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to evaluate on experiments with iterative prompting starting with box.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A DataFrame that contains the evaluation results.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box_prompt</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">overwrite_results</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments", "modulename": "micro_sam.evaluation.experiments", "kind": "module", "doc": "<p>Predefined experiment settings for experiments with different prompt strategies.</p>\n"}, {"fullname": "micro_sam.evaluation.experiments.ExperimentSetting", "modulename": "micro_sam.evaluation.experiments", "qualname": "ExperimentSetting", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Dict"}, {"fullname": "micro_sam.evaluation.experiments.full_experiment_settings", "modulename": "micro_sam.evaluation.experiments", "qualname": "full_experiment_settings", "kind": "function", "doc": "<p>The full experiment settings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>use_boxes:</strong>  Whether to run the experiments with or without boxes.</li>\n<li><strong>positive_range:</strong>  The different number of positive points that will be used.\nBy defaul the values are set to [1, 2, 4, 8, 16].</li>\n<li><strong>negative_range:</strong>  The different number of negative points that will be used.\nBy defaul the values are set to [0, 1, 2, 4, 8, 16].</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of experiment settings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">positive_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">negative_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments.default_experiment_settings", "modulename": "micro_sam.evaluation.experiments", "qualname": "default_experiment_settings", "kind": "function", "doc": "<p>The three default experiment settings.</p>\n\n<p>For the default experiments we use a single positive prompt,\ntwo positive and four negative prompts and box prompts.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of experiment settings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments.get_experiment_setting_name", "modulename": "micro_sam.evaluation.experiments", "qualname": "get_experiment_setting_name", "kind": "function", "doc": "<p>Get the name for the given experiment setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>setting:</strong>  The experiment setting.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The name for this experiment setting.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">setting</span><span class=\"p\">:</span> <span class=\"n\">Dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference", "modulename": "micro_sam.evaluation.inference", "kind": "module", "doc": "<p>Inference with Segment Anything models and different prompt strategies.</p>\n"}, {"fullname": "micro_sam.evaluation.inference.precompute_all_embeddings", "modulename": "micro_sam.evaluation.inference", "qualname": "precompute_all_embeddings", "kind": "function", "doc": "<p>Precompute all image embeddings.</p>\n\n<p>To enable running different inference tasks in parallel afterwards.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the embeddings will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.precompute_all_prompts", "modulename": "micro_sam.evaluation.inference", "qualname": "precompute_all_prompts", "kind": "function", "doc": "<p>Precompute all point prompts.</p>\n\n<p>To enable running different inference tasks in parallel afterwards.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The file paths to the ground-truth segmentations.</li>\n<li><strong>prompt_save_dir:</strong>  The directory where the prompt files will be saved.</li>\n<li><strong>prompt_settings:</strong>  The settings for which the prompts will be computed.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_save_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_settings</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_inference_with_prompts", "modulename": "micro_sam.evaluation.inference", "qualname": "run_inference_with_prompts", "kind": "function", "doc": "<p>Run segment anything inference for multiple images using prompts derived from groundtruth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the image embddings will be saved or are already saved.</li>\n<li><strong>use_points:</strong>  Whether to use point prompts.</li>\n<li><strong>use_boxes:</strong>  Whether to use box prompts</li>\n<li><strong>n_positives:</strong>  The number of positive point prompts that will be sampled.</li>\n<li><strong>n_negativess:</strong>  The number of negative point prompts that will be sampled.</li>\n<li><strong>dilation:</strong>  The dilation factor for the radius around the ground-truth object\naround which points will not be sampled.</li>\n<li><strong>prompt_save_dir:</strong>  The directory where point prompts will be saved or are already saved.\nThis enables running multiple experiments in a reproducible manner.</li>\n<li><strong>batch_size:</strong>  The batch size used for batched prediction.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">n_positives</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_negatives</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_save_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">512</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_inference_with_iterative_prompting", "modulename": "micro_sam.evaluation.inference", "qualname": "run_inference_with_iterative_prompting", "kind": "function", "doc": "<p>Run segment anything inference for multiple images using prompts iteratively\n    derived from model outputs and groundtruth</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the image embeddings will be saved or are already saved.</li>\n<li><strong>prediction_dir:</strong>  The directory where the predictions from SegmentAnything will be saved per iteration.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to use the first prompt as bounding box or a single point</li>\n<li><strong>dilation:</strong>  The dilation factor for the radius around the ground-truth object\naround which points will not be sampled.</li>\n<li><strong>batch_size:</strong>  The batch size used for batched predictions.</li>\n<li><strong>n_iterations:</strong>  The number of iterations for iterative prompting.</li>\n<li><strong>use_masks:</strong>  Whether to make use of logits from previous prompt-based segmentation</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box_prompt</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">n_iterations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">use_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_amg", "modulename": "micro_sam.evaluation.inference", "qualname": "run_amg", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.inference", "qualname": "run_instance_segmentation_with_decoder", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation", "modulename": "micro_sam.evaluation.instance_segmentation", "kind": "module", "doc": "<p>Inference and evaluation for the automatic instance segmentation functionality.</p>\n"}, {"fullname": "micro_sam.evaluation.instance_segmentation.default_grid_search_values_amg", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "default_grid_search_values_amg", "kind": "function", "doc": "<p>Default grid-search parameter for AMG-based instance segmentation.</p>\n\n<p>Return grid search values for the two most important parameters:</p>\n\n<ul>\n<li><code>pred_iou_thresh</code>, the threshold for keeping objects according to the IoU predicted by the model.</li>\n<li><code>stability_score_thresh</code>, the theshold for keepong objects according to their stability.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>iou_thresh_values:</strong>  The values for <code>pred_iou_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>stability_score_values:</strong>  The values for <code>stability_score_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.default_grid_search_values_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "default_grid_search_values_instance_segmentation_with_decoder", "kind": "function", "doc": "<p>Default grid-search parameter for decoder-based instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>center_distance_threshold_values:</strong>  The values for <code>center_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>boundary_distance_threshold_values:</strong>  The values for <code>boundary_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>distance_smoothing_values:</strong>  The values for <code>distance_smoothing</code> used in the gridsearch.\nBy default values in the range from 1.0 to 2.0 with a stepsize of 0.1 will be used.</li>\n<li><strong>min_size_values:</strong>  The values for <code>min_size</code> used in the gridsearch.\nBy default the values 50, 100 and 200  are used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">center_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_grid_search", "kind": "function", "doc": "<p>Run grid search for automatic mask generation.</p>\n\n<p>The parameters and their respective value ranges for the grid search are specified via the\n'grid_search_values' argument. For example, to run a grid search over the parameters 'pred_iou_thresh'\nand 'stability_score_thresh', you can pass the following:</p>\n\n<pre><code>grid_search_values = {\n    \"pred_iou_thresh\": [0.6, 0.7, 0.8, 0.9],\n    \"stability_score_thresh\": [0.6, 0.7, 0.8, 0.9],\n}\n</code></pre>\n\n<p>All combinations of the parameters will be checked.</p>\n\n<p>You can use the functions <code>default_grid_search_values_instance_segmentation_with_decoder</code>\nor <code>default_grid_search_values_amg</code> to get the default grid search parameters for the two\nrespective instance segmentation methods.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>generate</code> function.</li>\n<li><strong>image_paths:</strong>  The input images for the grid search.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation for the grid search.</li>\n<li><strong>result_dir:</strong>  Folder to cache the evaluation results per image.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>fixed_generate_kwargs:</strong>  Fixed keyword arguments for the <code>generate</code> method of the segmenter.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>image_key:</strong>  Key for loading the image data from a more complex file format like HDF5.\nIf not given a simple image format like tif is assumed.</li>\n<li><strong>gt_key:</strong>  Key for loading the ground-truth data from a more complex file format like HDF5.\nIf not given a simple image format like tif is assumed.</li>\n<li><strong>rois:</strong>  Region of interests to resetrict the evaluation to.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">image_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gt_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">slice</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_inference", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_inference", "kind": "function", "doc": "<p>Run inference for automatic mask generation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>image_paths:</strong>  The input images.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>prediction_dir:</strong>  Folder to save the predictions.</li>\n<li><strong>generate_kwargs:</strong>  The keyword arguments for the <code>generate</code> method of the segmenter.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.evaluate_instance_segmentation_grid_search", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "evaluate_instance_segmentation_grid_search", "kind": "function", "doc": "<p>Evaluate gridsearch results.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>result_dir:</strong>  The folder with the gridsearch results.</li>\n<li><strong>grid_search_parameters:</strong>  The names for the gridsearch parameters.</li>\n<li><strong>criterion:</strong>  The metric to use for determining the best parameters.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The best parameter setting.\n  The evaluation score for the best setting.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_parameters</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">criterion</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mSA&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.save_grid_search_best_params", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "save_grid_search_best_params", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">best_kwargs</span>, </span><span class=\"param\"><span class=\"n\">best_msa</span>, </span><span class=\"param\"><span class=\"n\">grid_search_result_dir</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search_and_inference", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_grid_search_and_inference", "kind": "function", "doc": "<p>Run grid search and inference for automatic mask generation.</p>\n\n<p>Please refer to the documentation of <code>run_instance_segmentation_grid_search</code>\nfor details on how to specify the grid search parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>generate</code> function.</li>\n<li><strong>val_image_paths:</strong>  The input images for the grid search.</li>\n<li><strong>val_gt_paths:</strong>  The ground-truth segmentation for the grid search.</li>\n<li><strong>test_image_paths:</strong>  The input images for inference.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>prediction_dir:</strong>  Folder to save the predictions.</li>\n<li><strong>result_dir:</strong>  Folder to cache the evaluation results per image.</li>\n<li><strong>fixed_generate_kwargs:</strong>  Fixed keyword arguments for the <code>generate</code> method of the segmenter.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell", "modulename": "micro_sam.evaluation.livecell", "kind": "module", "doc": "<p>Inference and evaluation for the <a href=\"https://www.nature.com/articles/s41592-021-01249-6\">LIVECell dataset</a> and\nthe different cell lines contained in it.</p>\n"}, {"fullname": "micro_sam.evaluation.livecell.CELL_TYPES", "modulename": "micro_sam.evaluation.livecell", "qualname": "CELL_TYPES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;A172&#x27;, &#x27;BT474&#x27;, &#x27;BV2&#x27;, &#x27;Huh7&#x27;, &#x27;MCF7&#x27;, &#x27;SHSY5Y&#x27;, &#x27;SkBr3&#x27;, &#x27;SKOV3&#x27;]"}, {"fullname": "micro_sam.evaluation.livecell.livecell_inference", "modulename": "micro_sam.evaluation.livecell", "qualname": "livecell_inference", "kind": "function", "doc": "<p>Run inference for livecell with a fixed prompt setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segment anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>use_points:</strong>  Whether to use point prompts.</li>\n<li><strong>use_boxes:</strong>  Whether to use box prompts.</li>\n<li><strong>n_positives:</strong>  The number of positive point prompts.</li>\n<li><strong>n_negatives:</strong>  The number of negative point prompts.</li>\n<li><strong>prompt_folder:</strong>  The folder where the prompts should be saved.</li>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">n_positives</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_negatives</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_precompute_embeddings", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_precompute_embeddings", "kind": "function", "doc": "<p>Run precomputation of val and test image embeddings for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_iterative_prompting", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_iterative_prompting", "kind": "function", "doc": "<p>Run inference on livecell with iterative prompting setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segment anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to use the first prompt as bounding box or a single point.</li>\n<li><strong>use_masks:</strong>  Whether to make use of logits from previous prompt-based segmentation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_amg", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_amg", "kind": "function", "doc": "<p>Run automatic mask generation grid-search and inference for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>iou_thresh_values:</strong>  The values for <code>pred_iou_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>stability_score_values:</strong>  The values for <code>stability_score_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path where the predicted images are stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_instance_segmentation_with_decoder", "kind": "function", "doc": "<p>Run automatic mask generation grid-search and inference for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>center_distance_threshold_values:</strong>  The values for <code>center_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>boundary_distance_threshold_values:</strong>  The values for <code>boundary_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>distance_smoothing_values:</strong>  The values for <code>distance_smoothing</code> used in the gridsearch.\nBy default values in the range from 1.0 to 2.0 with a stepsize of 0.1 will be used.</li>\n<li><strong>min_size_values:</strong>  The values for <code>min_size</code> used in the gridsearch.\nBy default the values 50, 100 and 200  are used.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path where the predicted images are stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">center_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_inference", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_inference", "kind": "function", "doc": "<p>Run LIVECell inference with command line tool.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_evaluation", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_evaluation", "kind": "function", "doc": "<p>Run LiveCELL evaluation with command line tool.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "kind": "module", "doc": "<p>Functionality for qualitative comparison of Segment Anything models on microscopy data.</p>\n"}, {"fullname": "micro_sam.evaluation.model_comparison.generate_data_for_model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "generate_data_for_model_comparison", "kind": "function", "doc": "<p>Generate samples for qualitative model comparison.</p>\n\n<p>This precomputes the input for <code>model_comparison</code> and <code>model_comparison_with_napari</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loader:</strong>  The torch dataloader from which samples are drawn.</li>\n<li><strong>output_folder:</strong>  The folder where the samples will be saved.</li>\n<li><strong>model_type1:</strong>  The first model to use for comparison.\nThe value needs to be a valid model_type for <code>micro_sam.util.get_sam_model</code>.</li>\n<li><strong>model_type2:</strong>  The second model to use for comparison.\nThe value needs to be a valid model_type for <code>micro_sam.util.get_sam_model</code>.</li>\n<li><strong>n_samples:</strong>  The number of samples to draw from the dataloader.</li>\n<li><strong>checkpoint1:</strong>  Optional checkpoint for the first model.</li>\n<li><strong>checkpoint2:</strong>  Optional checkpoint for the second model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type1</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type2</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">model_type3</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint1</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint2</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint3</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison.model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "model_comparison", "kind": "function", "doc": "<p>Create images for a qualitative model comparision.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_folder:</strong>  The folder with the data precomputed by <code>generate_data_for_model_comparison</code>.</li>\n<li><strong>n_images_per_sample:</strong>  The number of images to generate per precomputed sample.</li>\n<li><strong>min_size:</strong>  The min size of ground-truth objects to take into account.</li>\n<li><strong>plot_folder:</strong>  The folder where to save the plots. If not given the plots will be displayed.</li>\n<li><strong>point_radius:</strong>  The radius of the point overlay.</li>\n<li><strong>outline_dilation:</strong>  The dilation factor of the outline overlay.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">n_images_per_sample</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">plot_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_radius</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">outline_dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">have_model3</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison.model_comparison_with_napari", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "model_comparison_with_napari", "kind": "function", "doc": "<p>Use napari to display the qualtiative comparison results for two models.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_folder:</strong>  The folder with the data precomputed by <code>generate_data_for_model_comparison</code>.</li>\n<li><strong>show_points:</strong>  Whether to show the results for point or for box prompts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">show_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.default_grid_search_values_multi_dimensional_segmentation", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "default_grid_search_values_multi_dimensional_segmentation", "kind": "function", "doc": "<p>Default grid-search parameters for multi-dimensional prompt-based instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>iou_threshold_values:</strong>  The values for <code>iou_threshold</code> used in the grid-search.\nBy default values in the range from 0.5 to 0.9 with a stepsize of 0.1 will be used.</li>\n<li><strong>projection_method_values:</strong>  The values for <code>projection</code> method used in the grid-search.\nBy default the values <code>mask</code>, <code>bounding_box</code> and <code>points</code> are used.</li>\n<li><strong>box_extension_values:</strong>  The values for <code>box_extension</code> used in the grid-search.\nBy default values in the range from 0 to 0.25 with a stepsize of 0.025 will be used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">iou_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">projection_method_values</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension_values</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.segment_slices_from_ground_truth", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "segment_slices_from_ground_truth", "kind": "function", "doc": "<p>Segment all objects in a volume by prompt-based segmentation in one slice per object.</p>\n\n<p>This function first segments each object in the respective specified slice using interactive\n(prompt-based) segmentation functionality. Then it segments the particular object in the\nremaining slices in the volume.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>ground_truth:</strong>  The label volume with instance segmentations.</li>\n<li><strong>model_type:</strong>  Choice of segment anything model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the model checkpoint.</li>\n<li><strong>embedding_path:</strong>  Path to cache the computed embeddings.</li>\n<li><strong>iou_threshold:</strong>  The criterion to decide whether to link the objects in the consecutive slice's segmentation.</li>\n<li><strong>projection:</strong>  The projection (prompting) method to generate prompts for consecutive slices.</li>\n<li><strong>box_extension:</strong>  Extension factor for increasing the box size after projection.</li>\n<li><strong>device:</strong>  The selected device for computation.</li>\n<li><strong>interactive_seg_mode:</strong>  Method for guiding prompt-based instance segmentation.</li>\n<li><strong>verbose:</strong>  Whether to get the trace for projected segmentations.</li>\n<li><strong>return_segmentation:</strong>  Whether to return the segmented volume.</li>\n<li><strong>min_size:</strong>  The minimal size for evaluating an object in the ground-truth.\nThe size is measured within the central slice.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">ground_truth</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.8</span>,</span><span class=\"param\">\t<span class=\"n\">projection</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mask&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mf\">0.025</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">interactive_seg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;box&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_segmentation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.run_multi_dimensional_segmentation_grid_search", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "run_multi_dimensional_segmentation_grid_search", "kind": "function", "doc": "<p>Run grid search for prompt-based multi-dimensional instance segmentation.</p>\n\n<p>The parameters and their respective value ranges for the grid search are specified via the\n<code>grid_search_values</code> argument. For example, to run a grid search over the parameters <code>iou_threshold</code>,\n<code>projection</code> and <code>box_extension</code>, you can pass the following:</p>\n\n<pre><code>grid_search_values = {\n    \"iou_threshold\": [0.5, 0.6, 0.7, 0.8, 0.9],\n    \"projection\": [\"mask\", \"bounding_box\", \"points\"],\n    \"box_extension\": [0, 0.1, 0.2, 0.3, 0.4, 0,5],\n}\n</code></pre>\n\n<p>All combinations of the parameters will be checked.\nIf passed None, the function <code>default_grid_search_values_multi_dimensional_segmentation</code> is used\nto get the default grid search parameters for the instance segmentation method.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>ground_truth:</strong>  The label volume with instance segmentations.</li>\n<li><strong>model_type:</strong>  Choice of segment anything model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the model checkpoint.</li>\n<li><strong>embedding_path:</strong>  Path to cache the computed embeddings.</li>\n<li><strong>result_path:</strong>  Path to save the grid search results.</li>\n<li><strong>interactive_seg_mode:</strong>  Method for guiding prompt-based instance segmentation.</li>\n<li><strong>verbose:</strong>  Whether to get the trace for projected segmentations.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>segment_slices_from_ground_truth</code> function.</li>\n<li><strong>min_size:</strong>  The minimal size for evaluating an object in the ground-truth.\nThe size is measured within the central slice.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">ground_truth</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">interactive_seg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;box&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.inference", "modulename": "micro_sam.inference", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.inference.batched_inference", "modulename": "micro_sam.inference", "qualname": "batched_inference", "kind": "function", "doc": "<p>Run batched inference for input prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>image:</strong>  The input image.</li>\n<li><strong>batch_size:</strong>  The batch size to use for inference.</li>\n<li><strong>boxes:</strong>  The box prompts. Array of shape N_PROMPTS x 4.\nThe bounding boxes are represented by [MIN_X, MIN_Y, MAX_X, MAX_Y].</li>\n<li><strong>points:</strong>  The point prompt coordinates. Array of shape N_PROMPTS x 1 x 2.\nThe points are represented by their coordinates [X, Y], which are given\nin the last dimension.</li>\n<li><strong>point_labels:</strong>  The point prompt labels. Array of shape N_PROMPTS x 1.\nThe labels are either 0 (negative prompt) or 1 (positive prompt).</li>\n<li><strong>multimasking:</strong>  Whether to predict with 3 or 1 mask.</li>\n<li><strong>embedding_path:</strong>  Cache path for the image embeddings.</li>\n<li><strong>return_instance_segmentation:</strong>  Whether to return a instance segmentation\nor the individual mask data.</li>\n<li><strong>segmentation_ids:</strong>  Fixed segmentation ids to assign to the masks\nderived from the prompts.</li>\n<li><strong>reduce_multimasking:</strong>  Whether to choose the most likely masks with\nhighest ious from multimasking</li>\n<li><strong>logits_masks:</strong>  The logits masks. Array of shape N_PROMPTS x 1 x 256 x 256.\nWhether to use the logits masks from previous segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predicted segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">boxes</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimasking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_instance_segmentation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reduce_multimasking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">logits_masks</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation", "modulename": "micro_sam.instance_segmentation", "kind": "module", "doc": "<p>Automated instance segmentation functionality.\nThe classes implemented here extend the automatic instance segmentation from Segment Anything:\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html</a></p>\n"}, {"fullname": "micro_sam.instance_segmentation.mask_data_to_segmentation", "modulename": "micro_sam.instance_segmentation", "qualname": "mask_data_to_segmentation", "kind": "function", "doc": "<p>Convert the output of the automatic mask generation to an instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>masks:</strong>  The outputs generated by AutomaticMaskGenerator or EmbeddingMaskGenerator.\nOnly supports output_mode=binary_mask.</li>\n<li><strong>with_background:</strong>  Whether the segmentation has background. If yes this function assures that the largest\nobject in the output will be mapped to zero (the background value).</li>\n<li><strong>min_object_size:</strong>  The minimal size of an object in pixels.</li>\n<li><strong>max_object_size:</strong>  The maximal size of an object in pixels.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">masks</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">min_object_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">max_object_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase", "kind": "class", "doc": "<p>Base class for the automatic mask generators.</p>\n", "bases": "abc.ABC"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.is_initialized", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.is_initialized", "kind": "variable", "doc": "<p>Whether the mask generator has already been initialized.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_list", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_list", "kind": "variable", "doc": "<p>The list of mask data after initialization.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_boxes", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_boxes", "kind": "variable", "doc": "<p>The list of crop boxes.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.original_size", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.original_size", "kind": "variable", "doc": "<p>The original image size.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.get_state", "kind": "function", "doc": "<p>Get the initialized state of the mask generator.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>State of the mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.set_state", "kind": "function", "doc": "<p>Set the state of the mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The state of the mask generator, e.g. from serialized state.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.clear_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.clear_state", "kind": "function", "doc": "<p>Clear the state of the mask generator.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>This class implements the same logic as\n<a href=\"https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py\">https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py</a>\nIt decouples the computationally expensive steps of generating masks from the cheap post-processing operation\nto filter these masks to enable grid search and interactively changing the post-processing.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">amg</span> <span class=\"o\">=</span> <span class=\"n\">AutomaticMaskGenerator</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">)</span>\n<span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>  <span class=\"c1\"># Initialize the masks, this takes care of all expensive computations.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">pred_iou_thresh</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the masks. This is fast and enables testing parameters</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>crop_n_layers:</strong>  If &gt;0, the mask prediction will be run again on crops of the image.</li>\n<li><strong>crop_overlap_ratio:</strong>  Sets the degree to which crops overlap.</li>\n<li><strong>crop_n_points_downscale_factor:</strong>  How the number of points is downsampled when predicting with crops.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n</ul>\n", "bases": "AMGBase"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_layers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">crop_overlap_ratio</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.3413333333333333</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_points_downscale_factor</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred_iou_thresh:</strong>  Filter threshold in [0, 1], using the mask quality predicted by the model.</li>\n<li><strong>stability_score_thresh:</strong>  Filter threshold in [0, 1], using the stability of the mask\nunder changes to the cutoff used to binarize the model prediction.</li>\n<li><strong>box_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks.</li>\n<li><strong>crop_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks between crops.</li>\n<li><strong>min_mask_region_area:</strong>  Minimal size for the predicted masks.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">pred_iou_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.88</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.95</span>,</span><span class=\"param\">\t<span class=\"n\">box_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">crop_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">min_mask_region_area</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>Implements the same functionality as <code>AutomaticMaskGenerator</code> but for tiled embeddings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n</ul>\n", "bases": "AutomaticMaskGenerator"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_shape:</strong>  The tile shape for embedding prediction.</li>\n<li><strong>halo:</strong>  The overlap of between tiles.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter", "kind": "class", "doc": "<p>Adapter to contain the UNETR decoder in a single module.</p>\n\n<p>To apply the decoder on top of pre-computed embeddings for\nthe segmentation functionality.\nSee also: <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unetr.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unetr.py</a></p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unetr</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.base", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.base", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.out_conv", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.out_conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv_out", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv_out", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.decoder_head", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.decoder_head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.final_activation", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.final_activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.postprocess_masks", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.postprocess_masks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.decoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv1", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv2", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv3", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv4", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv4", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.forward", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">input_shape</span>, </span><span class=\"param\"><span class=\"n\">original_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_unetr", "modulename": "micro_sam.instance_segmentation", "qualname": "get_unetr", "kind": "function", "doc": "<p>Get UNETR model for automatic instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_encoder:</strong>  The image encoder of the SAM model.\nThis is used as encoder by the UNETR too.</li>\n<li><strong>decoder_state:</strong>  Optional decoder state to initialize the weights\nof the UNETR decoder.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The UNETR model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_encoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">decoder_state</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "get_decoder", "kind": "function", "doc": "<p>Get decoder to predict outputs for automatic instance segmentation</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_encoder:</strong>  The image encoder of the SAM model.</li>\n<li><strong>decoder_state:</strong>  State to initialize the weights of the UNETR decoder.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The decoder for instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_encoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">decoder_state</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">DecoderAdapter</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_predictor_and_decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "get_predictor_and_decoder", "kind": "function", "doc": "<p>Load the SAM model (predictor) and instance segmentation decoder.</p>\n\n<p>This requires a checkpoint that contains the state for both predictor\nand decoder.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The type of the image encoder used in the SAM model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the checkpoint from which to load the data.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The SAM predictor.\n  The decoder for instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">DecoderAdapter</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a decoder.</p>\n\n<p>Implements the same interface as <code>AutomaticMaskGenerator</code>.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">segmenter</span> <span class=\"o\">=</span> <span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">,</span> <span class=\"n\">decoder</span><span class=\"p\">)</span>\n<span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>   <span class=\"c1\"># Predict the image embeddings and decoder outputs.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">center_distance_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the instance segmentation.</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>decoder:</strong>  The decoder to predict intermediate representations\nfor instance segmentation.</li>\n</ul>\n"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.is_initialized", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.is_initialized", "kind": "variable", "doc": "<p>Whether the mask generator has already been initialized.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to be verbose.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>center_distance_threshold:</strong>  Center distance predictions below this value will be\nused to find seeds (intersected with thresholded boundary distance predictions).</li>\n<li><strong>boundary_distance_threshold:</strong>  Boundary distance predictions below this value will be\nused to find seeds (intersected with thresholded center distance predictions).</li>\n<li><strong>foreground_smoothing:</strong>  Sigma value for smoothing the foreground predictions, to avoid\ncheckerboard artifacts in the prediction.</li>\n<li><strong>foreground_threshold:</strong>  Foreground predictions above this value will be used as foreground mask.</li>\n<li><strong>distance_smoothing:</strong>  Sigma value for smoothing the distance predictions.</li>\n<li><strong>min_size:</strong>  Minimal object size in the segmentation result.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in. Pass None to directly return the instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">center_distance_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_smoothing</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.6</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.get_state", "kind": "function", "doc": "<p>Get the initialized state of the instance segmenter.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Instance segmentation state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.set_state", "kind": "function", "doc": "<p>Set the state of the instance segmenter.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The instance segmentation state</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.clear_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.clear_state", "kind": "function", "doc": "<p>Clear the state of the instance segmenter.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledInstanceSegmentationWithDecoder", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledInstanceSegmentationWithDecoder", "kind": "class", "doc": "<p>Same as <code>InstanceSegmentationWithDecoder</code> but for tiled image embeddings.</p>\n", "bases": "InstanceSegmentationWithDecoder"}, {"fullname": "micro_sam.instance_segmentation.TiledInstanceSegmentationWithDecoder.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledInstanceSegmentationWithDecoder.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Dummy input to be compatible with other function signatures.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_amg", "modulename": "micro_sam.instance_segmentation", "qualname": "get_amg", "kind": "function", "doc": "<p>Get the automatic mask generator class.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>is_tiled:</strong>  Whether tiled embeddings are used.</li>\n<li><strong>decoder:</strong>  Decoder to predict instacne segmmentation.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The automatic mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">is_tiled</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation", "modulename": "micro_sam.multi_dimensional_segmentation", "kind": "module", "doc": "<p>Multi-dimensional segmentation with segment anything.</p>\n"}, {"fullname": "micro_sam.multi_dimensional_segmentation.PROJECTION_MODES", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "PROJECTION_MODES", "kind": "variable", "doc": "<p></p>\n", "default_value": "(&#x27;box&#x27;, &#x27;mask&#x27;, &#x27;points&#x27;, &#x27;points_and_mask&#x27;, &#x27;single_point&#x27;)"}, {"fullname": "micro_sam.multi_dimensional_segmentation.segment_mask_in_volume", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "segment_mask_in_volume", "kind": "function", "doc": "<p>Segment an object mask in in volumetric data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The initial segmentation for the object.</li>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>image_embeddings:</strong>  The precomputed image embeddings for the volume.</li>\n<li><strong>segmented_slices:</strong>  List of slices for which this object has already been segmented.</li>\n<li><strong>stop_lower:</strong>  Whether to stop at the lowest segmented slice.</li>\n<li><strong>stop_upper:</strong>  Wheter to stop at the topmost segmented slice.</li>\n<li><strong>iou_threshold:</strong>  The IOU threshold for continuing segmentation across 3d.</li>\n<li><strong>projection:</strong>  The projection method to use. One of 'box', 'mask', 'points', 'points_and_mask' or 'single point'.\nPass a dictionary to choose the excact combination of projection modes.</li>\n<li><strong>update_progress:</strong>  Callback to update an external progress bar.</li>\n<li><strong>box_extension:</strong>  Extension factor for increasing the box size after projection.</li>\n<li><strong>verbose:</strong>  Whether to print details about the segmentation steps.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Array with the volumetric segmentation.\n  Tuple with the first and last segmented slice.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">segmented_slices</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">stop_lower</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">stop_upper</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">iou_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">projection</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">update_progress</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation.merge_instance_segmentation_3d", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "merge_instance_segmentation_3d", "kind": "function", "doc": "<p>Merge stacked 2d instance segmentations into a consistent 3d segmentation.</p>\n\n<p>Solves a multicut problem based on the overlap of objects to merge across z.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>slice_segmentation:</strong>  The stacked segmentation across the slices.\nWe assume that the segmentation is labeled consecutive across z.</li>\n<li><strong>beta:</strong>  The bias term for the multicut. Higher values lead to a larger\ndegree of over-segmentation and vice versa.</li>\n<li><strong>with_background:</strong>  Whether this is a segmentation problem with background.\nIn that case all edges connecting to the background are set to be repulsive.</li>\n<li><strong>gap_closing:</strong>  If given, gaps in the segmentation are closed with a binary closing\noperation. The value is used to determine the number of iterations for the closing.</li>\n<li><strong>min_z_extent:</strong>  Require a minimal extent in z for the segmented objects.\nThis can help to prevent segmentation artifacts.</li>\n<li><strong>verbose:</strong>  Verbosity flag.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The merged segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">slice_segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gap_closing</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_z_extent</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation.automatic_3d_segmentation", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "automatic_3d_segmentation", "kind": "function", "doc": "<p>Segment volume in 3d.</p>\n\n<p>First segments slices individually in 2d and then merges them across 3d\nbased on overlap of objects between slices.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>predictor:</strong>  The SAM model.</li>\n<li><strong>segmentor:</strong>  The instance segmentation class.</li>\n<li><strong>embedding_path:</strong>  The path to save pre-computed embeddings.</li>\n<li><strong>with_background:</strong>  Whether the segmentation has background.</li>\n<li><strong>gap_closing:</strong>  If given, gaps in the segmentation are closed with a binary closing\noperation. The value is used to determine the number of iterations for the closing.</li>\n<li><strong>min_z_extent:</strong>  Require a minimal extent in z for the segmented objects.\nThis can help to prevent segmentation artifacts.</li>\n<li><strong>verbose:</strong>  Verbosity flag.</li>\n<li><strong>kwargs:</strong>  Keyword arguments for the 'generate' method of the 'segmentor'.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">segmentor</span><span class=\"p\">:</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gap_closing</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_z_extent</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state", "modulename": "micro_sam.precompute_state", "kind": "module", "doc": "<p>Precompute image embeddings and automatic mask generator state for image data.</p>\n"}, {"fullname": "micro_sam.precompute_state.cache_amg_state", "modulename": "micro_sam.precompute_state", "qualname": "cache_amg_state", "kind": "function", "doc": "<p>Compute and cache or load the state for the automatic mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n<li><strong>save_path:</strong>  The embedding save path. The AMG state will be stored in 'save_path/amg_state.pickle'.</li>\n<li><strong>verbose:</strong>  Whether to run the computation verbose.</li>\n<li><strong>i:</strong>  The index for which to cache the state.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The automatic mask generator class with the cached state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">raw</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state.cache_is_state", "modulename": "micro_sam.precompute_state", "qualname": "cache_is_state", "kind": "function", "doc": "<p>Compute and cache or load the state for the automatic mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>decoder:</strong>  The instance segmentation decoder.</li>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n<li><strong>save_path:</strong>  The embedding save path. The AMG state will be stored in 'save_path/amg_state.pickle'.</li>\n<li><strong>verbose:</strong>  Whether to run the computation verbose.</li>\n<li><strong>i:</strong>  The index for which to cache the state.</li>\n<li><strong>skip_load:</strong>  Skip loading the state if it is precomputed.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation class with the cached state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">raw</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">skip_load</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state.precompute_state", "modulename": "micro_sam.precompute_state", "qualname": "precompute_state", "kind": "function", "doc": "<p>Precompute the image embeddings and other optional state for the input image(s).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_path:</strong>  The input image file(s). Can either be a single image file (e.g. tif or png),\na container file (e.g. hdf5 or zarr) or a folder with images files.\nIn case of a container file the argument <code>key</code> must be given. In case of a folder\nit can be given to provide a glob pattern to subselect files from the folder.</li>\n<li><strong>output_path:</strong>  The output path were the embeddings and other state will be saved.</li>\n<li><strong>pattern:</strong>  Glob pattern to select files in a folder. The embeddings will be computed\nfor each of these files. To select all files in a folder pass \"*\".</li>\n<li><strong>model_type:</strong>  The SegmentAnything model to use. Will use the standard vit_h model by default.</li>\n<li><strong>checkpoint_path:</strong>  Path to a checkpoint for a custom model.</li>\n<li><strong>key:</strong>  The key to the input file. This is needed for contaner files (e.g. hdf5 or zarr)\nor to load several images as 3d volume. Provide a glob pattern, e.g. \"*.tif\", for this case.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic instance segmentation\nin addition to the image embeddings.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation", "modulename": "micro_sam.prompt_based_segmentation", "kind": "module", "doc": "<p>Functions for prompt-based segmentation with Segment Anything.</p>\n"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_points", "kind": "function", "doc": "<p>Segmentation from point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points:</strong>  The point prompts given in the image coordinate system.</li>\n<li><strong>labels:</strong>  The labels (positive or negative) associated with the points.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>use_best_multimask:</strong>  Whether to use multimask output and then choose the best mask.\nBy default this is used for a single positive point and not otherwise.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_best_multimask</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_mask", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_mask", "kind": "function", "doc": "<p>Segmentation from a mask prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>mask:</strong>  The mask used to derive prompts.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>use_box:</strong>  Whether to derive the bounding box prompt from the mask.</li>\n<li><strong>use_mask:</strong>  Whether to use the mask itself as prompt.</li>\n<li><strong>use_points:</strong>  Whether to derive point prompts from the mask.</li>\n<li><strong>original_size:</strong>  Full image shape. Use this if the mask that is being passed\ndownsampled compared to the original image.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>box_extension:</strong>  Relative factor used to enlarge the bounding box prompt.</li>\n<li><strong>box:</strong>  Precomputed bounding box.</li>\n<li><strong>points:</strong>  Precomputed point prompts.</li>\n<li><strong>labels:</strong>  Positive/negative labels corresponding to the point prompts.</li>\n<li><strong>use_single_point:</strong>  Whether to derive just a single point from the mask.\nIn case use_points is true.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">original_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_logits</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_single_point</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box", "kind": "function", "doc": "<p>Segmentation from a box prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>box_extension:</strong>  Relative factor used to enlarge the bounding box prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box_and_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box_and_points", "kind": "function", "doc": "<p>Segmentation from a box prompt and point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>points:</strong>  The point prompts, given in the image coordinates system.</li>\n<li><strong>labels:</strong>  The point labels, either positive or negative.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_generators", "modulename": "micro_sam.prompt_generators", "kind": "module", "doc": "<p>Classes for generating prompts from ground-truth segmentation masks.\nFor training or evaluation of prompt-based segmentation.</p>\n"}, {"fullname": "micro_sam.prompt_generators.PromptGeneratorBase", "modulename": "micro_sam.prompt_generators", "qualname": "PromptGeneratorBase", "kind": "class", "doc": "<p>PromptGeneratorBase is an interface to implement specific prompt generators.</p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator", "kind": "class", "doc": "<p>Generate point and/or box prompts from an instance segmentation.</p>\n\n<p>You can use this class to derive prompts from an instance segmentation, either for\nevaluation purposes or for training Segment Anything on custom data.\nIn order to use this generator you need to precompute the bounding boxes and center\ncoordiantes of the instance segmentation, using e.g. <code>util.get_centers_and_bounding_boxes</code>.</p>\n\n<p>Here's an example for how to use this class:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># Initialize generator for 1 positive and 4 negative point prompts.</span>\n<span class=\"n\">prompt_generator</span> <span class=\"o\">=</span> <span class=\"n\">PointAndBoxPromptGenerator</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">dilation_strength</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Precompute the bounding boxes for the given segmentation</span>\n<span class=\"n\">bounding_boxes</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">get_centers_and_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">segmentation</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># generate point prompts for the objects with ids 1, 2 and 3</span>\n<span class=\"n\">seg_ids</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"n\">object_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">segmentation</span> <span class=\"o\">==</span> <span class=\"n\">seg_id</span> <span class=\"k\">for</span> <span class=\"n\">seg_id</span> <span class=\"ow\">in</span> <span class=\"n\">seg_ids</span><span class=\"p\">])[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n<span class=\"n\">this_bounding_boxes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">bounding_boxes</span><span class=\"p\">[</span><span class=\"n\">seg_id</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">seg_id</span> <span class=\"ow\">in</span> <span class=\"n\">seg_ids</span><span class=\"p\">]</span>\n<span class=\"n\">point_coords</span><span class=\"p\">,</span> <span class=\"n\">point_labels</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">prompt_generator</span><span class=\"p\">(</span><span class=\"n\">object_mask</span><span class=\"p\">,</span> <span class=\"n\">this_bounding_boxes</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>n_positive_points:</strong>  The number of positive point prompts to generate per mask.</li>\n<li><strong>n_negative_points:</strong>  The number of negative point prompts to generate per mask.</li>\n<li><strong>dilation_strength:</strong>  The factor by which the mask is dilated before generating prompts.</li>\n<li><strong>get_point_prompts:</strong>  Whether to generate point prompts.</li>\n<li><strong>get_box_prompts:</strong>  Whether to generate box prompts.</li>\n</ul>\n", "bases": "PromptGeneratorBase"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.__init__", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_positive_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_strength</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">get_point_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_box_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_positive_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_positive_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_negative_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_negative_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.dilation_strength", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.dilation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_box_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_box_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_point_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_point_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.IterativePromptGenerator", "modulename": "micro_sam.prompt_generators", "qualname": "IterativePromptGenerator", "kind": "class", "doc": "<p>Generate point prompts from an instance segmentation iteratively.</p>\n", "bases": "PromptGeneratorBase"}, {"fullname": "micro_sam.sam_annotator", "modulename": "micro_sam.sam_annotator", "kind": "module", "doc": "<p>The interactive annotation tools.</p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.Annotator2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "Annotator2d", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.Annotator2d.__init__", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "Annotator2d.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.annotator_2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "annotator_2d", "kind": "function", "doc": "<p>Start the 2d annotation tool for a given image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The image data.</li>\n<li><strong>embedding_path:</strong>  Filepath where to save the embeddings.</li>\n<li><strong>segmentation_result:</strong>  An initial segmentation to load.\nThis can be used to correct segmentations with Segment Anything or to save and load progress.\nThe segmentation will be loaded as the 'committed_objects' layer.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_result</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.annotator_3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.Annotator3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "Annotator3d", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.Annotator3d.__init__", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "Annotator3d.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.annotator_3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "annotator_3d", "kind": "function", "doc": "<p>Start the 3d annotation tool for a given image volume.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The volumetric image data.</li>\n<li><strong>embedding_path:</strong>  Filepath for saving the precomputed embeddings.</li>\n<li><strong>segmentation_result:</strong>  An initial segmentation to load.\nThis can be used to correct segmentations with Segment Anything or to save and load progress.\nThe segmentation will be loaded as the 'committed_objects' layer.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_result</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.AnnotatorTracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "AnnotatorTracking", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.AnnotatorTracking.__init__", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "AnnotatorTracking.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.annotator_tracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "annotator_tracking", "kind": "function", "doc": "<p>Start the tracking annotation tool fora given timeseries.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>embedding_path:</strong>  Filepath for saving the precomputed embeddings.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.image_series_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "image_series_annotator", "kind": "function", "doc": "<p>Run the annotation tool for a series of images (supported for both 2d and 3d images).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>images:</strong>  List of the file paths or list of (set of) slices for the images to be annotated.</li>\n<li><strong>output_folder:</strong>  The folder where the segmentation results are saved.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>embedding_path:</strong>  Filepath where to save the embeddings.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>is_volumetric:</strong>  Whether to use the 3d annotator.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">images</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_volumetric</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.image_folder_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "image_folder_annotator", "kind": "function", "doc": "<p>Run the 2d annotation tool for a series of images in a folder.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_folder:</strong>  The folder with the images to be annotated.</li>\n<li><strong>output_folder:</strong>  The folder where the segmentation results are saved.</li>\n<li><strong>pattern:</strong>  The glob patter for loading files from <code>input_folder</code>.\nBy default all files will be loaded.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for <code>micro_sam.sam_annotator.image_series_annotator</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;*&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator", "kind": "class", "doc": "<p>QWidget(parent: typing.Optional[QWidget] = None, flags: Union[Qt.WindowFlags, Qt.WindowType] = Qt.WindowFlags())</p>\n", "bases": "micro_sam.sam_annotator._widgets._WidgetBase"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator.__init__", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span>, </span><span class=\"param\"><span class=\"n\">parent</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator.run_button", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator.run_button", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.training_ui", "modulename": "micro_sam.sam_annotator.training_ui", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget", "kind": "class", "doc": "<p>QWidget(parent: typing.Optional[QWidget] = None, flags: Union[Qt.WindowFlags, Qt.WindowType] = Qt.WindowFlags())</p>\n", "bases": "micro_sam.sam_annotator._widgets._WidgetBase"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget.__init__", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">parent</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget.run_button", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget.run_button", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.util", "modulename": "micro_sam.sam_annotator.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.util.point_layer_to_prompts", "modulename": "micro_sam.sam_annotator.util", "qualname": "point_layer_to_prompts", "kind": "function", "doc": "<p>Extract point prompts for SAM from a napari point layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer:</strong>  The point layer from which to extract the prompts.</li>\n<li><strong>i:</strong>  Index for the data (required for 3d or timeseries data).</li>\n<li><strong>track_id:</strong>  Id of the current track (required for tracking data).</li>\n<li><strong>with_stop_annotation:</strong>  Whether a single negative point will be interpreted\nas stop annotation or just returned as normal prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The point coordinates for the prompts.\n  The labels (positive or negative / 1 or 0) for the prompts.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">track_id</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_stop_annotation</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.shape_layer_to_prompts", "modulename": "micro_sam.sam_annotator.util", "qualname": "shape_layer_to_prompts", "kind": "function", "doc": "<p>Extract prompts for SAM from a napari shape layer.</p>\n\n<p>Extracts the bounding box for 'rectangle' shapes and the bounding box and corresponding mask\nfor 'ellipse' and 'polygon' shapes.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prompt_layer:</strong>  The napari shape layer.</li>\n<li><strong>shape:</strong>  The image shape.</li>\n<li><strong>i:</strong>  Index for the data (required for 3d or timeseries data).</li>\n<li><strong>track_id:</strong>  Id of the current track (required for tracking data).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The box prompts.\n  The mask prompts.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">Shapes</span>,</span><span class=\"param\">\t<span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">track_id</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.prompt_layer_to_state", "modulename": "micro_sam.sam_annotator.util", "qualname": "prompt_layer_to_state", "kind": "function", "doc": "<p>Get the state of the track from a point layer for a given timeframe.</p>\n\n<p>Only relevant for annotator_tracking.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prompt_layer:</strong>  The napari layer.</li>\n<li><strong>i:</strong>  Timeframe of the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The state of this frame (either \"division\" or \"track\").</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">prompt_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>, </span><span class=\"param\"><span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.prompt_layers_to_state", "modulename": "micro_sam.sam_annotator.util", "qualname": "prompt_layers_to_state", "kind": "function", "doc": "<p>Get the state of the track from a point layer and shape layer for a given timeframe.</p>\n\n<p>Only relevant for annotator_tracking.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>point_layer:</strong>  The napari point layer.</li>\n<li><strong>box_layer:</strong>  The napari box layer.</li>\n<li><strong>i:</strong>  Timeframe of the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The state of this frame (either \"division\" or \"track\").</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">point_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>,</span><span class=\"param\">\t<span class=\"n\">box_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">Shapes</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data", "modulename": "micro_sam.sample_data", "kind": "module", "doc": "<p>Sample microscopy data.</p>\n\n<p>You can change the download location for sample data and model weights\nby setting the environment variable: MICROSAM_CACHEDIR</p>\n\n<p>By default sample data is downloaded to a folder named 'micro_sam/sample_data'\ninside your default cache directory, eg:\n    * Mac: ~/Library/Caches/<AppName>\n    * Unix: ~/.cache/<AppName> or the value of the XDG_CACHE_HOME environment variable, if defined.\n    * Windows: C:\\Users&lt;user>\\AppData\\Local&lt;AppAuthor>&lt;AppName>\\Cache</p>\n"}, {"fullname": "micro_sam.sample_data.fetch_image_series_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_image_series_example_data", "kind": "function", "doc": "<p>Download the sample images for the image series annotator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_image_series", "modulename": "micro_sam.sample_data", "qualname": "sample_data_image_series", "kind": "function", "doc": "<p>Provides image series example image to napari.</p>\n\n<p>Opens as three separate image layers in napari (one per image in series).\nThe third image in the series has a different size and modality.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_wholeslide_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_wholeslide_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads part of a whole-slide image from the NeurIPS Cell Segmentation Challenge.\nSee <a href=\"https://neurips22-cellseg.grand-challenge.org/\">https://neurips22-cellseg.grand-challenge.org/</a> for details on the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_wholeslide", "modulename": "micro_sam.sample_data", "qualname": "sample_data_wholeslide", "kind": "function", "doc": "<p>Provides wholeslide 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_livecell_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_livecell_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the LiveCELL dataset.\nSee <a href=\"https://doi.org/10.1038/s41592-021-01249-6\">https://doi.org/10.1038/s41592-021-01249-6</a> for details on the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_livecell", "modulename": "micro_sam.sample_data", "qualname": "sample_data_livecell", "kind": "function", "doc": "<p>Provides livecell 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_hela_2d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_hela_2d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the HeLa CTC dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_hela_2d", "modulename": "micro_sam.sample_data", "qualname": "sample_data_hela_2d", "kind": "function", "doc": "<p>Provides HeLa 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_3d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_3d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 3d annotator.</p>\n\n<p>This downloads the Lucchi++ datasets from <a href=\"https://casser.io/connectomics/\">https://casser.io/connectomics/</a>.\nIt is a dataset for mitochondria segmentation in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_3d", "modulename": "micro_sam.sample_data", "qualname": "sample_data_3d", "kind": "function", "doc": "<p>Provides Lucchi++ 3d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_tracking_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_tracking_example_data", "kind": "function", "doc": "<p>Download the sample data for the tracking annotator.</p>\n\n<p>This data is the cell tracking challenge dataset DIC-C2DH-HeLa.\nCell tracking challenge webpage: <a href=\"http://data.celltrackingchallenge.net\">http://data.celltrackingchallenge.net</a>\nHeLa cells on a flat glass\nDr. G. van Cappellen. Erasmus Medical Center, Rotterdam, The Netherlands\nTraining dataset: <a href=\"http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip</a> (37 MB)\nChallenge dataset: <a href=\"http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip</a> (41 MB)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_tracking", "modulename": "micro_sam.sample_data", "qualname": "sample_data_tracking", "kind": "function", "doc": "<p>Provides tracking example dataset to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_tracking_segmentation_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_tracking_segmentation_data", "kind": "function", "doc": "<p>Download groundtruth segmentation for the tracking example data.</p>\n\n<p>This downloads the groundtruth segmentation for the image data from <code>fetch_tracking_example_data</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_segmentation", "modulename": "micro_sam.sample_data", "qualname": "sample_data_segmentation", "kind": "function", "doc": "<p>Provides segmentation example dataset to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.synthetic_data", "modulename": "micro_sam.sample_data", "qualname": "synthetic_data", "kind": "function", "doc": "<p>Create synthetic image data and segmentation for training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_nucleus_3d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_nucleus_3d_example_data", "kind": "function", "doc": "<p>Download the sample data for 3d segmentation of nuclei.</p>\n\n<p>This data contains a small crop from a volume from the publication\n\"Efficient automatic 3D segmentation of cell nuclei for high-content screening\"\n<a href=\"https://doi.org/10.1186/s12859-022-04737-4\">https://doi.org/10.1186/s12859-022-04737-4</a></p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training", "modulename": "micro_sam.training", "kind": "module", "doc": "<p>Functionality for training Segment Anything.</p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer", "modulename": "micro_sam.training.joint_sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer", "kind": "class", "doc": "<p>Trainer class for training the Segment Anything model.</p>\n\n<p>This class is derived from <code>torch_em.trainer.DefaultTrainer</code>.\nCheck out <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py</a>\nfor details on its usage and implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>convert_inputs:</strong>  The class that converts outputs of the dataloader to the expected input format of SAM.\nThe class <code>micro_sam.training.util.ConvertToSamInputs</code> can be used here.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iteration steps for which the masks predicted for one object are updated.\nIn each sub-iteration new point prompts are sampled where the model was wrong.</li>\n<li><strong>n_objects_per_batch:</strong>  If not given, we compute the loss for all objects in a sample.\nOtherwise the loss computation is limited to n_objects_per_batch, and the objects are randomly sampled.</li>\n<li><strong>mse_loss:</strong>  The regression loss to compare the IoU predicted by the model with the true IoU.</li>\n<li><strong>prompt_generator:</strong>  The iterative prompt generator which takes care of the iterative prompting logic for training</li>\n<li><strong>mask_prob:</strong>  The probability of using the mask inputs in the iterative prompting (per <code>n_sub_iteration</code>)</li>\n<li><strong>**kwargs:</strong>  The keyword arguments of the DefaultTrainer super class.</li>\n</ul>\n", "bases": "micro_sam.training.sam_trainer.SamTrainer"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.__init__", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">unetr</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">instance_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">instance_metric</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.unetr", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.unetr", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.instance_loss", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.instance_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.instance_metric", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.instance_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.save_checkpoint", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.save_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">current_metric</span>, </span><span class=\"param\"><span class=\"n\">best_metric</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">extra_save_dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.load_checkpoint", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.load_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.sam_trainer", "modulename": "micro_sam.training.sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer", "kind": "class", "doc": "<p>Trainer class for training the Segment Anything model.</p>\n\n<p>This class is derived from <code>torch_em.trainer.DefaultTrainer</code>.\nCheck out <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py</a>\nfor details on its usage and implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>convert_inputs:</strong>  The class that converts outputs of the dataloader to the expected input format of SAM.\nThe class <code>micro_sam.training.util.ConvertToSamInputs</code> can be used here.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iteration steps for which the masks predicted for one object are updated.\nIn each sub-iteration new point prompts are sampled where the model was wrong.</li>\n<li><strong>n_objects_per_batch:</strong>  If not given, we compute the loss for all objects in a sample.\nOtherwise the loss computation is limited to n_objects_per_batch, and the objects are randomly sampled.</li>\n<li><strong>mse_loss:</strong>  The regression loss to compare the IoU predicted by the model with the true IoU.</li>\n<li><strong>prompt_generator:</strong>  The iterative prompt generator which takes care of the iterative prompting logic for training</li>\n<li><strong>mask_prob:</strong>  The probability of using the mask inputs in the iterative prompting (per <code>n_sub_iteration</code>)</li>\n<li><strong>**kwargs:</strong>  The keyword arguments of the DefaultTrainer super class.</li>\n</ul>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.__init__", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">convert_inputs</span>,</span><span class=\"param\">\t<span class=\"n\">n_sub_iteration</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_objects_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mse_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">=</span> <span class=\"n\">MSELoss</span><span class=\"p\">()</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_generator</span><span class=\"p\">:</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">prompt_generators</span><span class=\"o\">.</span><span class=\"n\">PromptGeneratorBase</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">prompt_generators</span><span class=\"o\">.</span><span class=\"n\">IterativePromptGenerator</span> <span class=\"nb\">object</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.convert_inputs", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.convert_inputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.mse_loss", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.mse_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.n_objects_per_batch", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.n_objects_per_batch", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.n_sub_iteration", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.n_sub_iteration", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.prompt_generator", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.prompt_generator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.mask_prob", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.mask_prob", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam", "modulename": "micro_sam.training.trainable_sam", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM", "kind": "class", "doc": "<p>Wrapper to make the SegmentAnything model trainable.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sam:</strong>  The SegmentAnything Model.</li>\n<li><strong>device:</strong>  The device for training.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.__init__", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sam</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">modeling</span><span class=\"o\">.</span><span class=\"n\">sam</span><span class=\"o\">.</span><span class=\"n\">Sam</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.sam", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.device", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.transform", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.preprocess", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.preprocess", "kind": "function", "doc": "<p>Resize, normalize pixel values and pad to a square input.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  The input tensor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The resized, normalized and padded tensor.\n  The shape of the image after resizing.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.image_embeddings_oft", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.image_embeddings_oft", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batched_inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.forward", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.forward", "kind": "function", "doc": "<p>Forward pass.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batched_inputs:</strong>  The batched input images and prompts.</li>\n<li><strong>image_embeddings:</strong>  The precompute image embeddings. If not passed then they will be computed.</li>\n<li><strong>multimask_output:</strong>  Whether to predict mutiple or just a single mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predicted segmentation masks and iou values.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batched_inputs</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training", "modulename": "micro_sam.training.training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.training.FilePath", "modulename": "micro_sam.training.training", "qualname": "FilePath", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Union[str, os.PathLike]"}, {"fullname": "micro_sam.training.training.train_sam", "modulename": "micro_sam.training.training", "qualname": "train_sam", "kind": "function", "doc": "<p>Run training for a SAM model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>name:</strong>  The name of the model to be trained.\nThe checkpoint and logs wil have this name.</li>\n<li><strong>model_type:</strong>  The type of the SAM model.</li>\n<li><strong>train_loader:</strong>  The dataloader for training.</li>\n<li><strong>val_loader:</strong>  The dataloader for validation.</li>\n<li><strong>n_epochs:</strong>  The number of epochs to train for.</li>\n<li><strong>early_stopping:</strong>  Enable early stopping after this number of epochs\nwithout improvement.</li>\n<li><strong>n_objects_per_batch:</strong>  The number of objects per batch used to compute\nthe loss for interative segmentation. If None all objects will be used,\nif given objects will be randomly sub-sampled.</li>\n<li><strong>checkpoint_path:</strong>  Path to checkpoint for initializing the SAM model.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train additional UNETR decoder\nfor automatic instance segmentation.</li>\n<li><strong>freeze:</strong>  Specify parts of the model that should be frozen, namely:\nimage_encoder, prompt_encoder and mask_decoder\nBy default nothing is frozen and the full model is updated.</li>\n<li><strong>device:</strong>  The device to use for training.</li>\n<li><strong>lr:</strong>  The learning rate.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iterative prompts per training iteration.</li>\n<li><strong>save_root:</strong>  Optional root directory for saving the checkpoints and logs.\nIf not given the current working directory is used.</li>\n<li><strong>mask_prob:</strong>  The probability for using a mask as input in a given training sub-iteration.</li>\n<li><strong>n_iterations:</strong>  The number of iterations to use for training. This will over-ride n_epochs if given.</li>\n<li><strong>scheduler_class:</strong>  The learning rate scheduler to update the learning rate.\nBy default, ReduceLROnPlateau is used.</li>\n<li><strong>scheduler_kwargs:</strong>  The learning rate scheduler parameters.\nIf passed None, the chosen default parameters are used in ReduceLROnPlateau.</li>\n<li><strong>save_every_kth_epoch:</strong>  Save checkpoints after every kth epoch separately.</li>\n<li><strong>pbar_signals:</strong>  Controls for napari progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">n_epochs</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">n_objects_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">freeze</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-05</span>,</span><span class=\"param\">\t<span class=\"n\">n_sub_iteration</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">save_root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">n_iterations</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\tscheduler_class: Optional[torch.optim.lr_scheduler._LRScheduler] = &lt;class &#x27;torch.optim.lr_scheduler.ReduceLROnPlateau&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">scheduler_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_every_kth_epoch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_signals</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">PyQt5</span><span class=\"o\">.</span><span class=\"n\">QtCore</span><span class=\"o\">.</span><span class=\"n\">QObject</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.default_sam_dataset", "modulename": "micro_sam.training.training", "qualname": "default_sam_dataset", "kind": "function", "doc": "<p>Create a PyTorch Dataset for training a SAM model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>raw_paths:</strong>  The path(s) to the image data used for training.\nCan either be multiple 2D images or volumetric data.</li>\n<li><strong>raw_key:</strong>  The key for accessing the image data. Internal filepath for hdf5-like input\nor a glob pattern for selecting multiple files.</li>\n<li><strong>label_paths:</strong>  The path(s) to the label data used for training.\nCan either be multiple 2D images or volumetric data.</li>\n<li><strong>label_key:</strong>  The key for accessing the label data. Internal filepath for hdf5-like input\nor a glob pattern for selecting multiple files.</li>\n<li><strong>patch_shape:</strong>  The shape for training patches.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train with additional segmentation decoder.</li>\n<li><strong>with_channels:</strong>  Whether the image data has RGB channels.</li>\n<li><strong>sampler:</strong>  A sampler to reject batches according to a given criterion.</li>\n<li><strong>n_samples:</strong>  The number of samples for this dataset.</li>\n<li><strong>is_train:</strong>  Whether this dataset is used for training or validation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.default_sam_loader", "modulename": "micro_sam.training.training", "qualname": "default_sam_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.CONFIGURATIONS", "modulename": "micro_sam.training.training", "qualname": "CONFIGURATIONS", "kind": "variable", "doc": "<p>Best training configurations for given hardware resources.</p>\n", "default_value": "{&#x27;Minimal&#x27;: {&#x27;model_type&#x27;: &#x27;vit_t&#x27;, &#x27;n_objects_per_batch&#x27;: 4, &#x27;n_sub_iteration&#x27;: 4}, &#x27;CPU&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;, &#x27;n_objects_per_batch&#x27;: 10}, &#x27;gtx1080&#x27;: {&#x27;model_type&#x27;: &#x27;vit_t&#x27;, &#x27;n_objects_per_batch&#x27;: 5}, &#x27;rtx5000&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;, &#x27;n_objects_per_batch&#x27;: 10}, &#x27;V100&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;}, &#x27;A100&#x27;: {&#x27;model_type&#x27;: &#x27;vit_h&#x27;}}"}, {"fullname": "micro_sam.training.training.train_sam_for_configuration", "modulename": "micro_sam.training.training", "qualname": "train_sam_for_configuration", "kind": "function", "doc": "<p>Run training for a SAM model with the configuration for a given hardware resource.</p>\n\n<p>Selects the best training settings for the given configuration.\nThe available configurations are listed in <code>CONFIGURATIONS</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>name:</strong>  The name of the model to be trained.\nThe checkpoint and logs wil have this name.</li>\n<li><strong>configuration:</strong>  The configuration (= name of hardware resource).</li>\n<li><strong>train_loader:</strong>  The dataloader for training.</li>\n<li><strong>val_loader:</strong>  The dataloader for validation.</li>\n<li><strong>checkpoint_path:</strong>  Path to checkpoint for initializing the SAM model.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train additional UNETR decoder\nfor automatic instance segmentation.</li>\n<li><strong>kwargs:</strong>  Additional keyword parameterts that will be passed to <code>train_sam</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util", "modulename": "micro_sam.training.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.identity", "modulename": "micro_sam.training.util", "qualname": "identity", "kind": "function", "doc": "<p>Identity transformation.</p>\n\n<p>This is a helper function to skip data normalization when finetuning SAM.\nData normalization is performed within the model and should thus be skipped as\na preprocessing step in training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.require_8bit", "modulename": "micro_sam.training.util", "qualname": "require_8bit", "kind": "function", "doc": "<p>Transformation to require 8bit input data range (0-255).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.get_trainable_sam_model", "modulename": "micro_sam.training.util", "qualname": "get_trainable_sam_model", "kind": "function", "doc": "<p>Get the trainable sam model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The segment anything model that should be finetuned.\nThe weights of this model will be used for initialization, unless a\ncustom weight file is passed via <code>checkpoint_path</code>.</li>\n<li><strong>device:</strong>  The device to use for training.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the model weights.</li>\n<li><strong>freeze:</strong>  Specify parts of the model that should be frozen, namely: image_encoder, prompt_encoder and mask_decoder\nBy default nothing is frozen and the full model is updated.</li>\n<li><strong>return_state:</strong>  Whether to return the full checkpoint state.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The trainable segment anything model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">freeze</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">trainable_sam</span><span class=\"o\">.</span><span class=\"n\">TrainableSAM</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs", "kind": "class", "doc": "<p>Convert outputs of data loader to the expected batched inputs of the SegmentAnything model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>transform:</strong>  The transformation to resize the prompts. Should be the same transform used in the\nmodel to resize the inputs. If <code>None</code> the prompts will not be resized.</li>\n<li><strong>dilation_strength:</strong>  The dilation factor.\nIt determines a \"safety\" border from which prompts are not sampled to avoid ambiguous prompts\ndue to imprecise groundtruth masks.</li>\n<li><strong>box_distortion_factor:</strong>  Factor for distorting the box annotations derived from the groundtruth masks.</li>\n</ul>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.__init__", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ResizeLongestSide</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_strength</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">box_distortion_factor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.dilation_strength", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.dilation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.transform", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.box_distortion_factor", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.box_distortion_factor", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.__init__", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">desired_shape</span>, </span><span class=\"param\"><span class=\"n\">do_rescaling</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;constant&#39;</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.desired_shape", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.desired_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.padding", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.do_rescaling", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.do_rescaling", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.__init__", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">desired_shape</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;constant&#39;</span>, </span><span class=\"param\"><span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">0</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.desired_shape", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.desired_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.padding", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.min_size", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.min_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.util", "modulename": "micro_sam.util", "kind": "module", "doc": "<p>Helper functions for downloading Segment Anything models and predicting image embeddings.</p>\n"}, {"fullname": "micro_sam.util.get_cache_directory", "modulename": "micro_sam.util", "qualname": "get_cache_directory", "kind": "function", "doc": "<p>Get micro-sam cache directory location.</p>\n\n<p>Users can set the MICROSAM_CACHEDIR environment variable for a custom cache directory.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.microsam_cachedir", "modulename": "micro_sam.util", "qualname": "microsam_cachedir", "kind": "function", "doc": "<p>Return the micro-sam cache directory.</p>\n\n<p>Returns the top level cache directory for micro-sam models and sample data.</p>\n\n<p>Every time this function is called, we check for any user updates made to\nthe MICROSAM_CACHEDIR os environment variable since the last time.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.models", "modulename": "micro_sam.util", "qualname": "models", "kind": "function", "doc": "<p>Return the segmentation models registry.</p>\n\n<p>We recreate the model registry every time this function is called,\nso any user changes to the default micro-sam cache directory location\nare respected.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_device", "modulename": "micro_sam.util", "qualname": "get_device", "kind": "function", "doc": "<p>Get the torch device.</p>\n\n<p>If no device is passed the default device for your system is used.\nElse it will be checked if the device you have passed is supported.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>device:</strong>  The input device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The device.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_sam_model", "modulename": "micro_sam.util", "qualname": "get_sam_model", "kind": "function", "doc": "<p>Get the SegmentAnything Predictor.</p>\n\n<p>This function will download the required model or load it from the cached weight file.\nThis location of the cache can be changed by setting the environment variable: MICROSAM_CACHEDIR.\nThe name of the requested model can be set via <code>model_type</code>.\nSee <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>\nfor an overview of the available models</p>\n\n<p>Alternatively this function can also load a model from weights stored in a local filepath.\nThe corresponding file path is given via <code>checkpoint_path</code>. In this case <code>model_type</code>\nmust be given as the matching encoder architecture, e.g. \"vit_b\" if the weights are for\na SAM model with vit_b encoder.</p>\n\n<p>By default the models are downloaded to a folder named 'micro_sam/models'\ninside your default cache directory, eg:</p>\n\n<ul>\n<li>Mac: ~/Library/Caches/<AppName></li>\n<li>Unix: ~/.cache/<AppName> or the value of the XDG_CACHE_HOME environment variable, if defined.</li>\n<li>Windows: C:\\Users&lt;user>\\AppData\\Local&lt;AppAuthor>&lt;AppName>\\Cache\nSee the pooch.os_cache() documentation for more details:\n<a href=\"https://www.fatiando.org/pooch/latest/api/generated/pooch.os_cache.html\">https://www.fatiando.org/pooch/latest/api/generated/pooch.os_cache.html</a></li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The SegmentAnything model to use. Will use the standard vit_h model by default.\nTo get a list of all available model names you can call <code>get_model_names</code>.</li>\n<li><strong>device:</strong>  The device for the model. If none is given will use GPU if available.</li>\n<li><strong>checkpoint_path:</strong>  The path to a file with weights that should be used instead of using the\nweights corresponding to <code>model_type</code>. If given, <code>model_type</code> must match the architecture\ncorresponding to the weight file. E.g. if you use weights for SAM with vit_b encoder\nthen <code>model_type</code> must be given as \"vit_b\".</li>\n<li><strong>return_sam:</strong>  Return the sam model object as well as the predictor.</li>\n<li><strong>return_state:</strong>  Return the unpickled checkpoint state.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segment anything predictor.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_sam</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.export_custom_sam_model", "modulename": "micro_sam.util", "qualname": "export_custom_sam_model", "kind": "function", "doc": "<p>Export a finetuned segment anything model to the standard model format.</p>\n\n<p>The exported model can be used by the interactive annotation tools in <code>micro_sam.annotator</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint_path:</strong>  The path to the corresponding checkpoint if not in the default model folder.</li>\n<li><strong>model_type:</strong>  The SegmentAnything model type corresponding to the checkpoint (vit_h, vit_b, vit_l or vit_t).</li>\n<li><strong>save_path:</strong>  Where to save the exported model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_model_names", "modulename": "micro_sam.util", "qualname": "get_model_names", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">Iterable</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.precompute_image_embeddings", "modulename": "micro_sam.util", "qualname": "precompute_image_embeddings", "kind": "function", "doc": "<p>Compute the image embeddings (output of the encoder) for the input.</p>\n\n<p>If 'save_path' is given the embeddings will be loaded/saved in a zarr container.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>input_:</strong>  The input data. Can be 2 or 3 dimensional, corresponding to an image, volume or timeseries.</li>\n<li><strong>save_path:</strong>  Path to save the embeddings in a zarr container.</li>\n<li><strong>lazy_loading:</strong>  Whether to load all embeddings into memory or return an\nobject to load them on demand when required. This only has an effect if 'save_path' is given\nand if the input is 3 dimensional.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data. If not given will be deduced from the input data.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>verbose:</strong>  Whether to be verbose in the computation.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The image embeddings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">input_</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.set_precomputed", "modulename": "micro_sam.util", "qualname": "set_precomputed", "kind": "function", "doc": "<p>Set the precomputed image embeddings for a predictor.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_embeddings:</strong>  The precomputed image embeddings computed by <code>precompute_image_embeddings</code>.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_id:</strong>  Index for the tile. This is required if the embeddings are tiled.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predictor with set features.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.compute_iou", "modulename": "micro_sam.util", "qualname": "compute_iou", "kind": "function", "doc": "<p>Compute the intersection over union of two masks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask1:</strong>  The first mask.</li>\n<li><strong>mask2:</strong>  The second mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The intersection over union of the two masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mask1</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>, </span><span class=\"param\"><span class=\"n\">mask2</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_centers_and_bounding_boxes", "modulename": "micro_sam.util", "qualname": "get_centers_and_bounding_boxes", "kind": "function", "doc": "<p>Returns the center coordinates of the foreground instances in the ground-truth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The segmentation.</li>\n<li><strong>mode:</strong>  Determines the functionality used for computing the centers.</li>\n<li>If 'v', the object's eccentricity centers computed by vigra are used.</li>\n<li>If 'p' the object's centroids computed by skimage are used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dictionary that maps object ids to the corresponding centroid.\n  A dictionary that maps object_ids to the corresponding bounding box.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;v&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.load_image_data", "modulename": "micro_sam.util", "qualname": "load_image_data", "kind": "function", "doc": "<p>Helper function to load image data from file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The filepath to the image data.</li>\n<li><strong>key:</strong>  The internal filepath for complex data formats like hdf5.</li>\n<li><strong>lazy_loading:</strong>  Whether to lazyly load data. Only supported for n5 and zarr data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The image data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.segmentation_to_one_hot", "modulename": "micro_sam.util", "qualname": "segmentation_to_one_hot", "kind": "function", "doc": "<p>Convert the segmentation to one-hot encoded masks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The segmentation.</li>\n<li><strong>segmentation_ids:</strong>  Optional subset of ids that will be used to subsample the masks.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The one-hot encoded masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_block_shape", "modulename": "micro_sam.util", "qualname": "get_block_shape", "kind": "function", "doc": "<p>Get a suitable block shape for chunking a given shape.</p>\n\n<p>The primary use for this is determining chunk sizes for\nzarr arrays or block shapes for parallelization.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>shape:</strong>  The image or volume shape.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The block shape.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization", "modulename": "micro_sam.visualization", "kind": "module", "doc": "<p>Functionality for visualizing image embeddings.</p>\n"}, {"fullname": "micro_sam.visualization.compute_pca", "modulename": "micro_sam.visualization", "qualname": "compute_pca", "kind": "function", "doc": "<p>Compute the pca projection of the embeddings to visualize them as RGB image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings:</strong>  The embeddings. For example predicted by the SAM image encoder.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>PCA of the embeddings, mapped to the pixels.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization.project_embeddings_for_visualization", "modulename": "micro_sam.visualization", "qualname": "project_embeddings_for_visualization", "kind": "function", "doc": "<p>Project image embeddings to pixel-wise PCA.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PCA of the embeddings.\n  The scale factor for resizing to the original image size.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();