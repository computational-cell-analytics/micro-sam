window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "micro_sam", "modulename": "micro_sam", "kind": "module", "doc": "<h1 id=\"segment-anything-for-microscopy\">Segment Anything for Microscopy</h1>\n\n<p>Segment Anything for Microscopy implements automatic and interactive annotation for microscopy data. It is built on top of <a href=\"https://segment-anything.com/\">Segment Anything</a> by Meta AI and specializes it for microscopy and other bio-imaging data.\nIts core components are:</p>\n\n<ul>\n<li>The <code>micro_sam</code> tools for interactive data annotation with <a href=\"https://napari.org/stable/\">napari</a>.</li>\n<li>The <code>micro_sam</code> library to apply Segment Anything to 2d and 3d data or fine-tune it on your data.</li>\n<li>The <code>micro_sam</code> models that are fine-tuned on publicly available microscopy data.</li>\n</ul>\n\n<p>Our goal is to build fast and interactive annotation tools for microscopy data, like interactive cell segmentation from bounding boxes:</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d04cb158-9f5b-4460-98cd-023c4f19cccd\" alt=\"box-prompts\" /></p>\n\n<p><code>micro_sam</code> is under active development, but our goal is to keep the changes to the user interface and the interface of the python library as small as possible.\nOn our roadmap for more functionality are:</p>\n\n<ul>\n<li>Providing an installer for running <code>micro_sam</code> as a standalone application.</li>\n<li>Releasing more and better finetuned models as well as the code for fine-tuning.</li>\n<li>Integration of the finetuned models with <a href=\"https://bioimage.io/#/\">bioimage.io</a></li>\n<li>Implementing a napari plugin for <code>micro_sam</code>.</li>\n</ul>\n\n<p>If you run into any problems or have questions please open an issue on Github or reach out via <a href=\"https://forum.image.sc/\">image.sc</a> using the tag <code>micro-sam</code> and tagging @constantinpape.</p>\n\n<h2 id=\"quickstart\">Quickstart</h2>\n\n<p>You can install <code>micro_sam</code> via conda:</p>\n\n<pre><code>$ conda install -c conda-forge micro_sam napari pyqt\n</code></pre>\n\n<p>We also provide experimental installers for all operating systems.\nFor more details on the available installation options check out <a href=\"#installation\">the installation section</a>.</p>\n\n<p>After installing <code>micro_sam</code> you can run the annotation tool via <code>$ micro_sam.annotator</code>, which opens a menu for selecting the annotation tool and its inputs.\nSee <a href=\"#annotation-tools\">the annotation tool section</a> for an overview and explanation of the annotation functionality.</p>\n\n<p>The <code>micro_sam</code> python library can be used via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>It is explained in more detail <a href=\"#how-to-use-the-python-library\">here</a>.</p>\n\n<p>Our support for finetuned models is still experimental. We will soon release better finetuned models and host them on zenodo.\nFor now, check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/sam_annotator_2d.py#L62\">the example script for the 2d annotator</a> to see how the finetuned models can be used within <code>micro_sam</code>.</p>\n\n<h2 id=\"citation\">Citation</h2>\n\n<p>If you are using <code>micro_sam</code> in your research please cite</p>\n\n<ul>\n<li>Our <a href=\"https://doi.org/10.1101/2023.08.21.554208\">preprint</a></li>\n<li>and the original <a href=\"https://arxiv.org/abs/2304.02643\">Segment Anything publication</a>.</li>\n<li>If you use a <code>vit-tiny</code> models please also cite <a href=\"https://arxiv.org/abs/2306.14289\">Mobile SAM</a>.</li>\n</ul>\n\n<h1 id=\"installation\">Installation</h1>\n\n<p>There are three ways to install <code>micro_sam</code>:</p>\n\n<ul>\n<li><a href=\"#from-mamba\">From mamba</a> is the recommended way if you want to use all functionality.</li>\n<li><a href=\"#from-source\">From source</a> for setting up a development environment to use the latest version and be able to change and contribute to our software.</li>\n<li><a href=\"#from-installer\">From installer</a> to install without having to use mamba (supported platforms: Windows and Linux, only for CPU users). </li>\n</ul>\n\n<h2 id=\"from-mamba\">From mamba</h2>\n\n<p><a href=\"https://mamba.readthedocs.io/en/latest/\">mamba</a> is a drop-in replacement for conda, but much faster.\nWhile the steps below may also work with <code>conda</code>, we highly recommend using <code>mamba</code>.\nYou can follow the instructions <a href=\"https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html\">here</a> to install <code>mamba</code>.</p>\n\n<p><strong>IMPORTANT</strong>: Make sure to avoid installing anything in the base environment.</p>\n\n<p><code>micro_sam</code> can be installed in an existing environment via:</p>\n\n<pre><code>$ mamba install -c conda-forge micro_sam\n</code></pre>\n\n<p>or you should create a new environment (here called <code>micro-sam</code>) via:</p>\n\n<pre><code>$ mamba create -c conda-forge -n micro-sam micro_sam\n</code></pre>\n\n<p>if you want to use the GPU you need to install PyTorch from the <code>pytorch</code> channel instead of <code>conda-forge</code>. For example:</p>\n\n<pre><code>$ mamba create -c pytorch -c nvidia -c conda-forge micro_sam pytorch pytorch-cuda=12.1\n</code></pre>\n\n<p>You may need to change this command to install the correct CUDA version for your computer, see <a href=\"https://pytorch.org/\">https://pytorch.org/</a> for details.</p>\n\n<p>You also need to install napari to use the annotation tool:</p>\n\n<pre><code>$ mamba install -c conda-forge napari pyqt\n</code></pre>\n\n<p>(We don't include napari in the default installation dependencies to keep the choice of rendering backend flexible.)</p>\n\n<h2 id=\"from-source\">From source</h2>\n\n<p>To install <code>micro_sam</code> from source, we recommend to first set up an environment with the necessary requirements:</p>\n\n<ul>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_gpu.yaml\">environment_gpu.yaml</a>: sets up an environment with GPU support.</li>\n<li><a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/environment_cpu.yaml\">environment_cpu.yaml</a>: sets up an environment with CPU support.</li>\n</ul>\n\n<p>To create one of these environments and install <code>micro_sam</code> into it follow these steps</p>\n\n<ol>\n<li>Clone the repository:</li>\n</ol>\n\n<pre><code>$ git clone https://github.com/computational-cell-analytics/micro-sam\n</code></pre>\n\n<ol start=\"2\">\n<li>Enter it:</li>\n</ol>\n\n<pre><code>$ cd micro-sam\n</code></pre>\n\n<ol start=\"3\">\n<li>Create the GPU or CPU environment:</li>\n</ol>\n\n<pre><code>$ mamba env create -f &lt;ENV_FILE&gt;.yaml\n</code></pre>\n\n<ol start=\"4\">\n<li>Activate the environment:</li>\n</ol>\n\n<pre><code>$ mamba activate sam\n</code></pre>\n\n<ol start=\"5\">\n<li>Install <code>micro_sam</code>:</li>\n</ol>\n\n<pre><code>$ pip install -e .\n</code></pre>\n\n<p><strong>Troubleshooting:</strong></p>\n\n<ul>\n<li>Installation on MAC with a M1 or M2 processor:\n<ul>\n<li>The pytorch installation from <code>environment_cpu.yaml</code> does not work with a MAC that has an M1 or M2 processor. Instead you need to:\n<ul>\n<li>Create a new environment: <code>mamba create -c conda-forge python pip -n sam</code></li>\n<li>Activate it va <code>mamba activate sam</code></li>\n<li>Follow the instructions for how to install pytorch for MAC via conda from <a href=\"https://pytorch.org/\">pytorch.org</a>.</li>\n<li>Install additional dependencies: <code>mamba install -c conda-forge napari python-elf tqdm</code></li>\n<li>Install SegmentAnything: <code>pip install git+https://github.com/facebookresearch/segment-anything.git</code></li>\n<li>Install <code>micro_sam</code> by running <code>pip install -e .</code> in this folder.</li>\n</ul></li>\n<li><strong>Note:</strong> we have seen many issues with the pytorch installation on MAC. If a wrong pytorch version is installed for you (which will cause pytorch errors once you run the application) please try again with a clean <code>mambaforge</code> installation. Please install the <code>OS X, arm64</code> version from <a href=\"https://github.com/conda-forge/miniforge#mambaforge\">here</a>.</li>\n<li>Some MACs require a specific installation order of packages. If the steps layed out above don't work for you please check out the procedure described <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/77\">in this github issue</a>.</li>\n</ul></li>\n</ul>\n\n<h2 id=\"from-installer\">From installer</h2>\n\n<p>We also provide installers for Linux and Windows:</p>\n\n<ul>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/hM1bQ108YmcwyDn\">Linux</a></li>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/T1weJclOiYUUULE\">Windows</a>\n<!---</li>\n<li><a href=\"https://owncloud.gwdg.de/index.php/s/7YupGgACw9SHy2P\">Mac</a>\n--></li>\n</ul>\n\n<p><strong>The installers are still experimental and not fully tested.</strong> Mac is not supported yet, but we are working on also providing an installer for it.</p>\n\n<p>If you encounter problems with them then please consider installing <code>micro_sam</code> via <a href=\"#from-mamba\">mamba</a> instead.</p>\n\n<p><strong>Linux Installer:</strong></p>\n\n<p>To use the installer:</p>\n\n<ul>\n<li>Unpack the zip file you have downloaded.</li>\n<li>Make the installer executable: <code>$ chmod +x micro_sam-0.2.0post1-Linux-x86_64.sh</code></li>\n<li>Run the installer: <code>$./micro_sam-0.2.0post1-Linux-x86_64.sh$</code> \n<ul>\n<li>You can select where to install <code>micro_sam</code> during the installation. By default it will be installed in <code>$HOME/micro_sam</code>.</li>\n<li>The installer will unpack all <code>micro_sam</code> files to the installation directory.</li>\n</ul></li>\n<li>After the installation you can start the annotator with the command <code>.../micro_sam/bin/micro_sam.annotator</code>.\n<ul>\n<li>To make it easier to run the annotation tool you can add <code>.../micro_sam/bin</code> to your <code>PATH</code> or set a softlink to <code>.../micro_sam/bin/micro_sam.annotator</code>.</li>\n</ul></li>\n</ul>\n\n<p><!---\n<strong>Mac Installer:</strong></p>\n\n<p>To use the Mac installer you will need to enable installing unsigned applications. Please follow <a href=\"https://disable-gatekeeper.github.io/\">the instructions for 'Disabling Gatekeeper for one application only' here</a>.</p>\n\n<p>Alternative link on how to disable gatekeeper.\n<a href=\"https://www.makeuseof.com/how-to-disable-gatekeeper-mac/\">https://www.makeuseof.com/how-to-disable-gatekeeper-mac/</a></p>\n\n<p>TODO detailed instruction\n--></p>\n\n<p><strong>Windows Installer:</strong></p>\n\n<ul>\n<li>Unpack the zip file you have downloaded.</li>\n<li>Run the installer by double clicking on it.</li>\n<li>Choose installation type: <code>Just Me(recommended)</code> or <code>All Users(requires admin privileges)</code>.</li>\n<li>Choose installation path. By default it will be installed in <code>C:\\Users\\&lt;Username&gt;\\micro_sam</code> for <code>Just Me</code> installation or in <code>C:\\ProgramData\\micro_sam</code> for <code>All Users</code>.\n<ul>\n<li>The installer will unpack all micro_sam files to the installation directory.</li>\n</ul></li>\n<li>After the installation you can start the annotator by double clicking on <code>.\\micro_sam\\Scripts\\micro_sam.annotator.exe</code> or  with the command <code>.\\micro_sam\\Scripts\\micro_sam.annotator.exe</code> from the Command Prompt.</li>\n</ul>\n\n<h1 id=\"annotation-tools\">Annotation Tools</h1>\n\n<p><code>micro_sam</code> provides applications for fast interactive 2d segmentation, 3d segmentation and tracking.\nSee an example for interactive cell segmentation in phase-contrast microscopy (left), interactive segmentation\nof mitochondria in volume EM (middle) and interactive tracking of cells (right).</p>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/d5ee2080-ab08-4716-b4c4-c169b4ed29f5\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/dfca3d9b-dba5-440b-b0f9-72a0683ac410\" width=\"256\">\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/aefbf99f-e73a-4125-bb49-2e6592367a64\" width=\"256\"></p>\n\n<p>The annotation tools can be started from the <code>micro_sam</code> GUI, the command line or from python scripts. The <code>micro_sam</code> GUI can be started by</p>\n\n<pre><code>$ micro_sam.annotator\n</code></pre>\n\n<p>They are built using <a href=\"https://napari.org/stable/\">napari</a> and <a href=\"https://pyapp-kit.github.io/magicgui/\">magicgui</a> to provide the viewer and user interface.\nIf you are not familiar with napari yet, <a href=\"https://napari.org/stable/tutorials/fundamentals/quick_start.html\">start here</a>.\nThe <code>micro_sam</code> tools use <a href=\"https://napari.org/stable/howtos/layers/points.html\">the point layer</a>, <a href=\"https://napari.org/stable/howtos/layers/shapes.html\">shape layer</a> and <a href=\"https://napari.org/stable/howtos/layers/labels.html\">label layer</a>.</p>\n\n<p>The annotation tools are explained in detail below. In addition to the documentation here we also provide <a href=\"https://www.youtube.com/watch?v=ket7bDUP9tI&list=PLwYZXQJ3f36GQPpKCrSbHjGiH39X4XjSO\">video tutorials</a>.</p>\n\n<h2 id=\"starting-via-gui\">Starting via GUI</h2>\n\n<p>The annotation toools can be started from a central GUI, which can be started with the command <code>$ micro_sam.annotator</code> or using the executable <a href=\"#from-installer\">from an installer</a>.</p>\n\n<p>In the GUI you can select with of the four annotation tools you want to use:\n<img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/micro-sam-gui.png\"></p>\n\n<p>And after selecting them a new window will open where you can select the input file path and other optional parameter. Then click the top button to start the tool. <strong>Note: If you are not starting the annotation tool with a path to pre-computed embeddings then it can take several minutes to open napari after pressing the button because the embeddings are being computed.</strong></p>\n\n<p><strong>Changes in version 0.3:</strong></p>\n\n<p>We have made two changes in version 0.3 that are not reflected in the documentation below yet:</p>\n\n<ul>\n<li>We now support prompts from box, ellipse and polygon annotations. To reflect this we have renamed the <code>box_prompts</code> layer to <code>prompts</code> and the <code>prompts</code> layer to <code>point_prompts</code>.</li>\n<li>We support automatic segmentation in 3d! To use it, you can first run automated segmentation in the current slice via <code>Automatic Segmentation</code>, and then extend the segmentation of these objects to 3d by running <code>Segment All Slices</code> with <code>layer: auto segmentation</code>.</li>\n</ul>\n\n<h2 id=\"annotator-2d\">Annotator 2D</h2>\n\n<p>The 2d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>2d annotator</code> in the <code>micro_sam</code> GUI.</li>\n<li>running <code>$ micro_sam.annotator_2d</code> in the command line. Run <code>micro_sam.annotator_2d -h</code> for details.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_2d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py\">examples/annotator_2d.py</a> for details. </li>\n</ul>\n\n<p>The user interface of the 2d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/2d-annotator-menu.png\" width=\"768\"></p>\n\n<p>It contains the following elements:</p>\n\n<ol>\n<li>The napari layers for the image, segmentations and prompts:\n<ul>\n<li><code>box_prompts</code>: shape layer that is used to provide box prompts to SegmentAnything.</li>\n<li><code>prompts</code>: point layer that is used to provide prompts to SegmentAnything. Positive prompts (green points) for marking the object you want to segment, negative prompts (red points) for marking the outside of the object.</li>\n<li><code>current_object</code>: label layer that contains the object you're currently segmenting.</li>\n<li><code>committed_objects</code>: label layer with the objects that have already been segmented.</li>\n<li><code>auto_segmentation</code>: label layer results from using SegmentAnything for automatic instance segmentation.</li>\n<li><code>raw</code>: image layer that shows the image data.</li>\n</ul></li>\n<li>The prompt menu for changing the currently selected point from positive to negative and vice versa. This can also be done by pressing <code>t</code>.</li>\n<li>The menu for automatic segmentation. Pressing <code>Segment All Objects</code> will run automatic segmentation. The results will be displayed in the <code>auto_segmentation</code> layer. Change the parameters <code>pred iou thresh</code> and <code>stability score thresh</code> to control how many objects are segmented.</li>\n<li>The menu for interactive segmentation. Pressing <code>Segment Object</code> (or <code>s</code>) will run segmentation for the current prompts. The result is displayed in <code>current_object</code></li>\n<li>The menu for commiting the segmentation. When pressing <code>Commit</code> (or <code>c</code>) the result from the selected layer (either <code>current_object</code> or <code>auto_segmentation</code>) will be transferred from the respective layer to <code>committed_objects</code>.</li>\n<li>The menu for clearing the current annotations. Pressing <code>Clear Annotations</code> (or <code>shift c</code>) will clear the current annotations and the current segmentation.</li>\n</ol>\n\n<p>Note that point prompts and box prompts can be combined. When you're using point prompts you can only segment one object at a time. With box prompts you can segment several objects at once.</p>\n\n<p>Check out <a href=\"https://youtu.be/ket7bDUP9tI\">this video</a> for a tutorial for the 2d annotation tool.</p>\n\n<p>We also provide the <code>image series annotator</code>, which can be used for running the 2d annotator for several images in a folder. You can start by clicking <code>Image series annotator</code> in the GUI, running <code>micro_sam.image_series_annotator</code> in the command line or from a <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/image_series_annotator.py\">python script</a>.</p>\n\n<h2 id=\"annotator-3d\">Annotator 3D</h2>\n\n<p>The 3d annotator can be started by</p>\n\n<ul>\n<li>clicking <code>3d annotator</code> in the <code>micro_sam</code> GUI.</li>\n<li>running <code>$ micro_sam.annotator_3d</code> in the command line. Run <code>micro_sam.annotator_3d -h</code> for details.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_3d</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_3d.py\">examples/annotator_3d.py</a> for details.</li>\n</ul>\n\n<p>The user interface of the 3d annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/3d-annotator-menu.png\" width=\"768\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the image, segmentation and prompts. Same as for <a href=\"#annotator-2d\">the 2d annotator</a> but without the <code>auto_segmentation</code> layer.</li>\n<li>The prompt menu.</li>\n<li>The menu for interactive segmentation.</li>\n<li>The 3d segmentation menu. Pressing <code>Segment All Slices</code> (or <code>Shift-S</code>) will extend the segmentation for the current object across the volume.</li>\n<li>The menu for committing the segmentation.</li>\n<li>The menu for clearing the current annotations.</li>\n</ol>\n\n<p>Note that you can only segment one object at a time with the 3d annotator.</p>\n\n<p>Check out <a href=\"https://youtu.be/PEy9-rTCdS4\">this video</a> for a tutorial for the 3d annotation tool.</p>\n\n<h2 id=\"annotator-tracking\">Annotator Tracking</h2>\n\n<p>The tracking annotator can be started by</p>\n\n<ul>\n<li>clicking <code>Tracking annotator</code> in the <code>micro_sam</code> GUI.</li>\n<li>running <code>$ micro_sam.annotator_tracking</code> in the command line. Run <code>micro_sam.annotator_tracking -h</code> for details.</li>\n<li>calling <code>micro_sam.sam_annotator.annotator_tracking</code> in a python script. Check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_tracking.py\">examples/annotator_tracking.py</a> for details. </li>\n</ul>\n\n<p>The user interface of the tracking annotator looks like this:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/tracking-annotator-menu.png\" width=\"768\"></p>\n\n<p>Most elements are the same as in <a href=\"#annotator-2d\">the 2d annotator</a>:</p>\n\n<ol>\n<li>The napari layers that contain the image, segmentation and prompts. Same as for <a href=\"#annotator-2d\">the 2d segmentation app</a> but without the <code>auto_segmentation</code> layer, <code>current_tracks</code> and <code>committed_tracks</code> are the equivalent of <code>current_object</code> and <code>committed_objects</code>.</li>\n<li>The prompt menu.</li>\n<li>The menu with tracking settings: <code>track_state</code> is used to indicate that the object you are tracking is dividing in the current frame. <code>track_id</code> is used to select which of the tracks after division you are following.</li>\n<li>The menu for interactive segmentation.</li>\n<li>The tracking menu. Press <code>Track Object</code> (or <code>Shift-S</code>) to segment the current object across time.</li>\n<li>The menu for committing the current tracking result.</li>\n<li>The menu for clearing the current annotations.</li>\n</ol>\n\n<p>Note that the tracking annotator only supports 2d image data, volumetric data is not supported.</p>\n\n<p>Check out <a href=\"https://youtu.be/Xi5pRWMO6_w\">this video</a> for a tutorial for how to use the tracking annotation tool.</p>\n\n<h2 id=\"tips-tricks\">Tips &amp; Tricks</h2>\n\n<ul>\n<li>Segment Anything was trained with a fixed image size of 1024 x 1024 pixels. Inputs that do not match this size will be internally resized to match it. Hence, applying Segment Anything to a much larger image will often lead to inferior results, because it will be downsampled by a large factor and the objects in the image become too small.\nTo address this image we implement tiling: cutting up the input image into tiles of a fixed size (with a fixed overlap) and running Segment Anything for the individual tiles.\nYou can activate tiling by passing the parameters <code>tile_shape</code>, which determines the size of the inner tile and <code>halo</code>, which determines the size of the additional overlap.\n<ul>\n<li>If you're using the <code>micro_sam</code> GUI you can specify the values for the <code>halo</code> and <code>tile_shape</code> via the <code>Tile X</code>, <code>Tile Y</code>, <code>Halo X</code> and <code>Halo Y</code>.</li>\n<li>If you're using a python script you can pass them as tuples, e.g. <code>tile_shape=(1024, 1024), halo=(128, 128)</code>. See also <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/0921581e2964139194d235a87cb002d3f3667f45/examples/annotator_2d.py#L40\">the wholeslide_annotator example</a>.</li>\n<li>If you're using the command line functions you can pass them via the options <code>--tile_shape 1024 1024 --halo 128 128</code></li>\n<li>Note that prediction with tiling only works when the embeddings are cached to file, so you must specify an <code>embedding_path</code> (<code>-e</code> in the CLI).</li>\n<li>You should choose the <code>halo</code> such that it is larger than half of the maximal radius of the objects your segmenting.</li>\n</ul></li>\n<li>The applications pre-compute the image embeddings produced by SegmentAnything and (optionally) store them on disc. If you are using a CPU this step can take a while for 3d data or timeseries (you will see a progress bar with a time estimate). If you have access to a GPU without graphical interface (e.g. via a local computer cluster or a cloud provider), you can also pre-compute the embeddings there and then copy them to your laptop / local machine to speed this up. You can use the command <code>micro_sam.precompute_embeddings</code> for this (it is installed with the rest of the applications). You can specify the location of the precomputed embeddings via the <code>embedding_path</code> argument.</li>\n<li>Most other processing steps are very fast even on a CPU, so interactive annotation is possible. An exception is the automatic segmentation step (2d segmentation), which takes several minutes without a GPU (depending on the image size). For large volumes and timeseries segmenting an object in 3d / tracking across time can take a couple settings with a CPU (it is very fast with a GPU).</li>\n<li>You can also try using a smaller version of the SegmentAnything model to speed up the computations. For this you can pass the <code>model_type</code> argument and either set it to <code>vit_b</code> or to <code>vit_l</code> (default is <code>vit_h</code>). However, this may lead to worse results.</li>\n<li>You can save and load the results from the <code>committed_objects</code> / <code>committed_tracks</code> layer to correct segmentations you obtained from another tool (e.g. CellPose) or to save intermediate annotation results. The results can be saved via <code>File -&gt; Save Selected Layer(s) ...</code> in the napari menu (see the tutorial videos for details). They can be loaded again by specifying the corresponding location via the <code>segmentation_result</code> (2d and 3d segmentation) or <code>tracking_result</code> (tracking) argument.</li>\n</ul>\n\n<h2 id=\"known-limitations\">Known limitations</h2>\n\n<ul>\n<li>Segment Anything does not work well for very small or fine-grained objects (e.g. filaments).</li>\n<li>For the automatic segmentation functionality we currently rely on the automatic mask generation provided by SegmentAnything. It is slow and often misses objects in microscopy images. For now, we only offer this functionality in the 2d segmentation app; we are working on improving it and extending it to 3d segmentation and tracking.</li>\n<li>Prompt bounding boxes do not provide the full functionality for tracking yet (they cannot be used for divisions or for starting new tracks). See also <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/23\">this github issue</a>.</li>\n</ul>\n\n<h1 id=\"how-to-use-the-python-library\">How to use the Python Library</h1>\n\n<p>The python library can be imported via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam</span>\n</code></pre>\n</div>\n\n<p>The library</p>\n\n<ul>\n<li>implements function to apply Segment Anything to 2d and 3d data more conveniently in <code>micro_sam.prompt_based_segmentation</code>.</li>\n<li>provides more and improved automatic instance segmentation functionality in <code>micro_sam.instance_segmentation</code>.</li>\n<li>implements training functionality that can be used for finetuning on your own data in <code>micro_sam.training</code>.</li>\n<li>provides functionality for quantitative and qualitative evaluation of Segment Anything models in <code>micro_sam.evaluation</code>.</li>\n</ul>\n\n<p>You can import these sub-modules via</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span> <span class=\"nn\">micro_sam.prompt_based_segmentation</span>\n<span class=\"kn\">import</span> <span class=\"nn\">micro_sam.instance_segmentation</span>\n<span class=\"c1\"># etc.</span>\n</code></pre>\n</div>\n\n<p>This functionality is used to implement the interactive annotation tools and can also be used as a standalone python library.\nSome preliminary examples for how to use the python library can be found <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/use_as_library\">here</a>. Check out the <code>Submodules</code> documentation for more details.</p>\n\n<h2 id=\"training-your-own-model\">Training your own model</h2>\n\n<p>We reimplement the training logic described in the <a href=\"https://arxiv.org/abs/2304.02643\">Segment Anything publication</a> to enable finetuning on custom data.\nWe use this functionality to provide the <a href=\"#finetuned-models\">finetuned microscopy models</a> and it can also be used to finetune models on your own data.\nIn fact the best results can be expected when finetuning on your own data, and we found that it does not require much annotated training data to get siginficant improvements in model performance.\nSo a good strategy is to annotate a few images with one of the provided models using one of the interactive annotation tools and, if the annotation is not working as good as expected yet, finetune on the annotated data.\n<!--\nTODO: provide link to the paper with results on how much data is needed\n--></p>\n\n<p>The training logic is implemented in <code>micro_sam.training</code> and is based on <a href=\"https://github.com/constantinpape/torch-em\">torch-em</a>. Please check out <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/examples/finetuning\">examples/finetuning</a> to see how you can finetune on your own data with it. The script <code>finetune_hela.py</code> contains an example for finetuning on a small custom microscopy dataset and <code>use_finetuned_model.py</code> shows how this model can then be used in the interactive annotation tools.</p>\n\n<p>Since release v0.4.0 we also support training an additional decoder for automatic instance segmentation. This yields better results than the automatic mask generation of segment anything and is significantly faster.\nYou can enable training of the decoder by setting <code>train_instance_segmentation = True</code> <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/finetuning/finetune_hela.py#L165\">here</a>.\nThe script <code>instance_segmentation_with_finetuned_model.py</code> shows how to use it for automatic instance segmentation.\nWe will fully integrate this functionality with the annotation tool in the next release.</p>\n\n<p>More advanced examples, including quantitative and qualitative evaluation, of finetuned models can be found in <a href=\"https://github.com/computational-cell-analytics/micro-sam/tree/master/finetuning\">finetuning</a>, which contains the code for training and evaluating our microscopy models.</p>\n\n<h1 id=\"finetuned-models\">Finetuned models</h1>\n\n<p>In addition to the original Segment anything models, we provide models that finetuned on microscopy data using the functionality from <code>micro_sam.training</code>.\nThe models are hosted on zenodo. We currently offer the following models:</p>\n\n<ul>\n<li><code>vit_h</code>: Default Segment Anything model with vit-h backbone.</li>\n<li><code>vit_l</code>: Default Segment Anything model with vit-l backbone.</li>\n<li><code>vit_b</code>: Default Segment Anything model with vit-b backbone.</li>\n<li><code>vit_t</code>: Segment Anything model with vit-tiny backbone. From the <a href=\"https://arxiv.org/abs/2306.14289\">Mobile SAM publication</a>. </li>\n<li><code>vit_b_lm</code>: Finetuned Segment Anything model for cells and nuclei in light microscopy data with vit-b backbone.</li>\n<li><code>vit_b_em_organelles</code>: Finetuned Segment Anything model for mitochodria and nuclei in electron microscopy data with vit-b backbone.</li>\n<li><code>vit_b_em_boundaries</code>: Finetuned Segment Anything model for neurites and cells in electron microscopy data with vit-b backbone.</li>\n</ul>\n\n<p>See the two figures below of the improvements through the finetuned model for LM and EM data. </p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/lm_comparison.png\" width=\"768\"></p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/em_comparison.png\" width=\"768\"></p>\n\n<p>You can select which of the models is used in the annotation tools by selecting the corresponding name from the <code>Model Type</code> menu:</p>\n\n<p><img src=\"https://raw.githubusercontent.com/computational-cell-analytics/micro-sam/master/doc/images/model-type-selector.png\" width=\"256\"></p>\n\n<p>To use a specific model in the python library you need to pass the corresponding name as value to the <code>model_type</code> parameter exposed by all relevant functions.\nSee for example the <a href=\"https://github.com/computational-cell-analytics/micro-sam/blob/master/examples/annotator_2d.py#L62\">2d annotator example</a> where <code>use_finetuned_model</code> can be set to <code>True</code> to use the <code>vit_b_lm</code> model.</p>\n\n<p>Note that we are still working on improving these models and may update them from time to time. All older models will stay available for download on zenodo, see <a href=\"#model-sources\">model sources</a> below</p>\n\n<h2 id=\"which-model-should-i-choose\">Which model should I choose?</h2>\n\n<p>As a rule of thumb:</p>\n\n<ul>\n<li>Use the <code>vit_b_lm</code> model for segmenting cells or nuclei in light microscopy.</li>\n<li>Use the <code>vit_b_em_organelles</code> models for segmenting mitochondria, nuclei or other organelles in electron microscopy.</li>\n<li>Use the <code>vit_b_em_boundaries</code> models for segmenting cells or neurites in electron microscopy.</li>\n<li>For other use-cases use one of the default models.</li>\n</ul>\n\n<p>See also the figures above for examples where the finetuned models work better than the vanilla models.\nCurrently the model <code>vit_h</code> is used by default.</p>\n\n<p>We are working on further improving these models and adding new models for other biomedical imaging domains.</p>\n\n<h2 id=\"model-sources\">Model Sources</h2>\n\n<p>Here is an overview of all finetuned models we have released to zenodo so far:</p>\n\n<ul>\n<li><a href=\"https://zenodo.org/records/10524894\">vit_b_em_boundaries</a>: for segmenting compartments delineated by boundaries such as cells or neurites in EM.</li>\n<li><a href=\"https://zenodo.org/records/10524828\">vit_b_em_organelles</a>: for segmenting mitochondria, nuclei or other organelles in EM.</li>\n<li><a href=\"https://zenodo.org/records/10524791\">vit_b_lm</a>: for segmenting cells and nuclei in LM.</li>\n<li><a href=\"https://zenodo.org/records/8250291\">vit_h_em</a>: this model is outdated.</li>\n<li><a href=\"https://zenodo.org/records/8250299\">vit_h_lm</a>: this model is outdated.</li>\n</ul>\n\n<p>Some of these models contain multiple versions.</p>\n\n<h1 id=\"how-to-contribute\">How to contribute</h1>\n\n<ul>\n<li><a href=\"#discuss-your-ideas\">Discuss your ideas</a></li>\n<li><a href=\"#clone-the-repository\">Clone the repository</a></li>\n<li><a href=\"#create-your-development-environment\">Create your development environment</a></li>\n<li><a href=\"#make-your-changes\">Make your changes</a></li>\n<li><a href=\"#testing\">Testing</a>\n<ul>\n<li><a href=\"#run-the-tests\">Run the tests</a></li>\n<li><a href=\"#writing-your-own-tests\">Writing your own tests</a></li>\n</ul></li>\n<li><a href=\"#open-a-pull-request\">Open a pull request</a></li>\n<li><a href=\"#optional-build-the-documentation\">Optional: Build the documentation</a></li>\n<li><a href=\"#optional-benchmark-performance\">Optional: Benchmark performance</a>\n<ul>\n<li><a href=\"#run-the-benchmark-script\">Run the benchmark script</a></li>\n<li><a href=\"#line-profiling\">Line profiling</a></li>\n<li><a href=\"#snakeviz-visualization\">Snakeviz visualization</a></li>\n<li><a href=\"#memory-profiling-with-memray\">Memory profiling with memray</a></li>\n</ul></li>\n</ul>\n\n<h2 id=\"discuss-your-ideas\">Discuss your ideas</h2>\n\n<p>We welcome new contributions!</p>\n\n<p>First, discuss your idea by opening a <a href=\"https://github.com/computational-cell-analytics/micro-sam/issues/new\">new issue</a> in micro-sam.</p>\n\n<p>This allows you to ask questions, and have the current developers make suggestions about the best way to implement your ideas.</p>\n\n<p>You may also find it helpful to look at this <a href=\"#for-developers\">developer guide</a>, which explains the organization of the micro-sam code.</p>\n\n<h2 id=\"clone-the-repository\">Clone the repository</h2>\n\n<p>We use <a href=\"https://git-scm.com/\">git</a> for version control.</p>\n\n<p>Clone the repository, and checkout the development branch:</p>\n\n<pre><code>git clone https://github.com/computational-cell-analytics/micro-sam.git\ncd micro-sam\ngit checkout dev\n</code></pre>\n\n<h2 id=\"create-your-development-environment\">Create your development environment</h2>\n\n<p>We use <a href=\"https://docs.conda.io/en/latest/\">conda</a> to <a href=\"https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\">manage our environments</a>. If you don't have this already, install <a href=\"https://docs.conda.io/projects/miniconda/en/latest/\">miniconda</a> or <a href=\"https://mamba.readthedocs.io/en/latest/\">mamba</a> to get started.</p>\n\n<p>Now you can create the environment, install user and develoepr dependencies, and micro-sam as an editable installation:</p>\n\n<pre><code>conda env create environment-gpu.yml\nconda activate sam\npython -m pip install requirements-dev.txt\npython -m pip install -e .\n</code></pre>\n\n<h2 id=\"make-your-changes\">Make your changes</h2>\n\n<p>Now it's time to make your code changes.</p>\n\n<p>Typically, changes are made branching off from the development branch. Checkout <code>dev</code> and then create a new branch to work on your changes.</p>\n\n<pre><code>git checkout dev\ngit checkout -b my-new-feature\n</code></pre>\n\n<p>We use <a href=\"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\">google style python docstrings</a> to create documentation for all new code.</p>\n\n<p>You may also find it helpful to look at this <a href=\"#for-developers\">developer guide</a>, which explains the organization of the micro-sam code.</p>\n\n<h2 id=\"testing\">Testing</h2>\n\n<h3 id=\"run-the-tests\">Run the tests</h3>\n\n<p>The tests for micro-sam are run with <a href=\"https://docs.pytest.org/en/7.4.x/\">pytest</a></p>\n\n<p>To run the tests:</p>\n\n<pre><code>pytest\n</code></pre>\n\n<h3 id=\"writing-your-own-tests\">Writing your own tests</h3>\n\n<p>If you have written new code, you will need to write tests to go with it.</p>\n\n<h4 id=\"unit-tests\">Unit tests</h4>\n\n<p>Unit tests are the preferred style of tests for user contributions. Unit tests check small, isolated parts of the code for correctness. If your code is too complicated to write unit tests easily, you may need to consider breaking it up into smaller functions that are easier to test.</p>\n\n<h4 id=\"tests-involving-napari\">Tests involving napari</h4>\n\n<p>In cases where tests <em>must</em> use the napari viewer, <a href=\"https://napari.org/stable/plugins/test_deploy.html#tips-for-testing-napari-plugins\">these tips might be helpful</a> (in particular, the <code>make_napari_viewer_proxy</code> fixture).</p>\n\n<p>These kinds of tests should be used only in limited circumstances. Developers are <a href=\"https://napari.org/stable/plugins/test_deploy.html#prefer-smaller-unit-tests-when-possible\">advised to prefer smaller unit tests, and avoid integration tests</a> wherever possible.</p>\n\n<h4 id=\"code-coverage\">Code coverage</h4>\n\n<p>Pytest uses the <a href=\"https://pytest-cov.readthedocs.io/en/latest/\">pytest-cov</a> plugin to automatically determine which lines of code are covered by tests.</p>\n\n<p>A short summary report is printed to the terminal output whenever you run pytest. The full results are also automatically written to a file named <code>coverage.xml</code>.</p>\n\n<p>The <a href=\"https://marketplace.visualstudio.com/items?itemName=ryanluker.vscode-coverage-gutters\">Coverage Gutters VSCode extension</a> is useful for visualizing which parts of the code need better test coverage. PyCharm professional <a href=\"https://www.jetbrains.com/pycharm/guide/tips/spot-coverage-in-gutter/\">has a similar feature</a>, and you may be able to find similar tools for your preferred editor.</p>\n\n<p>We also use <a href=\"https://app.codecov.io/gh/computational-cell-analytics/micro-sam\">codecov.io</a> to display the code coverage results from our Github Actions continuous integration.</p>\n\n<h2 id=\"open-a-pull-request\">Open a pull request</h2>\n\n<p>Once you've made changes to the code and written some tests to go with it, you are ready to <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests\">open a pull request</a>. You can <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-requests\">mark your pull request as a draft</a> if you are still working on it, and still get the benefit of discussing the best approach with maintainers.</p>\n\n<p>Remember that typically changes to micro-sam are made branching off from the development branch. So, you will need to open your pull request to merge back into the <code>dev</code> branch <a href=\"https://github.com/computational-cell-analytics/micro-sam/compare/dev...dev\">like this</a>.</p>\n\n<h2 id=\"optional-build-the-documentation\">Optional: Build the documentation</h2>\n\n<p>We use <a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> to build the documentation.</p>\n\n<p>To build the documentation locally, run this command:</p>\n\n<pre><code>python build_doc.py\n</code></pre>\n\n<p>This will start a local server and display the HTML documentation. Any changes you make to the documentation will be updated in real time (you may need to refresh your browser to see the changes).</p>\n\n<p>If you want to save the HTML files, append <code>--out</code> to the command, like this:</p>\n\n<pre><code>python build_doc.py --out\n</code></pre>\n\n<p>This will save the HTML files into a new directory named <code>tmp</code>.</p>\n\n<p>You can add content to the documentation in two ways:</p>\n\n<ol>\n<li>By adding or updating <a href=\"https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html\">google style python docstrings</a> in the micro-sam code.\n<ul>\n<li><a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> will automatically find and include docstrings in the documentation.</li>\n</ul></li>\n<li>By adding or editing markdown files in the micro-sam <code>doc</code> directory.\n<ul>\n<li>If you add a new markdown file to the documentation, you must tell <a href=\"https://pdoc.dev/docs/pdoc.html\">pdoc</a> that it exists by adding a line to the <code>micro_sam/__init__.py</code> module docstring (eg: <code>.. include:: ../doc/my_amazing_new_docs_page.md</code>). Otherwise it will not be included in the final documentation build!</li>\n</ul></li>\n</ol>\n\n<h2 id=\"optional-benchmark-performance\">Optional: Benchmark performance</h2>\n\n<p>There are a number of options you can use to benchmark performance, and identify problems like slow run times or high memory use in micro-sam.</p>\n\n<ul>\n<li><a href=\"#run-the-benchmark-script\">Run the benchmark script</a></li>\n<li><a href=\"#line-profiling\">Line profiling</a></li>\n<li><a href=\"#snakeviz-visualization\">Snakeviz visualization</a></li>\n<li><a href=\"#memory-profiling-with-memray\">Memory profiling with memray</a></li>\n</ul>\n\n<h3 id=\"run-the-benchmark-script\">Run the benchmark script</h3>\n\n<p>There is a performance benchmark script available in the micro-sam repository at <code>development/benchmark.py</code>.</p>\n\n<p>To run the benchmark script:</p>\n\n<pre><code>python development/benchmark.py --model_type vit_t --device cpu`\n</code></pre>\n\n<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>\n\n<pre><code>python development/benchmark.py --help\n</code></pre>\n\n<h3 id=\"line-profiling\">Line profiling</h3>\n\n<p>For more detailed line by line performance results, we can use <a href=\"https://github.com/pyutils/line_profiler\">line-profiler</a>.</p>\n\n<blockquote>\n  <p><a href=\"https://github.com/pyutils/line_profiler\">line_profiler</a> is a module for doing line-by-line profiling of functions. kernprof is a convenient script for running either line_profiler or the Python standard library's cProfile or profile modules, depending on what is available.</p>\n</blockquote>\n\n<p>To do line-by-line profiling:</p>\n\n<ol>\n<li>Ensure you have line profiler installed: <code>python -m pip install line_profiler</code></li>\n<li>Add <code>@profile</code> decorator to any function in the call stack</li>\n<li>Run <code>kernprof -lv benchmark.py --model_type vit_t --device cpu</code></li>\n</ol>\n\n<p>For more details about how to use line-profiler and kernprof, see <a href=\"https://kernprof.readthedocs.io/en/latest/\">the documentation</a>.</p>\n\n<p>For more details about the user input arguments for the micro-sam benchmark script, see the help:</p>\n\n<pre><code>python development/benchmark.py --help\n</code></pre>\n\n<h3 id=\"snakeviz-visualization\">Snakeviz visualization</h3>\n\n<p>For more detailed visualizations of profiling results, we use <a href=\"https://jiffyclub.github.io/snakeviz/\">snakeviz</a>.</p>\n\n<blockquote>\n  <p>SnakeViz is a browser based graphical viewer for the output of Python\u2019s cProfile module</p>\n</blockquote>\n\n<ol>\n<li>Ensure you have snakeviz installed: <code>python -m pip install snakeviz</code></li>\n<li>Generate profile file: <code>python -m cProfile -o program.prof benchmark.py --model_type vit_h --device cpu</code></li>\n<li>Visualize profile file: <code>snakeviz program.prof</code></li>\n</ol>\n\n<p>For more details about how to use snakeviz, see <a href=\"https://jiffyclub.github.io/snakeviz/\">the documentation</a>.</p>\n\n<h3 id=\"memory-profiling-with-memray\">Memory profiling with memray</h3>\n\n<p>If you need to investigate memory use specifically, we use <a href=\"https://github.com/bloomberg/memray\">memray</a>.</p>\n\n<blockquote>\n  <p>Memray is a memory profiler for Python. It can track memory allocations in Python code, in native extension modules, and in the Python interpreter itself. It can generate several different types of reports to help you analyze the captured memory usage data. While commonly used as a CLI tool, it can also be used as a library to perform more fine-grained profiling tasks.</p>\n</blockquote>\n\n<p>For more details about how to use memray, see <a href=\"https://bloomberg.github.io/memray/getting_started.html\">the documentation</a>.</p>\n\n<h1 id=\"for-developers\">For Developers</h1>\n\n<p>This software consists of four different python (sub-)modules:</p>\n\n<ul>\n<li>The top-level <code>micro_sam</code> module implements general purpose functionality for using Segment Anything for multi-dimensional data.</li>\n<li><code>micro_sam.evaluation</code> provides functionality to evaluate Segment Anything models on (microscopy) segmentation tasks.</li>\n<li><code>micro_sam.traning</code> implements the training functionality to finetune Segment Anything for custom segmentation datasets.</li>\n<li><code>micro_sam.sam_annotator</code> implements the interactive annotation tools.</li>\n</ul>\n\n<h2 id=\"annotation-tools-2\">Annotation Tools</h2>\n\n<p>The annotation tools are implemented as napari plugins.</p>\n\n<p>There are four annotation tools:</p>\n\n<ul>\n<li><code>micro_sam.sam_annotator.annotator_2d</code>: for interactive segmentation of 2d images.</li>\n<li><code>micro_sam.sam_annotator.annotator_3d</code>: for interactive segmentation of volumetric images.</li>\n<li><code>micro_sam.sam_annotator.annotator_tracking</code>: for interactive tracking in timeseries of 2d images.</li>\n<li><code>micro_sam.sam_annotator.image_series_annotator</code>: for applying the 2d annotation tool to a series of images. This is not implemented as a separate plugin, but as a function that runns annotator 2d for multiple images.</li>\n</ul>\n\n<p>An overview of the functionality of the different tools:</p>\n\n<table>\n<thead>\n<tr>\n  <th>Functionality</th>\n  <th>annotator_2d</th>\n  <th>annotator_3d</th>\n  <th>annotator_tracking</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n  <td>Interactive segmentation</td>\n  <td>Yes</td>\n  <td>Yes</td>\n  <td>Yes</td>\n</tr>\n<tr>\n  <td>Interactive segmentation for multiple objects at a time</td>\n  <td>Yes</td>\n  <td>No</td>\n  <td>No</td>\n</tr>\n<tr>\n  <td>Interactive 3d segmentation via projection</td>\n  <td>No</td>\n  <td>Yes</td>\n  <td>Yes</td>\n</tr>\n<tr>\n  <td>Support for dividing objects</td>\n  <td>No</td>\n  <td>No</td>\n  <td>Yes</td>\n</tr>\n<tr>\n  <td>Automatic segmentation</td>\n  <td>Yes</td>\n  <td>Yes</td>\n  <td>No</td>\n</tr>\n</tbody>\n</table>\n\n<p>The functionality for <code>image_series_annotator</code> is not listed because it is identical with the functionality of <code>annotator_2d</code>.</p>\n\n<p>Each tool implements the follwing core logic:</p>\n\n<ol>\n<li>The image embeddings (prediction from SAM image encoder) are pre-computed for the input data (2d image, image volume or timeseries). These embeddings can be cached to a zarr file.</li>\n<li>Interactive (and automatic) segmentation functionality is implemented by a UI based on <code>napari</code> and <code>magicgui</code> functionality.</li>\n</ol>\n\n<p>Each tool has three different entry points:</p>\n\n<ul>\n<li>From napari plugin menu, e.g. <code>plugin-&gt;micro_sam-&gt;Annotator 2d</code>. (Called <em>plugin</em> in the following).</li>\n<li>From a python function, e.g. <code>micro_sam.sam_annotator.annotator_2d:annotator_2d</code>.  (Called <em>function</em> in the following.)</li>\n<li>From the command line, e.g. <code>micro_sam.annotator_2d</code>. (Called <em>CLI</em> in the following).</li>\n</ul>\n\n<p>Each tool is implemented in its own submodule, e.g. <code>micro_sam.sam_annotator.annotator_2d</code>.\nThe napari plugin is implemented by a class, e.g. <code>micro_sam.sam_annotator.annotator_2d:Annotator2d</code>, inheriting from <code>micro_sam.sam_annotator._annotator._AnnotatorBase</code>. This class implements the core logic for the plugins.\nThe concrete annotation tools are instantiated by passing widgets from <code>micro_sam.sam_annotator._widgets</code> to it, \nwhich implement the interactive segmentation in 2d, 3d etc.\nThese plugins are designed so that image embeddings can be computed for user-specified image layers in napari.</p>\n\n<p>The <em>function</em> and <em>CLI</em> entry points are implemented by <code>micro_sam.sam_annotator.annotator_2d:annotator_2d</code> (and corresponding functions for the other annotation tools). They are called with image data, precompute the embeddings for that image and start a napari viewer with this image and the annotation plugin.</p>\n\n<p><!--\nTODO update the flow chart so that it matches the new design.\nThe same overall design holds true for the other plugins. The flow chart below shows a flow chart with a simplified overview of the design of the 2d annotation tool. Rounded squares represent functions or the corresponding widget and squares napari layers or other data, orange represents the <em>plugin</em> enty point, cyan <em>CLI</em>. Arrows that do not have a label correspond to a simple input/output relation.</p>\n\n<p><img src=\"./images/2d-annotator-flow.png\" alt=\"annotator 2d flow diagram\" />\n--></p>\n\n<p><!---\nSource for the diagram is here:\n<a href=\"https://docs.google.com/presentation/d/1fMDNBYMYxeqe4dk6OmmFxoI8sYvCu4EPZS_LyTsTg_s/edit#slide=id.p\">https://docs.google.com/presentation/d/1fMDNBYMYxeqe4dk6OmmFxoI8sYvCu4EPZS_LyTsTg_s/edit#slide=id.p</a>\n--></p>\n\n<h1 id=\"using-micro_sam-on-band\">Using micro_sam on BAND</h1>\n\n<p>BAND is a service offered by EMBL Heidelberg that gives access to a virtual desktop for image analysis tasks. It is free to use and <code>micro_sam</code> is installed there.\nIn order to use BAND and start <code>micro_sam</code> on it follow these steps:</p>\n\n<h2 id=\"start-band\">Start BAND</h2>\n\n<ul>\n<li>Go to <a href=\"https://band.embl.de/\">https://band.embl.de/</a> and click <strong>Login</strong>. If you have not used BAND before you will need to register for BAND. Currently you can only sign up via a google account.</li>\n<li>Launch a BAND desktop with sufficient resources. It's particularly important to select a GPU. The settings from the image below are a good choice.</li>\n<li>Go to the desktop by clicking <strong>GO TO DESKTOP</strong> in the <strong>Running Desktops</strong> menu. See also the screenshot below.</li>\n</ul>\n\n<p><img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/f965fce2-b924-4fc8-871b-f3201e502138\" alt=\"image\" /></p>\n\n<h2 id=\"start-micro_sam-in-band\">Start micro_sam in BAND</h2>\n\n<ul>\n<li>Select <strong>Applications->Image Analysis->uSAM</strong> (see screenshot)\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5daeafb3-119b-4104-8708-aab2960cb21c\" alt=\"image\" /></li>\n<li>This will open the micro_sam menu, where you can select the tool you want to use (see screenshot). Note: this may take a few minutes.\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/900ce0b9-4cf8-418c-94f1-e99ac7bc0086\" alt=\"image\" /></li>\n<li>For testing if the tool works, it's best to use the <strong>2d annotator</strong> first.\n<ul>\n<li>You can find an example image to use here: <code>/scratch/cajal-connectomics/hela-2d-image.png</code>. Select it via <strong>Select image</strong>. (see screenshot)\n<img src=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/5fbd1c53-2ba1-47d4-ae50-dfab890ac9d3\" alt=\"image\" /></li>\n</ul></li>\n<li>Then press <strong>2d annotator</strong> and the tool will start.</li>\n</ul>\n\n<h2 id=\"transfering-data-to-band\">Transfering data to BAND</h2>\n\n<p>To copy data to and from BAND you can use any cloud storage, e.g. ownCloud, dropbox or google drive. For this, it's important to note that copy and paste, which you may need for accessing links on BAND, works a bit different in BAND:</p>\n\n<ul>\n<li>To copy text into BAND you first need to copy it on your computer (e.g. via selecting it + <code>ctrl + c</code>).</li>\n<li>Then go to the browser window with BAND and press <code>ctrl + shift + alt</code>. This will open a side window where you can paste your text via <code>ctrl + v</code>.</li>\n<li>Then select the text in this window and copy it via <code>ctrl + c</code>.</li>\n<li>Now you can close the side window via <code>ctrl + shift + alt</code> and paste the text in band via <code>ctrl + v</code></li>\n</ul>\n\n<p>The video below shows how to copy over a link from owncloud and then download the data on BAND using copy and paste:</p>\n\n<p><a href=\"https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287\">https://github.com/computational-cell-analytics/micro-sam/assets/4263537/825bf86e-017e-41fc-9e42-995d21203287</a></p>\n"}, {"fullname": "micro_sam.bioimageio", "modulename": "micro_sam.bioimageio", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.model_export", "modulename": "micro_sam.bioimageio.model_export", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.model_export.DEFAULTS", "modulename": "micro_sam.bioimageio.model_export", "qualname": "DEFAULTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;authors&#x27;: [Author(affiliation=&#x27;University Goettingen&#x27;, email=None, orcid=None, name=&#x27;Anwai Archit&#x27;, github_user=&#x27;anwai98&#x27;), Author(affiliation=&#x27;University Goettingen&#x27;, email=None, orcid=None, name=&#x27;Constantin Pape&#x27;, github_user=&#x27;constantinpape&#x27;)], &#x27;description&#x27;: &#x27;Finetuned Segment Anything Model for Microscopy&#x27;, &#x27;cite&#x27;: [CiteEntry(text=&#x27;Archit et al. Segment Anything for Microscopy&#x27;, doi=&#x27;10.1101/2023.08.21.554208&#x27;, url=None)], &#x27;tags&#x27;: [&#x27;segment-anything&#x27;, &#x27;instance-segmentation&#x27;]}"}, {"fullname": "micro_sam.bioimageio.model_export.export_sam_model", "modulename": "micro_sam.bioimageio.model_export", "qualname": "export_sam_model", "kind": "function", "doc": "<p>Export SAM model to BioImage.IO model format.</p>\n\n<p>The exported model can be uploaded to <a href=\"https://bioimage.io/#/\">bioimage.io</a> and\nbe used in tools that support the BioImage.IO model format.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The image for generating test data.</li>\n<li><strong>label_image:</strong>  The segmentation correspoding to <code>image</code>.\nIt is used to derive prompt inputs for the model.</li>\n<li><strong>model_type:</strong>  The type of the SAM model.</li>\n<li><strong>name:</strong>  The name of the exported model.</li>\n<li><strong>output_path:</strong>  Where the exported model is saved.</li>\n<li><strong>checkpoint_path:</strong>  Optional checkpoint for loading the SAM model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">label_image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor", "modulename": "micro_sam.bioimageio.predictor_adaptor", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor", "kind": "class", "doc": "<p>Wrapper around the SamPredictor.</p>\n\n<p>This model supports the same functionality as SamPredictor and can provide mask segmentations\nfrom box, point or mask input prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The type of the model for the image encoder.\nCan be one of 'vit_b', 'vit_l', 'vit_h' or 'vit_t'.\nFor 'vit_t' support the 'mobile_sam' package has to be installed.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.__init__", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span>)</span>"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.sam", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.load_state_dict", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.load_state_dict", "kind": "function", "doc": "<p>Copies parameters and buffers from <code>state_dict</code> into\nthis module and its descendants. If <code>strict</code> is <code>True</code>, then\nthe keys of <code>state_dict</code> must exactly match the keys returned\nby this module's <code>~torch.nn.Module.state_dict()</code> function.</p>\n\n<div class=\"pdoc-alert pdoc-alert-warning\">\n\n<p>If <code>assign</code> is <code>True</code> the optimizer must be created after\nthe call to <code>load_state_dict</code>.</p>\n\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state_dict (dict):</strong>  a dict containing parameters and\npersistent buffers.</li>\n<li><strong>strict (bool, optional):</strong>  whether to strictly enforce that the keys\nin <code>state_dict</code> match the keys returned by this module's\n<code>~torch.nn.Module.state_dict()</code> function. Default: <code>True</code></li>\n<li><strong>assign (bool, optional):</strong>  whether to assign items in the state\ndictionary to their corresponding keys in the module instead\nof copying them inplace into the module's current parameters and buffers.\nWhen <code>False</code>, the properties of the tensors in the current\nmodule are preserved while when <code>True</code>, the properties of the\nTensors in the state dict are preserved.\nDefault: <code>False</code></li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p><code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:\n      * <strong>missing_keys</strong> is a list of str containing the missing keys\n      * <strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p>\n</blockquote>\n\n<h6 id=\"note\">Note:</h6>\n\n<blockquote>\n  <p>If a parameter or buffer is registered as <code>None</code> and its corresponding key\n  exists in <code>state_dict</code>, <code>load_state_dict()</code> will raise a\n  <code>RuntimeError</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.bioimageio.predictor_adaptor.PredictorAdaptor.forward", "modulename": "micro_sam.bioimageio.predictor_adaptor", "qualname": "PredictorAdaptor.forward", "kind": "function", "doc": "<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  torch inputs of dimensions B x C x H x W</li>\n<li><strong>box_prompts:</strong>  box coordinates of dimensions B x OBJECTS x 4</li>\n<li><strong>point_prompts:</strong>  point coordinates of dimension B x OBJECTS x POINTS x 2</li>\n<li><strong>point_labels:</strong>  point labels of dimension B x OBJECTS x POINTS</li>\n<li><strong>mask_prompts:</strong>  mask prompts of dimension B x OBJECTS x 256 x 256</li>\n<li><strong>embeddings:</strong>  precomputed image embeddings B x 256 x 64 x 64</li>\n</ul>\n\n<p>Returns:</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">box_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prompts</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation", "modulename": "micro_sam.evaluation", "kind": "module", "doc": "<p>Functionality for evaluating Segment Anything models on microscopy data.</p>\n"}, {"fullname": "micro_sam.evaluation.evaluation", "modulename": "micro_sam.evaluation.evaluation", "kind": "module", "doc": "<p>Evaluation functionality for segmentation predictions from <code>micro_sam.evaluation.automatic_mask_generation</code>\nand <code>micro_sam.evaluation.inference</code>.</p>\n"}, {"fullname": "micro_sam.evaluation.evaluation.run_evaluation", "modulename": "micro_sam.evaluation.evaluation", "qualname": "run_evaluation", "kind": "function", "doc": "<p>Run evaluation for instance segmentation predictions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The list of paths to ground-truth images.</li>\n<li><strong>prediction_paths:</strong>  The list of paths with the instance segmentations to evaluate.</li>\n<li><strong>save_path:</strong>  Optional path for saving the results.</li>\n<li><strong>verbose:</strong>  Whether to print the progress.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A DataFrame that contains the evaluation results.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.evaluation.run_evaluation_for_iterative_prompting", "modulename": "micro_sam.evaluation.evaluation", "qualname": "run_evaluation_for_iterative_prompting", "kind": "function", "doc": "<p>Run evaluation for iterative prompt-based segmentation predictions.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The list of paths to ground-truth images.</li>\n<li><strong>prediction_root:</strong>  The folder with the iterative prompt-based instance segmentations to evaluate.</li>\n<li><strong>experiment_folder:</strong>  The folder where all the experiment results are stored.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to evaluate on experiments with iterative prompting starting with box.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A DataFrame that contains the evaluation results.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box_prompt</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">overwrite_results</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments", "modulename": "micro_sam.evaluation.experiments", "kind": "module", "doc": "<p>Predefined experiment settings for experiments with different prompt strategies.</p>\n"}, {"fullname": "micro_sam.evaluation.experiments.ExperimentSetting", "modulename": "micro_sam.evaluation.experiments", "qualname": "ExperimentSetting", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Dict"}, {"fullname": "micro_sam.evaluation.experiments.full_experiment_settings", "modulename": "micro_sam.evaluation.experiments", "qualname": "full_experiment_settings", "kind": "function", "doc": "<p>The full experiment settings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>use_boxes:</strong>  Whether to run the experiments with or without boxes.</li>\n<li><strong>positive_range:</strong>  The different number of positive points that will be used.\nBy defaul the values are set to [1, 2, 4, 8, 16].</li>\n<li><strong>negative_range:</strong>  The different number of negative points that will be used.\nBy defaul the values are set to [0, 1, 2, 4, 8, 16].</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of experiment settings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">positive_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">negative_range</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments.default_experiment_settings", "modulename": "micro_sam.evaluation.experiments", "qualname": "default_experiment_settings", "kind": "function", "doc": "<p>The three default experiment settings.</p>\n\n<p>For the default experiments we use a single positive prompt,\ntwo positive and four negative prompts and box prompts.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The list of experiment settings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.experiments.get_experiment_setting_name", "modulename": "micro_sam.evaluation.experiments", "qualname": "get_experiment_setting_name", "kind": "function", "doc": "<p>Get the name for the given experiment setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>setting:</strong>  The experiment setting.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The name for this experiment setting.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">setting</span><span class=\"p\">:</span> <span class=\"n\">Dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference", "modulename": "micro_sam.evaluation.inference", "kind": "module", "doc": "<p>Inference with Segment Anything models and different prompt strategies.</p>\n"}, {"fullname": "micro_sam.evaluation.inference.precompute_all_embeddings", "modulename": "micro_sam.evaluation.inference", "qualname": "precompute_all_embeddings", "kind": "function", "doc": "<p>Precompute all image embeddings.</p>\n\n<p>To enable running different inference tasks in parallel afterwards.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the embeddings will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.precompute_all_prompts", "modulename": "micro_sam.evaluation.inference", "qualname": "precompute_all_prompts", "kind": "function", "doc": "<p>Precompute all point prompts.</p>\n\n<p>To enable running different inference tasks in parallel afterwards.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>gt_paths:</strong>  The file paths to the ground-truth segmentations.</li>\n<li><strong>prompt_save_dir:</strong>  The directory where the prompt files will be saved.</li>\n<li><strong>prompt_settings:</strong>  The settings for which the prompts will be computed.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_save_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_settings</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_inference_with_prompts", "modulename": "micro_sam.evaluation.inference", "qualname": "run_inference_with_prompts", "kind": "function", "doc": "<p>Run segment anything inference for multiple images using prompts derived from groundtruth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the image embddings will be saved or are already saved.</li>\n<li><strong>use_points:</strong>  Whether to use point prompts.</li>\n<li><strong>use_boxes:</strong>  Whether to use box prompts</li>\n<li><strong>n_positives:</strong>  The number of positive point prompts that will be sampled.</li>\n<li><strong>n_negativess:</strong>  The number of negative point prompts that will be sampled.</li>\n<li><strong>dilation:</strong>  The dilation factor for the radius around the ground-truth object\naround which points will not be sampled.</li>\n<li><strong>prompt_save_dir:</strong>  The directory where point prompts will be saved or are already saved.\nThis enables running multiple experiments in a reproducible manner.</li>\n<li><strong>batch_size:</strong>  The batch size used for batched prediction.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">n_positives</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_negatives</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_save_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">512</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_inference_with_iterative_prompting", "modulename": "micro_sam.evaluation.inference", "qualname": "run_inference_with_iterative_prompting", "kind": "function", "doc": "<p>Run segment anything inference for multiple images using prompts iteratively\n    derived from model outputs and groundtruth</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_paths:</strong>  The image file paths.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation file paths.</li>\n<li><strong>embedding_dir:</strong>  The directory where the image embeddings will be saved or are already saved.</li>\n<li><strong>prediction_dir:</strong>  The directory where the predictions from SegmentAnything will be saved per iteration.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to use the first prompt as bounding box or a single point</li>\n<li><strong>dilation:</strong>  The dilation factor for the radius around the ground-truth object\naround which points will not be sampled.</li>\n<li><strong>batch_size:</strong>  The batch size used for batched predictions.</li>\n<li><strong>n_iterations:</strong>  The number of iterations for iterative prompting.</li>\n<li><strong>use_masks:</strong>  Whether to make use of logits from previous prompt-based segmentation</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box_prompt</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">n_iterations</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">use_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_amg", "modulename": "micro_sam.evaluation.inference", "qualname": "run_amg", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.inference.run_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.inference", "qualname": "run_instance_segmentation_with_decoder", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation", "modulename": "micro_sam.evaluation.instance_segmentation", "kind": "module", "doc": "<p>Inference and evaluation for the automatic instance segmentation functionality.</p>\n"}, {"fullname": "micro_sam.evaluation.instance_segmentation.default_grid_search_values_amg", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "default_grid_search_values_amg", "kind": "function", "doc": "<p>Default grid-search parameter for AMG-based instance segmentation.</p>\n\n<p>Return grid search values for the two most important parameters:</p>\n\n<ul>\n<li><code>pred_iou_thresh</code>, the threshold for keeping objects according to the IoU predicted by the model.</li>\n<li><code>stability_score_thresh</code>, the theshold for keepong objects according to their stability.</li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>iou_thresh_values:</strong>  The values for <code>pred_iou_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>stability_score_values:</strong>  The values for <code>stability_score_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.default_grid_search_values_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "default_grid_search_values_instance_segmentation_with_decoder", "kind": "function", "doc": "<p>Default grid-search parameter for decoder-based instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>center_distance_threshold_values:</strong>  The values for <code>center_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>boundary_distance_threshold_values:</strong>  The values for <code>boundary_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>distance_smoothing_values:</strong>  The values for <code>distance_smoothing</code> used in the gridsearch.\nBy default values in the range from 1.0 to 2.0 with a stepsize of 0.1 will be used.</li>\n<li><strong>min_size_values:</strong>  The values for <code>min_size</code> used in the gridsearch.\nBy default the values 50, 100 and 200  are used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">center_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_grid_search", "kind": "function", "doc": "<p>Run grid search for automatic mask generation.</p>\n\n<p>The parameters and their respective value ranges for the grid search are specified via the\n'grid_search_values' argument. For example, to run a grid search over the parameters 'pred_iou_thresh'\nand 'stability_score_thresh', you can pass the following:</p>\n\n<pre><code>grid_search_values = {\n    \"pred_iou_thresh\": [0.6, 0.7, 0.8, 0.9],\n    \"stability_score_thresh\": [0.6, 0.7, 0.8, 0.9],\n}\n</code></pre>\n\n<p>All combinations of the parameters will be checked.</p>\n\n<p>You can use the functions <code>default_grid_search_values_instance_segmentation_with_decoder</code>\nor <code>default_grid_search_values_amg</code> to get the default grid search parameters for the two\nrespective instance segmentation methods.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>generate</code> function.</li>\n<li><strong>image_paths:</strong>  The input images for the grid search.</li>\n<li><strong>gt_paths:</strong>  The ground-truth segmentation for the grid search.</li>\n<li><strong>result_dir:</strong>  Folder to cache the evaluation results per image.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>fixed_generate_kwargs:</strong>  Fixed keyword arguments for the <code>generate</code> method of the segmenter.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>image_key:</strong>  Key for loading the image data from a more complex file format like HDF5.\nIf not given a simple image format like tif is assumed.</li>\n<li><strong>gt_key:</strong>  Key for loading the ground-truth data from a more complex file format like HDF5.\nIf not given a simple image format like tif is assumed.</li>\n<li><strong>rois:</strong>  Region of interests to resetrict the evaluation to.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">image_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gt_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">rois</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">slice</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_inference", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_inference", "kind": "function", "doc": "<p>Run inference for automatic mask generation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>image_paths:</strong>  The input images.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>prediction_dir:</strong>  Folder to save the predictions.</li>\n<li><strong>generate_kwargs:</strong>  The keyword arguments for the <code>generate</code> method of the segmenter.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.evaluate_instance_segmentation_grid_search", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "evaluate_instance_segmentation_grid_search", "kind": "function", "doc": "<p>Evaluate gridsearch results.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>result_dir:</strong>  The folder with the gridsearch results.</li>\n<li><strong>grid_search_parameters:</strong>  The names for the gridsearch parameters.</li>\n<li><strong>criterion:</strong>  The metric to use for determining the best parameters.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The best parameter setting.\n  The evaluation score for the best setting.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_parameters</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">criterion</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mSA&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">],</span> <span class=\"nb\">float</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.save_grid_search_best_params", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "save_grid_search_best_params", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">best_kwargs</span>, </span><span class=\"param\"><span class=\"n\">best_msa</span>, </span><span class=\"param\"><span class=\"n\">grid_search_result_dir</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.instance_segmentation.run_instance_segmentation_grid_search_and_inference", "modulename": "micro_sam.evaluation.instance_segmentation", "qualname": "run_instance_segmentation_grid_search_and_inference", "kind": "function", "doc": "<p>Run grid search and inference for automatic mask generation.</p>\n\n<p>Please refer to the documentation of <code>run_instance_segmentation_grid_search</code>\nfor details on how to specify the grid search parameters.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmenter:</strong>  The class implementing the instance segmentation functionality.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>generate</code> function.</li>\n<li><strong>val_image_paths:</strong>  The input images for the grid search.</li>\n<li><strong>val_gt_paths:</strong>  The ground-truth segmentation for the grid search.</li>\n<li><strong>test_image_paths:</strong>  The input images for inference.</li>\n<li><strong>embedding_dir:</strong>  Folder to cache the image embeddings.</li>\n<li><strong>prediction_dir:</strong>  Folder to save the predictions.</li>\n<li><strong>result_dir:</strong>  Folder to cache the evaluation results per image.</li>\n<li><strong>fixed_generate_kwargs:</strong>  Fixed keyword arguments for the <code>generate</code> method of the segmenter.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmenter</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">val_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">val_gt_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">test_image_paths</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">prediction_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_generate_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell", "modulename": "micro_sam.evaluation.livecell", "kind": "module", "doc": "<p>Inference and evaluation for the <a href=\"https://www.nature.com/articles/s41592-021-01249-6\">LIVECell dataset</a> and\nthe different cell lines contained in it.</p>\n"}, {"fullname": "micro_sam.evaluation.livecell.CELL_TYPES", "modulename": "micro_sam.evaluation.livecell", "qualname": "CELL_TYPES", "kind": "variable", "doc": "<p></p>\n", "default_value": "[&#x27;A172&#x27;, &#x27;BT474&#x27;, &#x27;BV2&#x27;, &#x27;Huh7&#x27;, &#x27;MCF7&#x27;, &#x27;SHSY5Y&#x27;, &#x27;SkBr3&#x27;, &#x27;SKOV3&#x27;]"}, {"fullname": "micro_sam.evaluation.livecell.livecell_inference", "modulename": "micro_sam.evaluation.livecell", "qualname": "livecell_inference", "kind": "function", "doc": "<p>Run inference for livecell with a fixed prompt setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segment anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>use_points:</strong>  Whether to use point prompts.</li>\n<li><strong>use_boxes:</strong>  Whether to use box prompts.</li>\n<li><strong>n_positives:</strong>  The number of positive point prompts.</li>\n<li><strong>n_negatives:</strong>  The number of negative point prompts.</li>\n<li><strong>prompt_folder:</strong>  The folder where the prompts should be saved.</li>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">use_boxes</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">n_positives</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_negatives</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_precompute_embeddings", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_precompute_embeddings", "kind": "function", "doc": "<p>Run precomputation of val and test image embeddings for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_iterative_prompting", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_iterative_prompting", "kind": "function", "doc": "<p>Run inference on livecell with iterative prompting setting.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segment anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>start_with_box_prompt:</strong>  Whether to use the first prompt as bounding box or a single point.</li>\n<li><strong>use_masks:</strong>  Whether to make use of logits from previous prompt-based segmentation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">start_with_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_masks</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_amg", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_amg", "kind": "function", "doc": "<p>Run automatic mask generation grid-search and inference for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>iou_thresh_values:</strong>  The values for <code>pred_iou_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>stability_score_values:</strong>  The values for <code>stability_score_thresh</code> used in the gridsearch.\nBy default values in the range from 0.6 to 0.9 with a stepsize of 0.025 will be used.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path where the predicted images are stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_thresh_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_instance_segmentation_with_decoder", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_instance_segmentation_with_decoder", "kind": "function", "doc": "<p>Run automatic mask generation grid-search and inference for livecell.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint:</strong>  The segment anything model checkpoint.</li>\n<li><strong>input_folder:</strong>  The folder with the livecell data.</li>\n<li><strong>model_type:</strong>  The type of the segmenta anything model.</li>\n<li><strong>experiment_folder:</strong>  The folder where to save all data associated with the experiment.</li>\n<li><strong>center_distance_threshold_values:</strong>  The values for <code>center_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>boundary_distance_threshold_values:</strong>  The values for <code>boundary_distance_threshold</code> used in the gridsearch.\nBy default values in the range from 0.3 to 0.7 with a stepsize of 0.1 will be used.</li>\n<li><strong>distance_smoothing_values:</strong>  The values for <code>distance_smoothing</code> used in the gridsearch.\nBy default values in the range from 1.0 to 2.0 with a stepsize of 0.1 will be used.</li>\n<li><strong>min_size_values:</strong>  The values for <code>min_size</code> used in the gridsearch.\nBy default the values 50, 100 and 200  are used.</li>\n<li><strong>verbose_gs:</strong>  Whether to run the gridsearch for individual images in a verbose mode.</li>\n<li><strong>n_val_per_cell_type:</strong>  The number of validation images per cell type.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path where the predicted images are stored.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">experiment_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">center_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose_gs</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">n_val_per_cell_type</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">25</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_inference", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_inference", "kind": "function", "doc": "<p>Run LIVECell inference with command line tool.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.livecell.run_livecell_evaluation", "modulename": "micro_sam.evaluation.livecell", "qualname": "run_livecell_evaluation", "kind": "function", "doc": "<p>Run LiveCELL evaluation with command line tool.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "kind": "module", "doc": "<p>Functionality for qualitative comparison of Segment Anything models on microscopy data.</p>\n"}, {"fullname": "micro_sam.evaluation.model_comparison.generate_data_for_model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "generate_data_for_model_comparison", "kind": "function", "doc": "<p>Generate samples for qualitative model comparison.</p>\n\n<p>This precomputes the input for <code>model_comparison</code> and <code>model_comparison_with_napari</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>loader:</strong>  The torch dataloader from which samples are drawn.</li>\n<li><strong>output_folder:</strong>  The folder where the samples will be saved.</li>\n<li><strong>model_type1:</strong>  The first model to use for comparison.\nThe value needs to be a valid model_type for <code>micro_sam.util.get_sam_model</code>.</li>\n<li><strong>model_type2:</strong>  The second model to use for comparison.\nThe value needs to be a valid model_type for <code>micro_sam.util.get_sam_model</code>.</li>\n<li><strong>n_samples:</strong>  The number of samples to draw from the dataloader.</li>\n<li><strong>checkpoint1:</strong>  Optional checkpoint for the first model.</li>\n<li><strong>checkpoint2:</strong>  Optional checkpoint for the second model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type1</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type2</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">model_type3</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint1</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint2</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint3</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison.model_comparison", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "model_comparison", "kind": "function", "doc": "<p>Create images for a qualitative model comparision.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_folder:</strong>  The folder with the data precomputed by <code>generate_data_for_model_comparison</code>.</li>\n<li><strong>n_images_per_sample:</strong>  The number of images to generate per precomputed sample.</li>\n<li><strong>min_size:</strong>  The min size of ground-truth objects to take into account.</li>\n<li><strong>plot_folder:</strong>  The folder where to save the plots. If not given the plots will be displayed.</li>\n<li><strong>point_radius:</strong>  The radius of the point overlay.</li>\n<li><strong>outline_dilation:</strong>  The dilation factor of the outline overlay.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">n_images_per_sample</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">plot_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_radius</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">outline_dilation</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">have_model3</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.model_comparison.model_comparison_with_napari", "modulename": "micro_sam.evaluation.model_comparison", "qualname": "model_comparison_with_napari", "kind": "function", "doc": "<p>Use napari to display the qualtiative comparison results for two models.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>output_folder:</strong>  The folder with the data precomputed by <code>generate_data_for_model_comparison</code>.</li>\n<li><strong>show_points:</strong>  Whether to show the results for point or for box prompts.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">show_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.default_grid_search_values_multi_dimensional_segmentation", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "default_grid_search_values_multi_dimensional_segmentation", "kind": "function", "doc": "<p>Default grid-search parameters for multi-dimensional prompt-based instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>iou_threshold_values:</strong>  The values for <code>iou_threshold</code> used in the grid-search.\nBy default values in the range from 0.5 to 0.9 with a stepsize of 0.1 will be used.</li>\n<li><strong>projection_method_values:</strong>  The values for <code>projection</code> method used in the grid-search.\nBy default the values <code>mask</code>, <code>bounding_box</code> and <code>points</code> are used.</li>\n<li><strong>box_extension_values:</strong>  The values for <code>box_extension</code> used in the grid-search.\nBy default values in the range from 0 to 0.25 with a stepsize of 0.025 will be used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The values for grid search.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">iou_threshold_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">projection_method_values</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension_values</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.segment_slices_from_ground_truth", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "segment_slices_from_ground_truth", "kind": "function", "doc": "<p>Segment all objects in a volume by prompt-based segmentation in one slice per object.</p>\n\n<p>This function first segments each object in the respective specified slice using interactive\n(prompt-based) segmentation functionality. Then it segments the particular object in the\nremaining slices in the volume.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>ground_truth:</strong>  The label volume with instance segmentations.</li>\n<li><strong>model_type:</strong>  Choice of segment anything model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the model checkpoint.</li>\n<li><strong>embedding_path:</strong>  Path to cache the computed embeddings.</li>\n<li><strong>iou_threshold:</strong>  The criterion to decide whether to link the objects in the consecutive slice's segmentation.</li>\n<li><strong>projection:</strong>  The projection (prompting) method to generate prompts for consecutive slices.</li>\n<li><strong>box_extension:</strong>  Extension factor for increasing the box size after projection.</li>\n<li><strong>device:</strong>  The selected device for computation.</li>\n<li><strong>interactive_seg_mode:</strong>  Method for guiding prompt-based instance segmentation.</li>\n<li><strong>verbose:</strong>  Whether to get the trace for projected segmentations.</li>\n<li><strong>return_segmentation:</strong>  Whether to return the segmented volume.</li>\n<li><strong>min_size:</strong>  The minimal size for evaluating an object in the ground-truth.\nThe size is measured within the central slice.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">ground_truth</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">iou_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.8</span>,</span><span class=\"param\">\t<span class=\"n\">projection</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;mask&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mf\">0.025</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">interactive_seg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;box&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_segmentation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.evaluation.multi_dimensional_segmentation.run_multi_dimensional_segmentation_grid_search", "modulename": "micro_sam.evaluation.multi_dimensional_segmentation", "qualname": "run_multi_dimensional_segmentation_grid_search", "kind": "function", "doc": "<p>Run grid search for prompt-based multi-dimensional instance segmentation.</p>\n\n<p>The parameters and their respective value ranges for the grid search are specified via the\n<code>grid_search_values</code> argument. For example, to run a grid search over the parameters <code>iou_threshold</code>,\n<code>projection</code> and <code>box_extension</code>, you can pass the following:</p>\n\n<pre><code>grid_search_values = {\n    \"iou_threshold\": [0.5, 0.6, 0.7, 0.8, 0.9],\n    \"projection\": [\"mask\", \"bounding_box\", \"points\"],\n    \"box_extension\": [0, 0.1, 0.2, 0.3, 0.4, 0,5],\n}\n</code></pre>\n\n<p>All combinations of the parameters will be checked.\nIf passed None, the function <code>default_grid_search_values_multi_dimensional_segmentation</code> is used\nto get the default grid search parameters for the instance segmentation method.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>ground_truth:</strong>  The label volume with instance segmentations.</li>\n<li><strong>model_type:</strong>  Choice of segment anything model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the model checkpoint.</li>\n<li><strong>embedding_path:</strong>  Path to cache the computed embeddings.</li>\n<li><strong>result_path:</strong>  Path to save the grid search results.</li>\n<li><strong>interactive_seg_mode:</strong>  Method for guiding prompt-based instance segmentation.</li>\n<li><strong>verbose:</strong>  Whether to get the trace for projected segmentations.</li>\n<li><strong>grid_search_values:</strong>  The grid search values for parameters of the <code>segment_slices_from_ground_truth</code> function.</li>\n<li><strong>min_size:</strong>  The minimal size for evaluating an object in the ground-truth.\nThe size is measured within the central slice.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">ground_truth</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">result_dir</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">interactive_seg_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;box&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">grid_search_values</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">List</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.inference", "modulename": "micro_sam.inference", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.inference.batched_inference", "modulename": "micro_sam.inference", "qualname": "batched_inference", "kind": "function", "doc": "<p>Run batched inference for input prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>image:</strong>  The input image.</li>\n<li><strong>batch_size:</strong>  The batch size to use for inference.</li>\n<li><strong>boxes:</strong>  The box prompts. Array of shape N_PROMPTS x 4.\nThe bounding boxes are represented by [MIN_X, MIN_Y, MAX_X, MAX_Y].</li>\n<li><strong>points:</strong>  The point prompt coordinates. Array of shape N_PROMPTS x 1 x 2.\nThe points are represented by their coordinates [X, Y], which are given\nin the last dimension.</li>\n<li><strong>point_labels:</strong>  The point prompt labels. Array of shape N_PROMPTS x 1.\nThe labels are either 0 (negative prompt) or 1 (positive prompt).</li>\n<li><strong>multimasking:</strong>  Whether to predict with 3 or 1 mask.</li>\n<li><strong>embedding_path:</strong>  Cache path for the image embeddings.</li>\n<li><strong>return_instance_segmentation:</strong>  Whether to return a instance segmentation\nor the individual mask data.</li>\n<li><strong>segmentation_ids:</strong>  Fixed segmentation ids to assign to the masks\nderived from the prompts.</li>\n<li><strong>reduce_multimasking:</strong>  Whether to choose the most likely masks with\nhighest ious from multimasking</li>\n<li><strong>logits_masks:</strong>  The logits masks. Array of shape N_PROMPTS x 1 x 256 x 256.\nWhether to use the logits masks from previous segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predicted segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">boxes</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">point_labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimasking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_instance_segmentation</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">list</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">reduce_multimasking</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">logits_masks</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation", "modulename": "micro_sam.instance_segmentation", "kind": "module", "doc": "<p>Automated instance segmentation functionality.\nThe classes implemented here extend the automatic instance segmentation from Segment Anything:\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html</a></p>\n"}, {"fullname": "micro_sam.instance_segmentation.mask_data_to_segmentation", "modulename": "micro_sam.instance_segmentation", "qualname": "mask_data_to_segmentation", "kind": "function", "doc": "<p>Convert the output of the automatic mask generation to an instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>masks:</strong>  The outputs generated by AutomaticMaskGenerator or EmbeddingMaskGenerator.\nOnly supports output_mode=binary_mask.</li>\n<li><strong>with_background:</strong>  Whether the segmentation has background. If yes this function assures that the largest\nobject in the output will be mapped to zero (the background value).</li>\n<li><strong>min_object_size:</strong>  The minimal size of an object in pixels.</li>\n<li><strong>max_object_size:</strong>  The maximal size of an object in pixels.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">masks</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">min_object_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">max_object_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase", "kind": "class", "doc": "<p>Base class for the automatic mask generators.</p>\n", "bases": "abc.ABC"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.is_initialized", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.is_initialized", "kind": "variable", "doc": "<p>Whether the mask generator has already been initialized.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_list", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_list", "kind": "variable", "doc": "<p>The list of mask data after initialization.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.crop_boxes", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.crop_boxes", "kind": "variable", "doc": "<p>The list of crop boxes.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.original_size", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.original_size", "kind": "variable", "doc": "<p>The original image size.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.get_state", "kind": "function", "doc": "<p>Get the initialized state of the mask generator.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>State of the mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.set_state", "kind": "function", "doc": "<p>Set the state of the mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The state of the mask generator, e.g. from serialized state.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AMGBase.clear_state", "modulename": "micro_sam.instance_segmentation", "qualname": "AMGBase.clear_state", "kind": "function", "doc": "<p>Clear the state of the mask generator.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>This class implements the same logic as\n<a href=\"https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py\">https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/automatic_mask_generator.py</a>\nIt decouples the computationally expensive steps of generating masks from the cheap post-processing operation\nto filter these masks to enable grid search and interactively changing the post-processing.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">amg</span> <span class=\"o\">=</span> <span class=\"n\">AutomaticMaskGenerator</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">)</span>\n<span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>  <span class=\"c1\"># Initialize the masks, this takes care of all expensive computations.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">amg</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">pred_iou_thresh</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the masks. This is fast and enables testing parameters</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>crop_n_layers:</strong>  If &gt;0, the mask prediction will be run again on crops of the image.</li>\n<li><strong>crop_overlap_ratio:</strong>  Sets the degree to which crops overlap.</li>\n<li><strong>crop_n_points_downscale_factor:</strong>  How the number of points is downsampled when predicting with crops.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n</ul>\n", "bases": "AMGBase"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_layers</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">crop_overlap_ratio</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.3413333333333333</span>,</span><span class=\"param\">\t<span class=\"n\">crop_n_points_downscale_factor</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.AutomaticMaskGenerator.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "AutomaticMaskGenerator.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>pred_iou_thresh:</strong>  Filter threshold in [0, 1], using the mask quality predicted by the model.</li>\n<li><strong>stability_score_thresh:</strong>  Filter threshold in [0, 1], using the stability of the mask\nunder changes to the cutoff used to binarize the model prediction.</li>\n<li><strong>box_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks.</li>\n<li><strong>crop_nms_thresh:</strong>  The IoU threshold used by nonmax suppression to filter duplicate masks between crops.</li>\n<li><strong>min_mask_region_area:</strong>  Minimal size for the predicted masks.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">pred_iou_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.88</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.95</span>,</span><span class=\"param\">\t<span class=\"n\">box_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">crop_nms_thresh</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span>,</span><span class=\"param\">\t<span class=\"n\">min_mask_region_area</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a point grid.</p>\n\n<p>Implements the same functionality as <code>AutomaticMaskGenerator</code> but for tiled embeddings.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points_per_side:</strong>  The number of points to be sampled along one side of the image.\nIf None, <code>point_grids</code> must provide explicit point sampling.</li>\n<li><strong>points_per_batch:</strong>  The number of points run simultaneously by the model.\nHigher numbers may be faster but use more GPU memory.</li>\n<li><strong>point_grids:</strong>  A lisst over explicit grids of points used for sampling masks.\nNormalized to [0, 1] with respect to the image coordinate system.</li>\n<li><strong>stability_score_offset:</strong>  The amount to shift the cutoff when calculating the stability score.</li>\n</ul>\n", "bases": "AutomaticMaskGenerator"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_side</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">32</span>,</span><span class=\"param\">\t<span class=\"n\">points_per_batch</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">point_grids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">stability_score_offset</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.TiledAutomaticMaskGenerator.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledAutomaticMaskGenerator.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and masks for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_shape:</strong>  The tile shape for embedding prediction.</li>\n<li><strong>halo:</strong>  The overlap of between tiles.</li>\n<li><strong>verbose:</strong>  Whether to print computation progress.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter", "kind": "class", "doc": "<p>Adapter to contain the UNETR decoder in a single module.</p>\n\n<p>To apply the decoder on top of pre-computed embeddings for\nthe segmentation functionality.\nSee also: <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unetr.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/model/unetr.py</a></p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">unetr</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.base", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.base", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.out_conv", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.out_conv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv_out", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv_out", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.decoder_head", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.decoder_head", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.final_activation", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.final_activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.postprocess_masks", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.postprocess_masks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.decoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv1", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv2", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv3", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.deconv4", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.deconv4", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.instance_segmentation.DecoderAdapter.forward", "modulename": "micro_sam.instance_segmentation", "qualname": "DecoderAdapter.forward", "kind": "function", "doc": "<p>Defines the computation performed at every call.</p>\n\n<p>Should be overridden by all subclasses.</p>\n\n<div class=\"pdoc-alert pdoc-alert-note\">\n\n<p>Although the recipe for forward pass needs to be defined within\nthis function, one should call the <code>Module</code> instance afterwards\ninstead of this since the former takes care of running the\nregistered hooks while the latter silently ignores them.</p>\n\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">input_</span>, </span><span class=\"param\"><span class=\"n\">input_shape</span>, </span><span class=\"param\"><span class=\"n\">original_shape</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_unetr", "modulename": "micro_sam.instance_segmentation", "qualname": "get_unetr", "kind": "function", "doc": "<p>Get UNETR model for automatic instance segmentation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_encoder:</strong>  The image encoder of the SAM model.\nThis is used as encoder by the UNETR too.</li>\n<li><strong>decoder_state:</strong>  Optional decoder state to initialize the weights\nof the UNETR decoder.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The UNETR model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_encoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">decoder_state</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "get_decoder", "kind": "function", "doc": "<p>Get decoder to predict outputs for automatic instance segmentation</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_encoder:</strong>  The image encoder of the SAM model.</li>\n<li><strong>decoder_state:</strong>  State to initialize the weights of the UNETR decoder.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The decoder for instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_encoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">decoder_state</span><span class=\"p\">:</span> <span class=\"n\">collections</span><span class=\"o\">.</span><span class=\"n\">OrderedDict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">DecoderAdapter</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_predictor_and_decoder", "modulename": "micro_sam.instance_segmentation", "qualname": "get_predictor_and_decoder", "kind": "function", "doc": "<p>Load the SAM model (predictor) and instance segmentation decoder.</p>\n\n<p>This requires a checkpoint that contains the state for both predictor\nand decoder.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The type of the image encoder used in the SAM model.</li>\n<li><strong>checkpoint_path:</strong>  Path to the checkpoint from which to load the data.</li>\n<li><strong>device:</strong>  The device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The SAM predictor.\n  The decoder for instance segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">DecoderAdapter</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder", "kind": "class", "doc": "<p>Generates an instance segmentation without prompts, using a decoder.</p>\n\n<p>Implements the same interface as <code>AutomaticMaskGenerator</code>.</p>\n\n<p>Use this class as follows:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"n\">segmenter</span> <span class=\"o\">=</span> <span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">(</span><span class=\"n\">predictor</span><span class=\"p\">,</span> <span class=\"n\">decoder</span><span class=\"p\">)</span>\n<span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">initialize</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>   <span class=\"c1\"># Predict the image embeddings and decoder outputs.</span>\n<span class=\"n\">masks</span> <span class=\"o\">=</span> <span class=\"n\">segmenter</span><span class=\"o\">.</span><span class=\"n\">generate</span><span class=\"p\">(</span><span class=\"n\">center_distance_threshold</span><span class=\"o\">=</span><span class=\"mf\">0.75</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate the instance segmentation.</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>decoder:</strong>  The decoder to predict intermediate representations\nfor instance segmentation.</li>\n</ul>\n"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.__init__", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span></span>)</span>"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.is_initialized", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.is_initialized", "kind": "variable", "doc": "<p>Whether the mask generator has already been initialized.</p>\n"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Whether to be verbose.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.generate", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.generate", "kind": "function", "doc": "<p>Generate instance segmentation for the currently initialized image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>center_distance_threshold:</strong>  Center distance predictions below this value will be\nused to find seeds (intersected with thresholded boundary distance predictions).</li>\n<li><strong>boundary_distance_threshold:</strong>  Boundary distance predictions below this value will be\nused to find seeds (intersected with thresholded center distance predictions).</li>\n<li><strong>foreground_smoothing:</strong>  Sigma value for smoothing the foreground predictions, to avoid\ncheckerboard artifacts in the prediction.</li>\n<li><strong>foreground_threshold:</strong>  Foreground predictions above this value will be used as foreground mask.</li>\n<li><strong>distance_smoothing:</strong>  Sigma value for smoothing the distance predictions.</li>\n<li><strong>min_size:</strong>  Minimal object size in the segmentation result.</li>\n<li><strong>output_mode:</strong>  The form masks are returned in. Pass None to directly return the instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">center_distance_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">boundary_distance_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">foreground_smoothing</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">distance_smoothing</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1.6</span>,</span><span class=\"param\">\t<span class=\"n\">min_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">output_mode</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;binary_mask&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.get_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.get_state", "kind": "function", "doc": "<p>Get the initialized state of the instance segmenter.</p>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Instance segmentation state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.set_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.set_state", "kind": "function", "doc": "<p>Set the state of the instance segmenter.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>state:</strong>  The instance segmentation state</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.InstanceSegmentationWithDecoder.clear_state", "modulename": "micro_sam.instance_segmentation", "qualname": "InstanceSegmentationWithDecoder.clear_state", "kind": "function", "doc": "<p>Clear the state of the instance segmenter.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.TiledInstanceSegmentationWithDecoder", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledInstanceSegmentationWithDecoder", "kind": "class", "doc": "<p>Same as <code>InstanceSegmentationWithDecoder</code> but for tiled image embeddings.</p>\n", "bases": "InstanceSegmentationWithDecoder"}, {"fullname": "micro_sam.instance_segmentation.TiledInstanceSegmentationWithDecoder.initialize", "modulename": "micro_sam.instance_segmentation", "qualname": "TiledInstanceSegmentationWithDecoder.initialize", "kind": "function", "doc": "<p>Initialize image embeddings and decoder predictions for an image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The input image, volume or timeseries.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\nSee <code>util.precompute_image_embeddings</code> for details.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>verbose:</strong>  Dummy input to be compatible with other function signatures.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.instance_segmentation.get_amg", "modulename": "micro_sam.instance_segmentation", "qualname": "get_amg", "kind": "function", "doc": "<p>Get the automatic mask generator class.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>is_tiled:</strong>  Whether tiled embeddings are used.</li>\n<li><strong>decoder:</strong>  Decoder to predict instacne segmmentation.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The automatic mask generator.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">is_tiled</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span><span class=\"p\">,</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">InstanceSegmentationWithDecoder</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation", "modulename": "micro_sam.multi_dimensional_segmentation", "kind": "module", "doc": "<p>Multi-dimensional segmentation with segment anything.</p>\n"}, {"fullname": "micro_sam.multi_dimensional_segmentation.PROJECTION_MODES", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "PROJECTION_MODES", "kind": "variable", "doc": "<p></p>\n", "default_value": "(&#x27;box&#x27;, &#x27;mask&#x27;, &#x27;points&#x27;, &#x27;points_and_mask&#x27;, &#x27;single_point&#x27;)"}, {"fullname": "micro_sam.multi_dimensional_segmentation.segment_mask_in_volume", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "segment_mask_in_volume", "kind": "function", "doc": "<p>Segment an object mask in in volumetric data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The initial segmentation for the object.</li>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>image_embeddings:</strong>  The precomputed image embeddings for the volume.</li>\n<li><strong>segmented_slices:</strong>  List of slices for which this object has already been segmented.</li>\n<li><strong>stop_lower:</strong>  Whether to stop at the lowest segmented slice.</li>\n<li><strong>stop_upper:</strong>  Wheter to stop at the topmost segmented slice.</li>\n<li><strong>iou_threshold:</strong>  The IOU threshold for continuing segmentation across 3d.</li>\n<li><strong>projection:</strong>  The projection method to use. One of 'box', 'mask', 'points', 'points_and_mask' or 'single point'.\nPass a dictionary to choose the excact combination of projection modes.</li>\n<li><strong>update_progress:</strong>  Callback to update an external progress bar.</li>\n<li><strong>box_extension:</strong>  Extension factor for increasing the box size after projection.</li>\n<li><strong>verbose:</strong>  Whether to print details about the segmentation steps.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>Array with the volumetric segmentation.\n  Tuple with the first and last segmented slice.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">segmented_slices</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">stop_lower</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">stop_upper</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">iou_threshold</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">projection</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">dict</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">update_progress</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation.merge_instance_segmentation_3d", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "merge_instance_segmentation_3d", "kind": "function", "doc": "<p>Merge stacked 2d instance segmentations into a consistent 3d segmentation.</p>\n\n<p>Solves a multicut problem based on the overlap of objects to merge across z.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>slice_segmentation:</strong>  The stacked segmentation across the slices.\nWe assume that the segmentation is labeled consecutive across z.</li>\n<li><strong>beta:</strong>  The bias term for the multicut. Higher values lead to a larger\ndegree of over-segmentation and vice versa.</li>\n<li><strong>with_background:</strong>  Whether this is a segmentation problem with background.\nIn that case all edges connecting to the background are set to be repulsive.</li>\n<li><strong>gap_closing:</strong>  If given, gaps in the segmentation are closed with a binary closing\noperation. The value is used to determine the number of iterations for the closing.</li>\n<li><strong>min_z_extent:</strong>  Require a minimal extent in z for the segmented objects.\nThis can help to prevent segmentation artifacts.</li>\n<li><strong>verbose:</strong>  Verbosity flag.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The merged segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">slice_segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">beta</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gap_closing</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_z_extent</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.multi_dimensional_segmentation.automatic_3d_segmentation", "modulename": "micro_sam.multi_dimensional_segmentation", "qualname": "automatic_3d_segmentation", "kind": "function", "doc": "<p>Segment volume in 3d.</p>\n\n<p>First segments slices individually in 2d and then merges them across 3d\nbased on overlap of objects between slices.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>volume:</strong>  The input volume.</li>\n<li><strong>predictor:</strong>  The SAM model.</li>\n<li><strong>segmentor:</strong>  The instance segmentation class.</li>\n<li><strong>embedding_path:</strong>  The path to save pre-computed embeddings.</li>\n<li><strong>with_background:</strong>  Whether the segmentation has background.</li>\n<li><strong>gap_closing:</strong>  If given, gaps in the segmentation are closed with a binary closing\noperation. The value is used to determine the number of iterations for the closing.</li>\n<li><strong>min_z_extent:</strong>  Require a minimal extent in z for the segmented objects.\nThis can help to prevent segmentation artifacts.</li>\n<li><strong>verbose:</strong>  Verbosity flag.</li>\n<li><strong>kwargs:</strong>  Keyword arguments for the 'generate' method of the 'segmentor'.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segmentation.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">segmentor</span><span class=\"p\">:</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_background</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">gap_closing</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">min_z_extent</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state", "modulename": "micro_sam.precompute_state", "kind": "module", "doc": "<p>Precompute image embeddings and automatic mask generator state for image data.</p>\n"}, {"fullname": "micro_sam.precompute_state.cache_amg_state", "modulename": "micro_sam.precompute_state", "qualname": "cache_amg_state", "kind": "function", "doc": "<p>Compute and cache or load the state for the automatic mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n<li><strong>save_path:</strong>  The embedding save path. The AMG state will be stored in 'save_path/amg_state.pickle'.</li>\n<li><strong>verbose:</strong>  Whether to run the computation verbose.</li>\n<li><strong>i:</strong>  The index for which to cache the state.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The automatic mask generator class with the cached state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">raw</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state.cache_is_state", "modulename": "micro_sam.precompute_state", "qualname": "cache_is_state", "kind": "function", "doc": "<p>Compute and cache or load the state for the automatic mask generator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>decoder:</strong>  The instance segmentation decoder.</li>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n<li><strong>save_path:</strong>  The embedding save path. The AMG state will be stored in 'save_path/amg_state.pickle'.</li>\n<li><strong>verbose:</strong>  Whether to run the computation verbose.</li>\n<li><strong>i:</strong>  The index for which to cache the state.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for the amg class.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The instance segmentation class with the cached state.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">decoder</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">raw</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">instance_segmentation</span><span class=\"o\">.</span><span class=\"n\">AMGBase</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.precompute_state.precompute_state", "modulename": "micro_sam.precompute_state", "qualname": "precompute_state", "kind": "function", "doc": "<p>Precompute the image embeddings and other optional state for the input image(s).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_path:</strong>  The input image file(s). Can either be a single image file (e.g. tif or png),\na container file (e.g. hdf5 or zarr) or a folder with images files.\nIn case of a container file the argument <code>key</code> must be given. In case of a folder\nit can be given to provide a glob pattern to subselect files from the folder.</li>\n<li><strong>output_path:</strong>  The output path were the embeddings and other state will be saved.</li>\n<li><strong>model_type:</strong>  The SegmentAnything model to use. Will use the standard vit_h model by default.</li>\n<li><strong>checkpoint_path:</strong>  Path to a checkpoint for a custom model.</li>\n<li><strong>key:</strong>  The key to the input file. This is needed for contaner files (e.g. hdf5 or zarr)\nand can be used to provide a glob pattern if the input is a folder with image files.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic instance segmentation\nin addition to the image embeddings.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">output_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation", "modulename": "micro_sam.prompt_based_segmentation", "kind": "module", "doc": "<p>Functions for prompt-based segmentation with Segment Anything.</p>\n"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_points", "kind": "function", "doc": "<p>Segmentation from point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>points:</strong>  The point prompts given in the image coordinate system.</li>\n<li><strong>labels:</strong>  The labels (positive or negative) associated with the points.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>use_best_multimask:</strong>  Whether to use multimask output and then choose the best mask.\nBy default this is used for a single positive point and not otherwise.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">use_best_multimask</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">bool</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_mask", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_mask", "kind": "function", "doc": "<p>Segmentation from a mask prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>mask:</strong>  The mask used to derive prompts.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>use_box:</strong>  Whether to derive the bounding box prompt from the mask.</li>\n<li><strong>use_mask:</strong>  Whether to use the mask itself as prompt.</li>\n<li><strong>use_points:</strong>  Whether to derive point prompts from the mask.</li>\n<li><strong>original_size:</strong>  Full image shape. Use this if the mask that is being passed\ndownsampled compared to the original image.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>box_extension:</strong>  Relative factor used to enlarge the bounding box prompt.</li>\n<li><strong>box:</strong>  Precomputed bounding box.</li>\n<li><strong>points:</strong>  Precomputed point prompts.</li>\n<li><strong>labels:</strong>  Positive/negative labels corresponding to the point prompts.</li>\n<li><strong>use_single_point:</strong>  Whether to derive just a single point from the mask.\nIn case use_points is true.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">mask</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_box</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_mask</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">use_points</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">original_size</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_logits</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">use_single_point</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box", "kind": "function", "doc": "<p>Segmentation from a box prompt.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n<li><strong>box_extension:</strong>  Relative factor used to enlarge the bounding box prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">box_extension</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_based_segmentation.segment_from_box_and_points", "modulename": "micro_sam.prompt_based_segmentation", "qualname": "segment_from_box_and_points", "kind": "function", "doc": "<p>Segmentation from a box prompt and point prompts.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The segment anything predictor.</li>\n<li><strong>box:</strong>  The box prompt.</li>\n<li><strong>points:</strong>  The point prompts, given in the image coordinates system.</li>\n<li><strong>labels:</strong>  The point labels, either positive or negative.</li>\n<li><strong>image_embeddings:</strong>  Optional precomputed image embeddings.\n   Has to be passed if the predictor is not yet initialized.\ni: Index for the image data. Required if the input data has three spatial dimensions\n    or a time dimension and two spatial dimensions.</li>\n<li><strong>multimask_output:</strong>  Whether to return multiple or just a single mask.</li>\n<li><strong>return_all:</strong>  Whether to return the score and logits in addition to the mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The binary segmentation mask.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">box</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">points</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">labels</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_all</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.prompt_generators", "modulename": "micro_sam.prompt_generators", "kind": "module", "doc": "<p>Classes for generating prompts from ground-truth segmentation masks.\nFor training or evaluation of prompt-based segmentation.</p>\n"}, {"fullname": "micro_sam.prompt_generators.PromptGeneratorBase", "modulename": "micro_sam.prompt_generators", "qualname": "PromptGeneratorBase", "kind": "class", "doc": "<p>PromptGeneratorBase is an interface to implement specific prompt generators.</p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator", "kind": "class", "doc": "<p>Generate point and/or box prompts from an instance segmentation.</p>\n\n<p>You can use this class to derive prompts from an instance segmentation, either for\nevaluation purposes or for training Segment Anything on custom data.\nIn order to use this generator you need to precompute the bounding boxes and center\ncoordiantes of the instance segmentation, using e.g. <code>util.get_centers_and_bounding_boxes</code>.</p>\n\n<p>Here's an example for how to use this class:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"c1\"># Initialize generator for 1 positive and 4 negative point prompts.</span>\n<span class=\"n\">prompt_generator</span> <span class=\"o\">=</span> <span class=\"n\">PointAndBoxPromptGenerator</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">dilation_strength</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Precompute the bounding boxes for the given segmentation</span>\n<span class=\"n\">bounding_boxes</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">get_centers_and_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">segmentation</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># generate point prompts for the objects with ids 1, 2 and 3</span>\n<span class=\"n\">seg_ids</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"n\">object_mask</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">stack</span><span class=\"p\">([</span><span class=\"n\">segmentation</span> <span class=\"o\">==</span> <span class=\"n\">seg_id</span> <span class=\"k\">for</span> <span class=\"n\">seg_id</span> <span class=\"ow\">in</span> <span class=\"n\">seg_ids</span><span class=\"p\">])[:,</span> <span class=\"kc\">None</span><span class=\"p\">]</span>\n<span class=\"n\">this_bounding_boxes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">bounding_boxes</span><span class=\"p\">[</span><span class=\"n\">seg_id</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">seg_id</span> <span class=\"ow\">in</span> <span class=\"n\">seg_ids</span><span class=\"p\">]</span>\n<span class=\"n\">point_coords</span><span class=\"p\">,</span> <span class=\"n\">point_labels</span><span class=\"p\">,</span> <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">prompt_generator</span><span class=\"p\">(</span><span class=\"n\">object_mask</span><span class=\"p\">,</span> <span class=\"n\">this_bounding_boxes</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>n_positive_points:</strong>  The number of positive point prompts to generate per mask.</li>\n<li><strong>n_negative_points:</strong>  The number of negative point prompts to generate per mask.</li>\n<li><strong>dilation_strength:</strong>  The factor by which the mask is dilated before generating prompts.</li>\n<li><strong>get_point_prompts:</strong>  Whether to generate point prompts.</li>\n<li><strong>get_box_prompts:</strong>  Whether to generate box prompts.</li>\n</ul>\n", "bases": "PromptGeneratorBase"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.__init__", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">n_positive_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_negative_points</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_strength</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">get_point_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">get_box_prompts</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_positive_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_positive_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.n_negative_points", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.n_negative_points", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.dilation_strength", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.dilation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_box_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_box_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.PointAndBoxPromptGenerator.get_point_prompts", "modulename": "micro_sam.prompt_generators", "qualname": "PointAndBoxPromptGenerator.get_point_prompts", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.prompt_generators.IterativePromptGenerator", "modulename": "micro_sam.prompt_generators", "qualname": "IterativePromptGenerator", "kind": "class", "doc": "<p>Generate point prompts from an instance segmentation iteratively.</p>\n", "bases": "PromptGeneratorBase"}, {"fullname": "micro_sam.sam_annotator", "modulename": "micro_sam.sam_annotator", "kind": "module", "doc": "<p>The interactive annotation tools.</p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.Annotator2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "Annotator2d", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.Annotator2d.__init__", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "Annotator2d.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_2d.annotator_2d", "modulename": "micro_sam.sam_annotator.annotator_2d", "qualname": "annotator_2d", "kind": "function", "doc": "<p>Start the 2d annotation tool for a given image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The image data.</li>\n<li><strong>embedding_path:</strong>  Filepath where to save the embeddings.</li>\n<li><strong>segmentation_result:</strong>  An initial segmentation to load.\nThis can be used to correct segmentations with Segment Anything or to save and load progress.\nThe segmentation will be loaded as the 'committed_objects' layer.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_result</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.annotator_3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.Annotator3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "Annotator3d", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.Annotator3d.__init__", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "Annotator3d.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_3d.annotator_3d", "modulename": "micro_sam.sam_annotator.annotator_3d", "qualname": "annotator_3d", "kind": "function", "doc": "<p>Start the 3d annotation tool for a given image volume.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image:</strong>  The volumetric image data.</li>\n<li><strong>embedding_path:</strong>  Filepath for saving the precomputed embeddings.</li>\n<li><strong>segmentation_result:</strong>  An initial segmentation to load.\nThis can be used to correct segmentations with Segment Anything or to save and load progress.\nThe segmentation will be loaded as the 'committed_objects' layer.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_result</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.AnnotatorTracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "AnnotatorTracking", "kind": "class", "doc": "<p>Base class for micro_sam annotation plugins.</p>\n\n<p>Implements the logic for the 2d, 3d and tracking annotator.\nThe annotators differ in their data dimensionality and the widgets.</p>\n", "bases": "micro_sam.sam_annotator._annotator._AnnotatorBase"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.AnnotatorTracking.__init__", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "AnnotatorTracking.__init__", "kind": "function", "doc": "<p>Create the annotator GUI.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>viewer:</strong>  The napari viewer.</li>\n<li><strong>ndim:</strong>  The number of spatial dimension of the image data (2 or 3).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.annotator_tracking.annotator_tracking", "modulename": "micro_sam.sam_annotator.annotator_tracking", "qualname": "annotator_tracking", "kind": "function", "doc": "<p>Start the tracking annotation tool fora given timeseries.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>raw:</strong>  The image data.</li>\n<li><strong>embedding_path:</strong>  Filepath for saving the precomputed embeddings.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>device:</strong>  The computational device to use for the SAM model.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.image_series_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "image_series_annotator", "kind": "function", "doc": "<p>Run the annotation tool for a series of images (supported for both 2d and 3d images).</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>images:</strong>  List of the file paths or list of (set of) slices for the images to be annotated.</li>\n<li><strong>output_folder:</strong>  The folder where the segmentation results are saved.</li>\n<li><strong>model_type:</strong>  The Segment Anything model to use. For details on the available models check out\n<a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>.</li>\n<li><strong>embedding_path:</strong>  Filepath where to save the embeddings.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled embedding prediction.\nIf <code>None</code> then the whole image is passed to Segment Anything.</li>\n<li><strong>halo:</strong>  Shape of the overlap between tiles, which is needed to segment objects on tile boarders.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>precompute_amg_state:</strong>  Whether to precompute the state for automatic mask generation.\nThis will take more time when precomputing embeddings, but will then make\nautomatic mask generation much faster.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the SAM model.</li>\n<li><strong>is_volumetric:</strong>  Whether to use the 3d annotator.</li>\n<li><strong>prefer_decoder:</strong>  Whether to use decoder based instance segmentation if\nthe model used has an additional decoder for instance segmentation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">images</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">embedding_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">precompute_amg_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_volumetric</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">prefer_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.image_folder_annotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "image_folder_annotator", "kind": "function", "doc": "<p>Run the 2d annotation tool for a series of images in a folder.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>input_folder:</strong>  The folder with the images to be annotated.</li>\n<li><strong>output_folder:</strong>  The folder where the segmentation results are saved.</li>\n<li><strong>pattern:</strong>  The glob patter for loading files from <code>input_folder</code>.\nBy default all files will be loaded.</li>\n<li><strong>viewer:</strong>  The viewer to which the SegmentAnything functionality should be added.\nThis enables using a pre-initialized viewer.</li>\n<li><strong>return_viewer:</strong>  Whether to return the napari viewer to further modify it before starting the tool.</li>\n<li><strong>kwargs:</strong>  The keyword arguments for <code>micro_sam.sam_annotator.image_series_annotator</code>.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The napari viewer, only returned if <code>return_viewer=True</code>.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">output_folder</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">pattern</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;*&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_viewer</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator", "kind": "class", "doc": "<p>QWidget(parent: typing.Optional[QWidget] = None, flags: Union[Qt.WindowFlags, Qt.WindowType] = Qt.WindowFlags())</p>\n", "bases": "micro_sam.sam_annotator._widgets._WidgetBase"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator.__init__", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">viewer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">viewer</span><span class=\"o\">.</span><span class=\"n\">Viewer</span>, </span><span class=\"param\"><span class=\"n\">parent</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.image_series_annotator.ImageSeriesAnnotator.run_button", "modulename": "micro_sam.sam_annotator.image_series_annotator", "qualname": "ImageSeriesAnnotator.run_button", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.training_ui", "modulename": "micro_sam.sam_annotator.training_ui", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget", "kind": "class", "doc": "<p>QWidget(parent: typing.Optional[QWidget] = None, flags: Union[Qt.WindowFlags, Qt.WindowType] = Qt.WindowFlags())</p>\n", "bases": "micro_sam.sam_annotator._widgets._WidgetBase"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget.__init__", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">parent</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.sam_annotator.training_ui.TrainingWidget.run_button", "modulename": "micro_sam.sam_annotator.training_ui", "qualname": "TrainingWidget.run_button", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.util", "modulename": "micro_sam.sam_annotator.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.sam_annotator.util.point_layer_to_prompts", "modulename": "micro_sam.sam_annotator.util", "qualname": "point_layer_to_prompts", "kind": "function", "doc": "<p>Extract point prompts for SAM from a napari point layer.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>layer:</strong>  The point layer from which to extract the prompts.</li>\n<li><strong>i:</strong>  Index for the data (required for 3d or timeseries data).</li>\n<li><strong>track_id:</strong>  Id of the current track (required for tracking data).</li>\n<li><strong>with_stop_annotation:</strong>  Whether a single negative point will be interpreted\nas stop annotation or just returned as normal prompt.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The point coordinates for the prompts.\n  The labels (positive or negative / 1 or 0) for the prompts.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">track_id</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_stop_annotation</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.shape_layer_to_prompts", "modulename": "micro_sam.sam_annotator.util", "qualname": "shape_layer_to_prompts", "kind": "function", "doc": "<p>Extract prompts for SAM from a napari shape layer.</p>\n\n<p>Extracts the bounding box for 'rectangle' shapes and the bounding box and corresponding mask\nfor 'ellipse' and 'polygon' shapes.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prompt_layer:</strong>  The napari shape layer.</li>\n<li><strong>shape:</strong>  The image shape.</li>\n<li><strong>i:</strong>  Index for the data (required for 3d or timeseries data).</li>\n<li><strong>track_id:</strong>  Id of the current track (required for tracking data).</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The box prompts.\n  The mask prompts.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">Shapes</span>,</span><span class=\"param\">\t<span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">track_id</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.prompt_layer_to_state", "modulename": "micro_sam.sam_annotator.util", "qualname": "prompt_layer_to_state", "kind": "function", "doc": "<p>Get the state of the track from a point layer for a given timeframe.</p>\n\n<p>Only relevant for annotator_tracking.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>prompt_layer:</strong>  The napari layer.</li>\n<li><strong>i:</strong>  Timeframe of the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The state of this frame (either \"division\" or \"track\").</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">prompt_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>, </span><span class=\"param\"><span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sam_annotator.util.prompt_layers_to_state", "modulename": "micro_sam.sam_annotator.util", "qualname": "prompt_layers_to_state", "kind": "function", "doc": "<p>Get the state of the track from a point layer and shape layer for a given timeframe.</p>\n\n<p>Only relevant for annotator_tracking.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>point_layer:</strong>  The napari point layer.</li>\n<li><strong>box_layer:</strong>  The napari box layer.</li>\n<li><strong>i:</strong>  Timeframe of the data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The state of this frame (either \"division\" or \"track\").</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">point_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">points</span><span class=\"o\">.</span><span class=\"n\">Points</span>,</span><span class=\"param\">\t<span class=\"n\">box_layer</span><span class=\"p\">:</span> <span class=\"n\">napari</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">shapes</span><span class=\"o\">.</span><span class=\"n\">Shapes</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"nb\">int</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data", "modulename": "micro_sam.sample_data", "kind": "module", "doc": "<p>Sample microscopy data.</p>\n\n<p>You can change the download location for sample data and model weights\nby setting the environment variable: MICROSAM_CACHEDIR</p>\n\n<p>By default sample data is downloaded to a folder named 'micro_sam/sample_data'\ninside your default cache directory, eg:\n    * Mac: ~/Library/Caches/<AppName>\n    * Unix: ~/.cache/<AppName> or the value of the XDG_CACHE_HOME environment variable, if defined.\n    * Windows: C:\\Users&lt;user>\\AppData\\Local&lt;AppAuthor>&lt;AppName>\\Cache</p>\n"}, {"fullname": "micro_sam.sample_data.fetch_image_series_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_image_series_example_data", "kind": "function", "doc": "<p>Download the sample images for the image series annotator.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_image_series", "modulename": "micro_sam.sample_data", "qualname": "sample_data_image_series", "kind": "function", "doc": "<p>Provides image series example image to napari.</p>\n\n<p>Opens as three separate image layers in napari (one per image in series).\nThe third image in the series has a different size and modality.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_wholeslide_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_wholeslide_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads part of a whole-slide image from the NeurIPS Cell Segmentation Challenge.\nSee <a href=\"https://neurips22-cellseg.grand-challenge.org/\">https://neurips22-cellseg.grand-challenge.org/</a> for details on the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_wholeslide", "modulename": "micro_sam.sample_data", "qualname": "sample_data_wholeslide", "kind": "function", "doc": "<p>Provides wholeslide 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_livecell_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_livecell_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the LiveCELL dataset.\nSee <a href=\"https://doi.org/10.1038/s41592-021-01249-6\">https://doi.org/10.1038/s41592-021-01249-6</a> for details on the data.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_livecell", "modulename": "micro_sam.sample_data", "qualname": "sample_data_livecell", "kind": "function", "doc": "<p>Provides livecell 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_hela_2d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_hela_2d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 2d annotator.</p>\n\n<p>This downloads a single image from the HeLa CTC dataset.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_hela_2d", "modulename": "micro_sam.sample_data", "qualname": "sample_data_hela_2d", "kind": "function", "doc": "<p>Provides HeLa 2d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_3d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_3d_example_data", "kind": "function", "doc": "<p>Download the sample data for the 3d annotator.</p>\n\n<p>This downloads the Lucchi++ datasets from <a href=\"https://casser.io/connectomics/\">https://casser.io/connectomics/</a>.\nIt is a dataset for mitochondria segmentation in EM.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_3d", "modulename": "micro_sam.sample_data", "qualname": "sample_data_3d", "kind": "function", "doc": "<p>Provides Lucchi++ 3d example image to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_tracking_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_tracking_example_data", "kind": "function", "doc": "<p>Download the sample data for the tracking annotator.</p>\n\n<p>This data is the cell tracking challenge dataset DIC-C2DH-HeLa.\nCell tracking challenge webpage: <a href=\"http://data.celltrackingchallenge.net\">http://data.celltrackingchallenge.net</a>\nHeLa cells on a flat glass\nDr. G. van Cappellen. Erasmus Medical Center, Rotterdam, The Netherlands\nTraining dataset: <a href=\"http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/training-datasets/DIC-C2DH-HeLa.zip</a> (37 MB)\nChallenge dataset: <a href=\"http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip\">http://data.celltrackingchallenge.net/challenge-datasets/DIC-C2DH-HeLa.zip</a> (41 MB)</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_tracking", "modulename": "micro_sam.sample_data", "qualname": "sample_data_tracking", "kind": "function", "doc": "<p>Provides tracking example dataset to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_tracking_segmentation_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_tracking_segmentation_data", "kind": "function", "doc": "<p>Download groundtruth segmentation for the tracking example data.</p>\n\n<p>This downloads the groundtruth segmentation for the image data from <code>fetch_tracking_example_data</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The folder that contains the downloaded data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.sample_data_segmentation", "modulename": "micro_sam.sample_data", "qualname": "sample_data_segmentation", "kind": "function", "doc": "<p>Provides segmentation example dataset to napari.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.synthetic_data", "modulename": "micro_sam.sample_data", "qualname": "synthetic_data", "kind": "function", "doc": "<p>Create synthetic image data and segmentation for training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span>, </span><span class=\"param\"><span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.sample_data.fetch_nucleus_3d_example_data", "modulename": "micro_sam.sample_data", "qualname": "fetch_nucleus_3d_example_data", "kind": "function", "doc": "<p>Download the sample data for 3d segmentation of nuclei.</p>\n\n<p>This data contains a small crop from a volume from the publication\n\"Efficient automatic 3D segmentation of cell nuclei for high-content screening\"\n<a href=\"https://doi.org/10.1186/s12859-022-04737-4\">https://doi.org/10.1186/s12859-022-04737-4</a></p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>save_directory:</strong>  Root folder to save the downloaded data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The path of the downloaded image.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">save_directory</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training", "modulename": "micro_sam.training", "kind": "module", "doc": "<p>Functionality for training Segment Anything.</p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer", "modulename": "micro_sam.training.joint_sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer", "kind": "class", "doc": "<p>Trainer class for training the Segment Anything model.</p>\n\n<p>This class is derived from <code>torch_em.trainer.DefaultTrainer</code>.\nCheck out <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py</a>\nfor details on its usage and implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>convert_inputs:</strong>  The class that converts outputs of the dataloader to the expected input format of SAM.\nThe class <code>micro_sam.training.util.ConvertToSamInputs</code> can be used here.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iteration steps for which the masks predicted for one object are updated.\nIn each sub-iteration new point prompts are sampled where the model was wrong.</li>\n<li><strong>n_objects_per_batch:</strong>  If not given, we compute the loss for all objects in a sample.\nOtherwise the loss computation is limited to n_objects_per_batch, and the objects are randomly sampled.</li>\n<li><strong>mse_loss:</strong>  The regression loss to compare the IoU predicted by the model with the true IoU.</li>\n<li><strong>prompt_generator:</strong>  The iterative prompt generator which takes care of the iterative prompting logic for training</li>\n<li><strong>mask_prob:</strong>  The probability of using the mask inputs in the iterative prompting (per <code>n_sub_iteration</code>)</li>\n<li><strong>**kwargs:</strong>  The keyword arguments of the DefaultTrainer super class.</li>\n</ul>\n", "bases": "micro_sam.training.sam_trainer.SamTrainer"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.__init__", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">unetr</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">instance_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">instance_metric</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.unetr", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.unetr", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.instance_loss", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.instance_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.instance_metric", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.instance_metric", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.save_checkpoint", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.save_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">name</span>, </span><span class=\"param\"><span class=\"n\">current_metric</span>, </span><span class=\"param\"><span class=\"n\">best_metric</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">extra_save_dict</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.joint_sam_trainer.JointSamTrainer.load_checkpoint", "modulename": "micro_sam.training.joint_sam_trainer", "qualname": "JointSamTrainer.load_checkpoint", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">checkpoint</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.sam_trainer", "modulename": "micro_sam.training.sam_trainer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer", "kind": "class", "doc": "<p>Trainer class for training the Segment Anything model.</p>\n\n<p>This class is derived from <code>torch_em.trainer.DefaultTrainer</code>.\nCheck out <a href=\"https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py\">https://github.com/constantinpape/torch-em/blob/main/torch_em/trainer/default_trainer.py</a>\nfor details on its usage and implementation.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>convert_inputs:</strong>  The class that converts outputs of the dataloader to the expected input format of SAM.\nThe class <code>micro_sam.training.util.ConvertToSamInputs</code> can be used here.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iteration steps for which the masks predicted for one object are updated.\nIn each sub-iteration new point prompts are sampled where the model was wrong.</li>\n<li><strong>n_objects_per_batch:</strong>  If not given, we compute the loss for all objects in a sample.\nOtherwise the loss computation is limited to n_objects_per_batch, and the objects are randomly sampled.</li>\n<li><strong>mse_loss:</strong>  The regression loss to compare the IoU predicted by the model with the true IoU.</li>\n<li><strong>prompt_generator:</strong>  The iterative prompt generator which takes care of the iterative prompting logic for training</li>\n<li><strong>mask_prob:</strong>  The probability of using the mask inputs in the iterative prompting (per <code>n_sub_iteration</code>)</li>\n<li><strong>**kwargs:</strong>  The keyword arguments of the DefaultTrainer super class.</li>\n</ul>\n", "bases": "torch_em.trainer.default_trainer.DefaultTrainer"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.__init__", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">convert_inputs</span>,</span><span class=\"param\">\t<span class=\"n\">n_sub_iteration</span><span class=\"p\">:</span> <span class=\"nb\">int</span>,</span><span class=\"param\">\t<span class=\"n\">n_objects_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mse_loss</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span> <span class=\"o\">=</span> <span class=\"n\">MSELoss</span><span class=\"p\">()</span>,</span><span class=\"param\">\t<span class=\"n\">prompt_generator</span><span class=\"p\">:</span> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">prompt_generators</span><span class=\"o\">.</span><span class=\"n\">PromptGeneratorBase</span> <span class=\"o\">=</span> <span class=\"o\">&lt;</span><span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">prompt_generators</span><span class=\"o\">.</span><span class=\"n\">IterativePromptGenerator</span> <span class=\"nb\">object</span><span class=\"o\">&gt;</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span>)</span>"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.convert_inputs", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.convert_inputs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.mse_loss", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.mse_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.n_objects_per_batch", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.n_objects_per_batch", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.n_sub_iteration", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.n_sub_iteration", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.prompt_generator", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.prompt_generator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.sam_trainer.SamTrainer.mask_prob", "modulename": "micro_sam.training.sam_trainer", "qualname": "SamTrainer.mask_prob", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam", "modulename": "micro_sam.training.trainable_sam", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM", "kind": "class", "doc": "<p>Wrapper to make the SegmentAnything model trainable.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>sam:</strong>  The SegmentAnything Model.</li>\n<li><strong>device:</strong>  The device for training.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.__init__", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.__init__", "kind": "function", "doc": "<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">sam</span><span class=\"p\">:</span> <span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">modeling</span><span class=\"o\">.</span><span class=\"n\">sam</span><span class=\"o\">.</span><span class=\"n\">Sam</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span></span>)</span>"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.sam", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.sam", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.device", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.transform", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.preprocess", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.preprocess", "kind": "function", "doc": "<p>Resize, normalize pixel values and pad to a square input.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>x:</strong>  The input tensor.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The resized, normalized and padded tensor.\n  The shape of the image after resizing.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.image_embeddings_oft", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.image_embeddings_oft", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">batched_inputs</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.trainable_sam.TrainableSAM.forward", "modulename": "micro_sam.training.trainable_sam", "qualname": "TrainableSAM.forward", "kind": "function", "doc": "<p>Forward pass.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>batched_inputs:</strong>  The batched input images and prompts.</li>\n<li><strong>image_embeddings:</strong>  The precompute image embeddings. If not passed then they will be computed.</li>\n<li><strong>multimask_output:</strong>  Whether to predict mutiple or just a single mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predicted segmentation masks and iou values.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">batched_inputs</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>,</span><span class=\"param\">\t<span class=\"n\">multimask_output</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training", "modulename": "micro_sam.training.training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.training.FilePath", "modulename": "micro_sam.training.training", "qualname": "FilePath", "kind": "variable", "doc": "<p></p>\n", "default_value": "typing.Union[str, os.PathLike]"}, {"fullname": "micro_sam.training.training.train_sam", "modulename": "micro_sam.training.training", "qualname": "train_sam", "kind": "function", "doc": "<p>Run training for a SAM model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>name:</strong>  The name of the model to be trained.\nThe checkpoint and logs wil have this name.</li>\n<li><strong>model_type:</strong>  The type of the SAM model.</li>\n<li><strong>train_loader:</strong>  The dataloader for training.</li>\n<li><strong>val_loader:</strong>  The dataloader for validation.</li>\n<li><strong>n_epochs:</strong>  The number of epochs to train for.</li>\n<li><strong>early_stopping:</strong>  Enable early stopping after this number of epochs\nwithout improvement.</li>\n<li><strong>n_objects_per_batch:</strong>  The number of objects per batch used to compute\nthe loss for interative segmentation. If None all objects will be used,\nif given objects will be randomly sub-sampled.</li>\n<li><strong>checkpoint_path:</strong>  Path to checkpoint for initializing the SAM model.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train additional UNETR decoder\nfor automatic instance segmentation.</li>\n<li><strong>freeze:</strong>  Specify parts of the model that should be frozen, namely:\nimage_encoder, prompt_encoder and mask_decoder\nBy default nothing is frozen and the full model is updated.</li>\n<li><strong>device:</strong>  The device to use for training.</li>\n<li><strong>lr:</strong>  The learning rate.</li>\n<li><strong>n_sub_iteration:</strong>  The number of iterative prompts per training iteration.</li>\n<li><strong>save_root:</strong>  Optional root directory for saving the checkpoints and logs.\nIf not given the current working directory is used.</li>\n<li><strong>mask_prob:</strong>  The probability for using a mask as input in a given training sub-iteration.</li>\n<li><strong>n_iterations:</strong>  The number of iterations to use for training. This will over-ride n_epochs if given.</li>\n<li><strong>scheduler_class:</strong>  The learning rate scheduler to update the learning rate.\nBy default, ReduceLROnPlateau is used.</li>\n<li><strong>scheduler_kwargs:</strong>  The learning rate scheduler parameters.\nIf passed None, the chosen default parameters are used in ReduceLROnPlateau.</li>\n<li><strong>save_every_kth_epoch:</strong>  Save checkpoints after every kth epoch separately.</li>\n<li><strong>pbar_signals:</strong>  Controls for napari progress bar.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">n_epochs</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">early_stopping</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">n_objects_per_batch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">25</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">freeze</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lr</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">1e-05</span>,</span><span class=\"param\">\t<span class=\"n\">n_sub_iteration</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">save_root</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">mask_prob</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">n_iterations</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\tscheduler_class: Optional[torch.optim.lr_scheduler._LRScheduler] = &lt;class &#x27;torch.optim.lr_scheduler.ReduceLROnPlateau&#x27;&gt;,</span><span class=\"param\">\t<span class=\"n\">scheduler_kwargs</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_every_kth_epoch</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_signals</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">PyQt5</span><span class=\"o\">.</span><span class=\"n\">QtCore</span><span class=\"o\">.</span><span class=\"n\">QObject</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.default_sam_dataset", "modulename": "micro_sam.training.training", "qualname": "default_sam_dataset", "kind": "function", "doc": "<p>Create a PyTorch Dataset for training a SAM model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>raw_paths:</strong>  The path(s) to the image data used for training.\nCan either be multiple 2D images or volumetric data.</li>\n<li><strong>raw_key:</strong>  The key for accessing the image data. Internal filepath for hdf5-like input\nor a glob pattern for selecting multiple files.</li>\n<li><strong>label_paths:</strong>  The path(s) to the label data used for training.\nCan either be multiple 2D images or volumetric data.</li>\n<li><strong>label_key:</strong>  The key for accessing the label data. Internal filepath for hdf5-like input\nor a glob pattern for selecting multiple files.</li>\n<li><strong>patch_shape:</strong>  The shape for training patches.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train with additional segmentation decoder.</li>\n<li><strong>with_channels:</strong>  Whether the image data has RGB channels.</li>\n<li><strong>sampler:</strong>  A sampler to reject batches according to a given criterion.</li>\n<li><strong>n_samples:</strong>  The number of samples for this dataset.</li>\n<li><strong>is_train:</strong>  Whether this dataset is used for training or validation.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The dataset.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">raw_paths</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">raw_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">label_paths</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">]],</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">label_key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">patch_shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span>,</span><span class=\"param\">\t<span class=\"n\">with_channels</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">sampler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">n_samples</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">is_train</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataset</span><span class=\"o\">.</span><span class=\"n\">Dataset</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.default_sam_loader", "modulename": "micro_sam.training.training", "qualname": "default_sam_loader", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.training.CONFIGURATIONS", "modulename": "micro_sam.training.training", "qualname": "CONFIGURATIONS", "kind": "variable", "doc": "<p>Best training configurations for given hardware resources.</p>\n", "default_value": "{&#x27;Minimal&#x27;: {&#x27;model_type&#x27;: &#x27;vit_t&#x27;, &#x27;n_objects_per_batch&#x27;: 4, &#x27;n_sub_iteration&#x27;: 4}, &#x27;CPU&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;, &#x27;n_objects_per_batch&#x27;: 10}, &#x27;gtx1080&#x27;: {&#x27;model_type&#x27;: &#x27;vit_t&#x27;, &#x27;n_objects_per_batch&#x27;: 5}, &#x27;rtx5000&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;, &#x27;n_objects_per_batch&#x27;: 10}, &#x27;V100&#x27;: {&#x27;model_type&#x27;: &#x27;vit_b&#x27;}, &#x27;A100&#x27;: {&#x27;model_type&#x27;: &#x27;vit_h&#x27;}}"}, {"fullname": "micro_sam.training.training.train_sam_for_configuration", "modulename": "micro_sam.training.training", "qualname": "train_sam_for_configuration", "kind": "function", "doc": "<p>Run training for a SAM model with the configuration for a given hardware resource.</p>\n\n<p>Selects the best training settings for the given configuration.\nThe available configurations are listed in <code>CONFIGURATIONS</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>name:</strong>  The name of the model to be trained.\nThe checkpoint and logs wil have this name.</li>\n<li><strong>configuration:</strong>  The configuration (= name of hardware resource).</li>\n<li><strong>train_loader:</strong>  The dataloader for training.</li>\n<li><strong>val_loader:</strong>  The dataloader for validation.</li>\n<li><strong>checkpoint_path:</strong>  Path to checkpoint for initializing the SAM model.</li>\n<li><strong>with_segmentation_decoder:</strong>  Whether to train additional UNETR decoder\nfor automatic instance segmentation.</li>\n<li><strong>kwargs:</strong>  Additional keyword parameterts that will be passed to <code>train_sam</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">train_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">val_loader</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">DataLoader</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">with_segmentation_decoder</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util", "modulename": "micro_sam.training.util", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.identity", "modulename": "micro_sam.training.util", "qualname": "identity", "kind": "function", "doc": "<p>Identity transformation.</p>\n\n<p>This is a helper function to skip data normalization when finetuning SAM.\nData normalization is performed within the model and should thus be skipped as\na preprocessing step in training.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.require_8bit", "modulename": "micro_sam.training.util", "qualname": "require_8bit", "kind": "function", "doc": "<p>Transformation to require 8bit input data range (0-255).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.get_trainable_sam_model", "modulename": "micro_sam.training.util", "qualname": "get_trainable_sam_model", "kind": "function", "doc": "<p>Get the trainable sam model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The segment anything model that should be finetuned.\nThe weights of this model will be used for initialization, unless a\ncustom weight file is passed via <code>checkpoint_path</code>.</li>\n<li><strong>device:</strong>  The device to use for training.</li>\n<li><strong>checkpoint_path:</strong>  Path to a custom checkpoint from which to load the model weights.</li>\n<li><strong>freeze:</strong>  Specify parts of the model that should be frozen, namely: image_encoder, prompt_encoder and mask_decoder\nBy default nothing is frozen and the full model is updated.</li>\n<li><strong>return_state:</strong>  Whether to return the full checkpoint state.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The trainable segment anything model.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">freeze</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">micro_sam</span><span class=\"o\">.</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">trainable_sam</span><span class=\"o\">.</span><span class=\"n\">TrainableSAM</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs", "kind": "class", "doc": "<p>Convert outputs of data loader to the expected batched inputs of the SegmentAnything model.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>transform:</strong>  The transformation to resize the prompts. Should be the same transform used in the\nmodel to resize the inputs. If <code>None</code> the prompts will not be resized.</li>\n<li><strong>dilation_strength:</strong>  The dilation factor.\nIt determines a \"safety\" border from which prompts are not sampled to avoid ambiguous prompts\ndue to imprecise groundtruth masks.</li>\n<li><strong>box_distortion_factor:</strong>  Factor for distorting the box annotations derived from the groundtruth masks.</li>\n</ul>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.__init__", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">transform</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">segment_anything</span><span class=\"o\">.</span><span class=\"n\">utils</span><span class=\"o\">.</span><span class=\"n\">transforms</span><span class=\"o\">.</span><span class=\"n\">ResizeLongestSide</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">dilation_strength</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>,</span><span class=\"param\">\t<span class=\"n\">box_distortion_factor</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.dilation_strength", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.dilation_strength", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.transform", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ConvertToSamInputs.box_distortion_factor", "modulename": "micro_sam.training.util", "qualname": "ConvertToSamInputs.box_distortion_factor", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.__init__", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">desired_shape</span>, </span><span class=\"param\"><span class=\"n\">do_rescaling</span><span class=\"o\">=</span><span class=\"kc\">True</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;constant&#39;</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.desired_shape", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.desired_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.padding", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeRawTrafo.do_rescaling", "modulename": "micro_sam.training.util", "qualname": "ResizeRawTrafo.do_rescaling", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.__init__", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">desired_shape</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;constant&#39;</span>, </span><span class=\"param\"><span class=\"n\">min_size</span><span class=\"o\">=</span><span class=\"mi\">0</span></span>)</span>"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.desired_shape", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.desired_shape", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.padding", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.padding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.training.util.ResizeLabelTrafo.min_size", "modulename": "micro_sam.training.util", "qualname": "ResizeLabelTrafo.min_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "micro_sam.util", "modulename": "micro_sam.util", "kind": "module", "doc": "<p>Helper functions for downloading Segment Anything models and predicting image embeddings.</p>\n"}, {"fullname": "micro_sam.util.get_cache_directory", "modulename": "micro_sam.util", "qualname": "get_cache_directory", "kind": "function", "doc": "<p>Get micro-sam cache directory location.</p>\n\n<p>Users can set the MICROSAM_CACHEDIR environment variable for a custom cache directory.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.microsam_cachedir", "modulename": "micro_sam.util", "qualname": "microsam_cachedir", "kind": "function", "doc": "<p>Return the micro-sam cache directory.</p>\n\n<p>Returns the top level cache directory for micro-sam models and sample data.</p>\n\n<p>Every time this function is called, we check for any user updates made to\nthe MICROSAM_CACHEDIR os environment variable since the last time.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.models", "modulename": "micro_sam.util", "qualname": "models", "kind": "function", "doc": "<p>Return the segmentation models registry.</p>\n\n<p>We recreate the model registry every time this function is called,\nso any user changes to the default micro-sam cache directory location\nare respected.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_device", "modulename": "micro_sam.util", "qualname": "get_device", "kind": "function", "doc": "<p>Get the torch device.</p>\n\n<p>If no device is passed the default device for your system is used.\nElse it will be checked if the device you have passed is supported.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>device:</strong>  The input device.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The device.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_sam_model", "modulename": "micro_sam.util", "qualname": "get_sam_model", "kind": "function", "doc": "<p>Get the SegmentAnything Predictor.</p>\n\n<p>This function will download the required model or load it from the cached weight file.\nThis location of the cache can be changed by setting the environment variable: MICROSAM_CACHEDIR.\nThe name of the requested model can be set via <code>model_type</code>.\nSee <a href=\"https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models\">https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models</a>\nfor an overview of the available models</p>\n\n<p>Alternatively this function can also load a model from weights stored in a local filepath.\nThe corresponding file path is given via <code>checkpoint_path</code>. In this case <code>model_type</code>\nmust be given as the matching encoder architecture, e.g. \"vit_b\" if the weights are for\na SAM model with vit_b encoder.</p>\n\n<p>By default the models are downloaded to a folder named 'micro_sam/models'\ninside your default cache directory, eg:</p>\n\n<ul>\n<li>Mac: ~/Library/Caches/<AppName></li>\n<li>Unix: ~/.cache/<AppName> or the value of the XDG_CACHE_HOME environment variable, if defined.</li>\n<li>Windows: C:\\Users&lt;user>\\AppData\\Local&lt;AppAuthor>&lt;AppName>\\Cache\nSee the pooch.os_cache() documentation for more details:\n<a href=\"https://www.fatiando.org/pooch/latest/api/generated/pooch.os_cache.html\">https://www.fatiando.org/pooch/latest/api/generated/pooch.os_cache.html</a></li>\n</ul>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>model_type:</strong>  The SegmentAnything model to use. Will use the standard vit_h model by default.\nTo get a list of all available model names you can call <code>get_model_names</code>.</li>\n<li><strong>device:</strong>  The device for the model. If none is given will use GPU if available.</li>\n<li><strong>checkpoint_path:</strong>  The path to a file with weights that should be used instead of using the\nweights corresponding to <code>model_type</code>. If given, <code>model_type</code> must match the architecture\ncorresponding to the weight file. E.g. if you use weights for SAM with vit_b encoder\nthen <code>model_type</code> must be given as \"vit_b\".</li>\n<li><strong>return_sam:</strong>  Return the sam model object as well as the predictor.</li>\n<li><strong>return_state:</strong>  Return the unpickled checkpoint state.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The segment anything predictor.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;vit_l&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">device</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">return_sam</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">return_state</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.export_custom_sam_model", "modulename": "micro_sam.util", "qualname": "export_custom_sam_model", "kind": "function", "doc": "<p>Export a finetuned segment anything model to the standard model format.</p>\n\n<p>The exported model can be used by the interactive annotation tools in <code>micro_sam.annotator</code>.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>checkpoint_path:</strong>  The path to the corresponding checkpoint if not in the default model folder.</li>\n<li><strong>model_type:</strong>  The SegmentAnything model type corresponding to the checkpoint (vit_h, vit_b, vit_l or vit_t).</li>\n<li><strong>save_path:</strong>  Where to save the exported model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">checkpoint_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_model_names", "modulename": "micro_sam.util", "qualname": "get_model_names", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">) -> <span class=\"n\">Iterable</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.precompute_image_embeddings", "modulename": "micro_sam.util", "qualname": "precompute_image_embeddings", "kind": "function", "doc": "<p>Compute the image embeddings (output of the encoder) for the input.</p>\n\n<p>If 'save_path' is given the embeddings will be loaded/saved in a zarr container.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>input_:</strong>  The input data. Can be 2 or 3 dimensional, corresponding to an image, volume or timeseries.</li>\n<li><strong>save_path:</strong>  Path to save the embeddings in a zarr container.</li>\n<li><strong>lazy_loading:</strong>  Whether to load all embeddings into memory or return an\nobject to load them on demand when required. This only has an effect if 'save_path' is given\nand if the input is 3 dimensional.</li>\n<li><strong>ndim:</strong>  The dimensionality of the data. If not given will be deduced from the input data.</li>\n<li><strong>tile_shape:</strong>  Shape of tiles for tiled prediction. By default prediction is run without tiling.</li>\n<li><strong>halo:</strong>  Overlap of the tiles for tiled prediction.</li>\n<li><strong>verbose:</strong>  Whether to be verbose in the computation.</li>\n<li><strong>pbar_init:</strong>  Callback to initialize an external progress bar. Must accept number of steps and description.\nCan be used together with pbar_update to handle napari progress bar in other thread.\nTo enables using this function within a threadworker.</li>\n<li><strong>pbar_update:</strong>  Callback to update an external progress bar.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The image embeddings.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">input_</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"p\">:</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">PathLike</span><span class=\"p\">,</span> <span class=\"n\">NoneType</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">ndim</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_shape</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">halo</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">verbose</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_init</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">pbar_update</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"o\">&lt;</span><span class=\"n\">built</span><span class=\"o\">-</span><span class=\"ow\">in</span> <span class=\"n\">function</span> <span class=\"nb\">callable</span><span class=\"o\">&gt;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.set_precomputed", "modulename": "micro_sam.util", "qualname": "set_precomputed", "kind": "function", "doc": "<p>Set the precomputed image embeddings for a predictor.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>predictor:</strong>  The SegmentAnything predictor.</li>\n<li><strong>image_embeddings:</strong>  The precomputed image embeddings computed by <code>precompute_image_embeddings</code>.</li>\n<li><strong>i:</strong>  Index for the image data. Required if <code>image</code> has three spatial dimensions\nor a time dimension and two spatial dimensions.</li>\n<li><strong>tile_id:</strong>  Index for the tile. This is required if the embeddings are tiled.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The predictor with set features.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">predictor</span><span class=\"p\">:</span> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>,</span><span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">i</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">tile_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">mobile_sam</span><span class=\"o\">.</span><span class=\"n\">predictor</span><span class=\"o\">.</span><span class=\"n\">SamPredictor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.compute_iou", "modulename": "micro_sam.util", "qualname": "compute_iou", "kind": "function", "doc": "<p>Compute the intersection over union of two masks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>mask1:</strong>  The first mask.</li>\n<li><strong>mask2:</strong>  The second mask.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The intersection over union of the two masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">mask1</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>, </span><span class=\"param\"><span class=\"n\">mask2</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_centers_and_bounding_boxes", "modulename": "micro_sam.util", "qualname": "get_centers_and_bounding_boxes", "kind": "function", "doc": "<p>Returns the center coordinates of the foreground instances in the ground-truth.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The segmentation.</li>\n<li><strong>mode:</strong>  Determines the functionality used for computing the centers.</li>\n<li>If 'v', the object's eccentricity centers computed by vigra are used.</li>\n<li>If 'p' the object's centroids computed by skimage are used.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>A dictionary that maps object ids to the corresponding centroid.\n  A dictionary that maps object_ids to the corresponding bounding box.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;v&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">],</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">tuple</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.load_image_data", "modulename": "micro_sam.util", "qualname": "load_image_data", "kind": "function", "doc": "<p>Helper function to load image data from file.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>path:</strong>  The filepath to the image data.</li>\n<li><strong>key:</strong>  The internal filepath for complex data formats like hdf5.</li>\n<li><strong>lazy_loading:</strong>  Whether to lazyly load data. Only supported for n5 and zarr data.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The image data.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">key</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">lazy_loading</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.segmentation_to_one_hot", "modulename": "micro_sam.util", "qualname": "segmentation_to_one_hot", "kind": "function", "doc": "<p>Convert the segmentation to one-hot encoded masks.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>segmentation:</strong>  The segmentation.</li>\n<li><strong>segmentation_ids:</strong>  Optional subset of ids that will be used to subsample the masks.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The one-hot encoded masks.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">segmentation</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>,</span><span class=\"param\">\t<span class=\"n\">segmentation_ids</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.util.get_block_shape", "modulename": "micro_sam.util", "qualname": "get_block_shape", "kind": "function", "doc": "<p>Get a suitable block shape for chunking a given shape.</p>\n\n<p>The primary use for this is determining chunk sizes for\nzarr arrays or block shapes for parallelization.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>shape:</strong>  The image or volume shape.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The block shape.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">shape</span><span class=\"p\">:</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization", "modulename": "micro_sam.visualization", "kind": "module", "doc": "<p>Functionality for visualizing image embeddings.</p>\n"}, {"fullname": "micro_sam.visualization.compute_pca", "modulename": "micro_sam.visualization", "qualname": "compute_pca", "kind": "function", "doc": "<p>Compute the pca projection of the embeddings to visualize them as RGB image.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>embeddings:</strong>  The embeddings. For example predicted by the SAM image encoder.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>PCA of the embeddings, mapped to the pixels.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embeddings</span><span class=\"p\">:</span> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span></span><span class=\"return-annotation\">) -> <span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span>:</span></span>", "funcdef": "def"}, {"fullname": "micro_sam.visualization.project_embeddings_for_visualization", "modulename": "micro_sam.visualization", "qualname": "project_embeddings_for_visualization", "kind": "function", "doc": "<p>Project image embeddings to pixel-wise PCA.</p>\n\n<h6 id=\"arguments\">Arguments:</h6>\n\n<ul>\n<li><strong>image_embeddings:</strong>  The image embeddings.</li>\n</ul>\n\n<h6 id=\"returns\">Returns:</h6>\n\n<blockquote>\n  <p>The PCA of the embeddings.\n  The scale factor for resizing to the original image size.</p>\n</blockquote>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">image_embeddings</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">numpy</span><span class=\"o\">.</span><span class=\"n\">ndarray</span><span class=\"p\">,</span> <span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">,</span> <span class=\"o\">...</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();