{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Segment Anything with `µsam`\n",
    "\n",
    "This notebook shows how to use Segment Anything for Microscopy to fine-tune a Segment Anything Model (SAM) on an open-source data with multiple channels.\n",
    "\n",
    "We use confocal microscopy images from the HPA Kaggle Challenge for protein identification (from [Ouyang et al.](https://doi.org/10.1038/s41592-019-0658-6)) in this notebook for the cell segmentation task. The functionalities shown here should work for your (microscopy) images too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this notebook\n",
    "\n",
    "If you have an environment with `µsam` on your computer you can run this notebook in there. You can follow the [installation instructions](https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation) to install it on your computer.\n",
    "\n",
    "You can also run this notebook in the cloud on [Kaggle Notebooks](https://www.kaggle.com/code/). This service offers free usage of a GPU to speed up running the code. The next cells will take care of the installation for you if you are using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are running this notebook on kaggle, google colab or local compute resources.\n",
    "\n",
    "import os\n",
    "current_spot = os.getcwd()\n",
    "\n",
    "if current_spot.startswith(\"/kaggle/working\"):\n",
    "    print(\"Kaggle says hi!\")\n",
    "    root_dir = \"/kaggle/working\"\n",
    "\n",
    "elif current_spot.startswith(\"/content\"):\n",
    "    print(\"Google Colab says hi!\")\n",
    "    print(\" NOTE: The scripts have not been tested on Google Colab, you might need to adapt the installations a bit.\")\n",
    "    root_dir = \"/content\"\n",
    "\n",
    "    # You might need to install condacolab on Google Colab to be able to install packages using conda / mamba\n",
    "    # !pip install -q condacolab\n",
    "    # import condacolab\n",
    "    # condacolab.install()\n",
    "\n",
    "else:\n",
    "    msg = \"You are using a behind-the-scenes resource. Follow our installation instructions here:\"\n",
    "    msg += \" https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation\"\n",
    "    print(msg)\n",
    "    root_dir = \"\"  # overwrite to set the root directory, where the data, checkpoints, and all relevant stuff will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The next cells will install the `micro_sam` library on Kaggle Notebooks. **Please skip these cells and go to `Importing the libraries` if you are running the notebook on your own computer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/computational-cell-analytics/micro-sam.git\n",
    "tmp_dir = os.path.join(root_dir, \"micro-sam\")\n",
    "!pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/constantinpape/torch-em.git\n",
    "tmp_dir = os.path.join(root_dir, \"torch-em\")\n",
    "!pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/constantinpape/elf.git\n",
    "tmp_dir = os.path.join(root_dir, \"elf\")\n",
    "!pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Known Issues on **Kaggle Notebooks**:\n",
    "\n",
    "1. `warning  libmamba Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program` (multiples lines of such warnings)\n",
    "    - We have received this warning while testing this notebook on Kaggle. It does not lead to any issues while making use of the installed packages. You can proceed and ignore the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -q -y -c conda-forge nifty affogato zarr z5py\n",
    "!pip uninstall -y --quiet qtpy  # qtpy is not supported in Kaggle / Google Colab, let's remove it to avoid errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from typing import List\n",
    "from natsort import natsorted\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import imageio.v3 as imageio\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import label as connected_components\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_em.data import datasets\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.util.util import get_random_colors\n",
    "\n",
    "import micro_sam.training as sam_training\n",
    "from micro_sam.training.util import normalize_to_8bit\n",
    "from micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's download the dataset\n",
    "\n",
    "First, we download the images and corresponding labels stored as `tif` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data into a directory\n",
    "DATA_FOLDER = os.path.join(root_dir, \"hpa\")\n",
    "\n",
    "URLS = [\n",
    "    \"https://owncloud.gwdg.de/index.php/s/zp1Fmm4zEtLuhy4/download\",  # train\n",
    "    \"https://owncloud.gwdg.de/index.php/s/yV7LhGbGfvFGRBE/download\",  # val\n",
    "    \"https://owncloud.gwdg.de/index.php/s/8tLY5jPmpw37beM/download\",  # test\n",
    "]\n",
    "\n",
    "CHECKSUMS = [\n",
    "    \"6e5f3ec6b0d505511bea752adaf35529f6b9bb9e7729ad3bdd90ffe5b2d302ab\",  # train\n",
    "    \"4d7a4188cc3d3877b3cf1fbad5f714ced9af4e389801e2136623eac2fde78e9c\",  # val\n",
    "    \"8963ff47cdef95cefabb8941f33a3916258d19d10f532a209bab849d07f9abfe\",  # test\n",
    "]\n",
    "\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for url, checksum, split in zip(URLS, CHECKSUMS, SPLITS):\n",
    "    data_dir = os.path.join(DATA_FOLDER, split)\n",
    "    if os.path.exists(data_dir):\n",
    "        continue\n",
    "    \n",
    "    os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "    zip_path = os.path.join(DATA_FOLDER, \"data.zip\")\n",
    "    datasets.util.download_source(path=zip_path, url=url, download=True, checksum=checksum)\n",
    "    datasets.util.unzip(zip_path=zip_path, dst=DATA_FOLDER)\n",
    "\n",
    "# Get filepaths to the image data.\n",
    "train_image_paths = natsorted(glob(os.path.join(DATA_FOLDER, split, \"images\", \"*.tif\")))\n",
    "val_image_paths = natsorted(glob(os.path.join(DATA_FOLDER, split, \"images\", \"*.tif\")))\n",
    "test_image_paths = natsorted(glob(os.path.join(DATA_FOLDER, split, \"images\", \"*.tif\")))\n",
    "\n",
    "# Get filepaths to the label data.\n",
    "train_label_paths = natsorted(glob(os.path.join(DATA_FOLDER, split, \"labels\", \"*.tif\")))\n",
    "val_label_paths = natsorted(glob(os.path.join(DATA_FOLDER, split, \"labels\", \"*.tif\")))\n",
    "\n",
    "print(f\"The inputs have been preprocessed and stored at: '{DATA_FOLDER}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand our inputs' data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label_path in zip(train_image_paths, train_label_paths):  # Checking the inputs for the train split.\n",
    "    image = imageio.imread(image_path)\n",
    "    labels = imageio.imread(label_path)\n",
    "\n",
    "    # The images should be of shape: H, W, 4 -> where, 4 is the number of channels.\n",
    "    if (image.ndim == 3 and image.shape[-1] == 3) or image.ndim == 2:\n",
    "        print(f\"Inputs '{image.shape}' match the channel expectations.\")\n",
    "    else:\n",
    "        print(f\"Inputs '{image.shape}' must match the channel expectations (of either one or three channels).\")\n",
    "\n",
    "    # The labels should be of shape: H, W\n",
    "    print(f\"Shape of corresponding labels: '{labels.shape}'\")\n",
    "\n",
    "    break  # comment this line out in case you would like to verify the shapes for all inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment Anything accepts inputs of either 1 channel or 3 channels. To fine-tune Segment Anything on our data, we must select either 1 channel or 3 channels out of the 4 channels available.\n",
    "\n",
    "Let's make the choice to choose the `microtubule` (first channel), `protein` (second channel) and `nuclei` (third channel) for finetuning Segment Anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the 'er' channel, i.e. the last channel.\n",
    "def preprocess_inputs(image_paths: List[str]):\n",
    "    for image_path in image_paths:\n",
    "        image = imageio.imread(image_path)\n",
    "\n",
    "        if image.ndim == 3 and image.shape[-1] == 4:  # Convert 4 channel inputs to 3 channels.\n",
    "            image = image[..., :-1]\n",
    "            imageio.imwrite(image_path, image)\n",
    "\n",
    "preprocess_inputs(train_image_paths)\n",
    "preprocess_inputs(val_label_paths)\n",
    "preprocess_inputs(test_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the dataloaders\n",
    "\n",
    "Our task is to segment cells in confocal microscopy images. The dataset comes from https://zenodo.org/records/4665863, and the dataloader has been implemented in [torch-em](https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/light_microscopy/hpa.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's visualize how our samples look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label_path in zip(train_image_paths, train_label_paths):  # Visualize inputs for the train split.\n",
    "    image = imageio.imread(image_path)\n",
    "    labels = imageio.imread(label_path)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    labels = connected_components(labels)\n",
    "    ax[1].imshow(labels, cmap=get_random_colors(labels), interpolation=\"nearest\")\n",
    "    ax[1].set_title(\"Ground Truth Instances\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    break  # comment this out in case you want to visualize all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's create the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro_sam.training.default_sam_loader is a convenience function to build a torch dataloader.\n",
    "# from image data and labels for training segmentation models.\n",
    "# This is wrapped around the 'torch_em.default_segmentation_loader'.\n",
    "# It supports image data in various formats. Here, we load image data and corresponding labels by providing\n",
    "# filepaths to the respective tif files that were download and preprocessed using the functionality above.\n",
    "# Next, we create a list of filepaths for the image and label data by fetching all '*.tif' files in the\n",
    "# respective directories.\n",
    "# For more information, here is the documentation: https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/README.md\n",
    "# Here is a detailed notebook on finetuning Segment Anything: https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb\n",
    "\n",
    "# Load images from tif stacks by setting `raw_key` and `label_key` to None.\n",
    "raw_key, label_key = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The script below returns the train or val data loader for finetuning Segment Anything.\n",
    "\n",
    "# The data loader must be a torch dataloader that returns `x, y` tensors,\n",
    "# where `x` is the image data and `y` are the corresponding labels.\n",
    "# The labels have to be in a label mask instance segmentation format.\n",
    "# i.e. a tensor of the same spatial shape as `x`, with each object mask having its own ID.\n",
    "# IMPORTANT: the ID 0 is reserved for backgroun, and the IDS must be consecutive.\n",
    "\n",
    "# Here, we use `micro_sam.training.default_sam_loader` for creating the suitable data loader from\n",
    "# the HPA data. You can either adapt this for your own data or write a suitable torch dataloader yourself.\n",
    "# Here is a quickstart notebook to create your own dataloaders: https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\n",
    "\n",
    "batch_size = 1  # the training batch size\n",
    "patch_shape = (512, 512)  # the size of patches for training\n",
    "\n",
    "# Train an additional convolutional decoder for end-to-end automatic instance segmentation\n",
    "train_instance_segmentation = True\n",
    "\n",
    "# The dataloader internally takes care of adding label transforms: i.e. used to convert the ground-truth\n",
    "# labels to the desired instances for finetuning Segment Anythhing, or, to learn the foreground and distances\n",
    "# to the object centers and object boundaries for automatic segmentation.\n",
    "\n",
    "train_loader = sam_training.default_sam_loader(\n",
    "    raw_paths=train_image_paths,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=train_label_paths,\n",
    "    label_key=label_key,\n",
    "    is_seg_dataset=False,\n",
    "    patch_shape=patch_shape,\n",
    "    with_channels=True,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    raw_transform=normalize_to_8bit,\n",
    "    n_samples=100,\n",
    ")\n",
    "\n",
    "val_loader = sam_training.default_sam_loader(\n",
    "    raw_paths=val_image_paths,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=val_label_paths,\n",
    "    label_key=label_key,\n",
    "    is_seg_dataset=False,\n",
    "    patch_shape=patch_shape,\n",
    "    with_channels=True,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    batch_size=batch_size,\n",
    "    raw_transform=normalize_to_8bit,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how our samples lookm from the dataloader.\n",
    "check_loader(train_loader, 4, plt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the actual model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All hyperparameters for training.\n",
    "n_objects_per_batch = 5  # the number of objects per batch that will be sampled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # the device/GPU used for training\n",
    "n_epochs = 5  # how long we train (in epochs)\n",
    "\n",
    "# The model_type determines which base model is used to initialize the weights that are finetuned.\n",
    "# We use vit_b here because it can be trained faster. Note that vit_h usually yields higher quality results.\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "# The name of the checkpoint. The checkpoints will be stored in './checkpoints/<checkpoint_name>'\n",
    "checkpoint_name = \"sam_hpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The user needs to decide whether to finetune the Segment Anything model, or the `µsam`'s \"finetuned microscopy models\" for their dataset. Here, we finetune on the Segment Anything model for simplicity. For example, if you choose to finetune the model from the light microscopy generalist models, you need to update the `model_type` to `vit_b_lm` and it takes care of initializing the model with the desired weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "sam_training.train_sam(\n",
    "    name=checkpoint_name,\n",
    "    save_root=os.path.join(root_dir, \"models\"),\n",
    "    model_type=model_type,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    n_objects_per_batch=n_objects_per_batch,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's spot our best checkpoint and download it to get started with the annotation tool\n",
    "best_checkpoint = os.path.join(\"models\", \"checkpoints\", checkpoint_name, \"best.pt\")\n",
    "\n",
    "# # Download link is automatically generated for the best model.\n",
    "print(\"Click here \\u2193\")\n",
    "FileLink(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run the automatic instance segmentation (AIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automatic_instance_segmentation(image, checkpoint, model_type=\"vit_b\", device=None):\n",
    "    \"\"\"Automatic Instance Segmentation trained with an additional instance segmentation decoder in SAM.\n",
    "\n",
    "    NOTE: It is supported only for `µsam` models.\n",
    "    \n",
    "    Args:\n",
    "        image: The input image.\n",
    "        checkpoint: Filepath to the model checkpoints.\n",
    "        model_type: The choice of the `µsam` model.\n",
    "        device: The torch device.\n",
    "\n",
    "    Returns:\n",
    "        The instance segmentation.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the 'predictor' and 'segmenter' to perform automatic instance segmentation.\n",
    "    predictor, segmenter = get_predictor_and_segmenter(model_type=model_type, checkpoint=checkpoint, device=device)\n",
    "\n",
    "    # Step 2: Get the instance segmentation for the given image.\n",
    "    instances = automatic_instance_segmentation(predictor=predictor, segmenter=segmenter, input_path=image, ndim=2)\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(best_checkpoint), \"Please train the model first to run inference on the finetuned model.\"\n",
    "assert train_instance_segmentation is True, \"Oops. You didn't opt for finetuning using the decoder-based automatic instance segmentation.\"\n",
    "\n",
    "for image_path in test_image_paths:\n",
    "    image = imageio.imread(image_path)\n",
    "    \n",
    "    # Predicted instances\n",
    "    prediction = run_automatic_instance_segmentation(\n",
    "        image=image, checkpoint_path=best_checkpoint, model_type=model_type, device=device\n",
    "    )\n",
    "\n",
    "    # Visualize the predictions\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "\n",
    "    ax[1].imshow(prediction, cmap=get_random_colors(prediction), interpolation=\"nearest\")\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(\"Predictions (AIS)\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What next?\n",
    "\n",
    "It's time to get started with your custom finetuned model using the annotator tool. Here is the documentation on how to get started with `µsam`: [Annotation Tools](https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#annotation-tools)\n",
    "\n",
    "Happy annotating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was last ran on October 18, 2024*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
