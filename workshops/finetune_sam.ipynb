{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Segment Anything with `µsam`\n",
    "\n",
    "This notebook shows how to use Segment Anything for Microscopy to fine-tune a Segment Anything Model (SAM) on an open-source data with multiple channels.\n",
    "\n",
    "We use confocal microscopy images from the HPA Kaggle Challenge for protein identification (from [Ouyang et al.](https://doi.org/10.1038/s41592-019-0658-6)) in this notebook for the cell segmentation task. The functionalities shown here should work for your (microscopy) images too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this notebook\n",
    "\n",
    "If you have an environment with `µsam` on your computer you can run this notebook in there. You can follow the [installation instructions](https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation) to install it on your computer.\n",
    "\n",
    "You can also run this notebook in the cloud on [Kaggle Notebooks](https://www.kaggle.com/code/). This service offers free usage of a GPU to speed up running the code. The next cells will take care of the installation for you if you are using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are running this notebook on kaggle, google colab or local compute resources.\n",
    "\n",
    "import os\n",
    "current_spot = os.getcwd()\n",
    "\n",
    "if current_spot.startswith(\"/kaggle/working\"):\n",
    "    print(\"Kaggle says hi!\")\n",
    "    root_dir = \"/kaggle/working\"\n",
    "\n",
    "elif current_spot.startswith(\"/content\"):\n",
    "    print(\"Google Colab says hi!\")\n",
    "    print(\" NOTE: The scripts have not been tested on Google Colab, you might need to adapt the installations a bit.\")\n",
    "    root_dir = \"/content\"\n",
    "\n",
    "    # You might need to install condacolab on Google Colab to be able to install packages using conda / mamba\n",
    "    # !pip install -q condacolab\n",
    "    # import condacolab\n",
    "    # condacolab.install()\n",
    "\n",
    "else:\n",
    "    msg = \"You are using a behind-the-scenes resource. Follow our installation instructions here:\"\n",
    "    msg += \" https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#installation\"\n",
    "    print(msg)\n",
    "    root_dir = \"\"  # overwrite to set the root directory, where the data, checkpoints, and all relevant stuff will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "The next cells will install the `micro_sam` library on Kaggle Notebooks. **Please skip these cells and go to `Importing the libraries` if you are running the notebook on your own computer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/computational-cell-analytics/micro-sam.git\n",
    "tmp_dir = os.path.join(root_dir, \"micro-sam\")\n",
    "!pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/constantinpape/torch-em.git\n",
    "tmp_dir = os.path.join(root_dir, \"torch-em\")\n",
    "!pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --quiet https://github.com/constantinpape/elf.git\n",
    "tmp_dir = os.path.join(root_dir, \"elf\")\n",
    "!pip install --quiet $tmp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Known Issues on **Kaggle Notebooks**:\n",
    "\n",
    "1. `warning  libmamba Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program` (multiples lines of such warnings)\n",
    "    - We have received this warning while testing this notebook on Kaggle. It does not lead to any issues while making use of the installed packages. You can proceed and ignore the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mamba install -q -y -c conda-forge nifty affogato zarr z5py\n",
    "!pip uninstall -y --quiet qtpy  # qtpy is not supported in Kaggle / Google Colab, let's remove it to avoid errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from natsort import natsorted\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.measure import label as connected_components\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch_em\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.data.datasets import get_hpa_segmentation_paths\n",
    "from torch_em.util.util import get_random_colors\n",
    "from torch_em.transform.label import PerObjectDistanceTransform\n",
    "\n",
    "from micro_sam import util\n",
    "import micro_sam.training as sam_training\n",
    "from micro_sam.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data\n",
    "from micro_sam.instance_segmentation import (\n",
    "    InstanceSegmentationWithDecoder,\n",
    "    get_predictor_and_decoder,\n",
    "    mask_data_to_segmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's download the dataset\n",
    "\n",
    "First, we download the volumes, assort the input data (thanks to `torch-em`) and store the images and corresponding labels in `tif` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data into a directory\n",
    "DATA_FOLDER = os.path.join(root_dir, \"data\")\n",
    "volume_paths = get_hpa_segmentation_paths(path=os.path.join(DATA_FOLDER, \"hpa\"))\n",
    "\n",
    "# Store inputs as tif files\n",
    "image_dir = os.path.join(DATA_FOLDER, \"hpa\", \"preprocessed\", \"images\")\n",
    "label_dir = os.path.join(DATA_FOLDER, \"hpa\", \"preprocessed\", \"labels\")\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "for volume_path in volume_paths:\n",
    "    fname = Path(volume_path).stem\n",
    "\n",
    "    with h5py.File(volume_path, \"r\") as f:\n",
    "        # Get the channel-wise inputs\n",
    "        image = np.stack(\n",
    "            [f[\"raw/microtubule\"], f[\"raw/protein\"], f[\"raw/nuclei\"], f[\"raw/er\"]], axis=-1\n",
    "        )\n",
    "        labels = f[\"labels\"]\n",
    "\n",
    "    image_path = os.path.join(image_dir, f\"{fname}.tif\")\n",
    "    label_path = os.path.join(label_dir, f\"{fname}.tif\")\n",
    "\n",
    "    imageio.imwrite(image_path, image)\n",
    "    imageio.imwrite(label_path, labels)\n",
    "\n",
    "print(f\"The inputs have been preprocessed and stored at: '{os.path.join(DATA_FOLDER, 'hpa', 'preprocessed')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's understand our inputs' data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = natsorted(glob(os.path.join(image_dir, \"*.tif\")))\n",
    "label_paths = natsorted(glob(os.path.join(label_dir, \"*.tif\")))\n",
    "\n",
    "for image_path, label_path in zip(image_paths, label_paths):\n",
    "    image = imageio.imread(image_path)\n",
    "    labels = imageio.imread(label_path)\n",
    "\n",
    "    print(f\"Shape of inputs: '{image.shape}'\")  # The images should be of shape: H, W, 4 -> where, 4 is the number of channels.\n",
    "    print(f\"Shape of corresponding labels: '{labels.shape}'\")  # The labels should be of shape: H, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment Anything accepts inputs of either 1 channel or 3 channels. To fine-tune Segment Anything on our data, we must select either 1 channel or 3 channels out of the 4 channels available.\n",
    "\n",
    "Let's make the choice to choose the `microtubule` (first channel), `protein` (second channel) and `nuclei` (third channel) for finetuning Segment Anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the 'er' channel, i.e. the last channel.\n",
    "for image_path in zip(image_paths):\n",
    "    image = imageio.imread(image_path)\n",
    "    image = image[..., :-1]\n",
    "    imageio.imwrite(image_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the dataloaders\n",
    "\n",
    "Our task is to segment cells in confocal microscopy images. The dataset comes from https://zenodo.org/records/4665863, and the dataloader has been implemented in [torch-em](https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/light_microscopy/hpa.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's visualize how our samples look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, label_path in zip(image_paths, label_paths):\n",
    "    image = imageio.imread(image_path)\n",
    "    labels = imageio.imread(label_path)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    labels = connected_components(labels)\n",
    "    ax[1].imshow(labels, cmap=get_random_colors(labels), interpolation=\"nearest\")\n",
    "    ax[1].set_title(\"Ground Truth Instances\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    break  # comment this out in case you want to visualize all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's create the dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
